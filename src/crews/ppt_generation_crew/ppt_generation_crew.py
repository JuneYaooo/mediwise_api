import os
from dotenv import load_dotenv
load_dotenv()
from crewai import Agent, Crew, Process, Task
from crewai.project import CrewBase, agent, crew, task
from src.llms import *
from src.utils.json_utils import JsonUtils
from src.utils.logger import BeijingLogger
from datetime import datetime
from pathlib import Path
import uuid as uuid_lib
from src.custom_tools.suvalue_ppt_template_tool import SuvaluePPTTemplateTool
from src.custom_tools.suvalue_generate_ppt_tool import SuvalueGeneratePPTTool
from src.custom_tools.medical_ppt_generation_tool import MedicalPPTGenerationTool
from src.custom_tools.medical_ppt_template import get_template_by_id, list_available_templates
from src.custom_tools.patient_journey_image_generator import generate_patient_journey_image_sync
from src.custom_tools.indicator_chart_image_generator import generate_indicator_chart_images_multiple_sync
from src.custom_tools.treatment_data_processor import TreatmentDataProcessor
# from src.custom_tools.treatment_gantt_chart_generator import generate_treatment_gantt_chart_sync  # ä¸å†éœ€è¦ï¼šPPTæ¨¡æ¿ä¼šè‡ªå·±ç”Ÿæˆç”˜ç‰¹å›¾
from app.utils.qiniu_upload_service import QiniuUploadService

# Tokenç®¡ç†å’Œæ•°æ®å‹ç¼©æ¨¡å—
from src.utils.token_manager import TokenManager
from src.utils.data_compressor import PatientDataCompressor
from src.utils.chunked_processor import ChunkedPPTProcessor
from src.utils.llm_retry_handler import LLMRetryHandler, TokenLimitError
from src.utils.output_completeness_guard import OutputCompletenessGuard
from src.utils.output_chunked_generator import OutputChunkedGenerator  # æ—§ç‰ˆåˆ†å—ç”Ÿæˆå™¨ï¼ˆå¾…æ›¿æ¢ï¼‰
from src.utils.universal_chunked_generator import UniversalChunkedGenerator  # ğŸ†• æ–°ç‰ˆåˆ†å—ç”Ÿæˆå™¨ï¼ˆå¸¦ä¸Šä¸‹æ–‡ä¼ é€’ï¼‰

# åˆå§‹åŒ– logger
logger = BeijingLogger().get_logger()


def process_raw_files_data(raw_files_data, filter_no_cropped_image=True):
    """
    å¤„ç†åŸå§‹æ–‡ä»¶æ•°æ®ï¼Œåªä¿ç•™éœ€è¦çš„å­—æ®µ

    PPTç”Ÿæˆç­–ç•¥ï¼š
    - å¦‚æœæœ‰è£å‰ªåçš„åŒ»å­¦å½±åƒå›¾ç‰‡(cropped_image_url)ï¼Œä¼˜å…ˆä½¿ç”¨è£å‰ªå›¾
    - å¦åˆ™ä½¿ç”¨åŸå§‹å›¾ç‰‡(cloud_storage_url)

    Args:
        raw_files_data (list): åŸå§‹æ–‡ä»¶æ•°æ®åˆ—è¡¨
        filter_no_cropped_image (bool): æ˜¯å¦è¿‡æ»¤æ‰cropped_image_availableä¸ºfalseçš„æ–‡ä»¶ï¼Œé»˜è®¤True

    Returns:
        list: å¤„ç†åçš„æ–‡ä»¶æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åªåŒ…å«ï¼š
            - cloud_storage_url (PPTä¸­ä½¿ç”¨è£å‰ªå›¾æˆ–åŸå›¾)
            - exam_date
            - file_type
            - has_medical_image
            - extracted_text (å‰200ä¸ªå­—ç¬¦)
    """
    if not raw_files_data or not isinstance(raw_files_data, list):
        return []

    processed_data = []
    cropped_count = 0
    original_count = 0
    filtered_count = 0

    for file_item in raw_files_data:
        if not isinstance(file_item, dict):
            continue

        # PPTä¼˜å…ˆä½¿ç”¨è£å‰ªåçš„åŒ»å­¦å½±åƒ
        image_url = file_item.get("cloud_storage_url")
        cropped_image_available = file_item.get("cropped_image_available")
        cropped_image_url = file_item.get("cropped_image_url")
        cropped_image_uuid = file_item.get("cropped_image_uuid")
        image_bbox = file_item.get("image_bbox")
        filename = file_item.get("filename", "æœªçŸ¥æ–‡ä»¶")

        # ğŸš¨ DEBUG: è¾“å‡ºæ¯ä¸ªæ–‡ä»¶çš„è£å‰ªå›¾ä¿¡æ¯
        logger.info(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {filename}")
        logger.info(f"  â”œâ”€ has_medical_image: {file_item.get('has_medical_image', False)}")
        logger.info(f"  â”œâ”€ cropped_image_available: {cropped_image_available}")
        logger.info(f"  â”œâ”€ cropped_image_uuid: {cropped_image_uuid}")
        logger.info(f"  â”œâ”€ cropped_image_url: {cropped_image_url[:80] if cropped_image_url else None}...")
        logger.info(f"  â”œâ”€ image_bbox: {image_bbox}")
        logger.info(f"  â””â”€ cloud_storage_url: {image_url[:80] if image_url else None}...")

        # å¦‚æœå¯ç”¨è¿‡æ»¤ï¼Œåˆ™åªä¿ç•™æœ‰åŒ»å­¦å½±åƒçš„æ–‡ä»¶
        # ä¼˜å…ˆçº§ï¼šè£å‰ªå›¾ > åŸå›¾ï¼ˆå¦‚æœhas_medical_image=trueï¼‰
        if filter_no_cropped_image:
            # å¿…é¡»æ»¡è¶³ï¼šhas_medical_image=true ä¸” (æœ‰è£å‰ªå›¾ æˆ– æœ‰åŸå›¾)
            has_medical_image = file_item.get('has_medical_image', False)
            has_cropped = cropped_image_available and cropped_image_url
            has_original = image_url

            if not has_medical_image:
                filtered_count += 1
                logger.info(f"  âš ï¸ è¿‡æ»¤æ‰è¯¥æ–‡ä»¶ï¼ˆhas_medical_image=Falseï¼‰: {filename}")
                continue

            if not has_cropped and not has_original:
                filtered_count += 1
                logger.info(f"  âš ï¸ è¿‡æ»¤æ‰è¯¥æ–‡ä»¶ï¼ˆæ— è£å‰ªå›¾ä¸”æ— åŸå›¾ï¼‰: {filename}")
                continue

        if cropped_image_available and cropped_image_url:
            image_url = cropped_image_url
            cropped_count += 1
            logger.info(f"  âœ… PPTä½¿ç”¨è£å‰ªå›¾: {filename} -> {image_url[:80]}...")
        else:
            original_count += 1
            logger.info(f"  â„¹ï¸ PPTä½¿ç”¨åŸå›¾: {filename}")

        processed_item = {
            "cloud_storage_url": image_url,  # ä½¿ç”¨è£å‰ªå›¾æˆ–åŸå›¾
            "exam_date": file_item.get("exam_date"),
            "file_type": file_item.get("file_type"),
            "has_medical_image": file_item.get("has_medical_image", False),
            "extracted_text": file_item.get("extracted_text", "")[:2000]  # åªå–å‰200ä¸ªå­—ç¬¦
        }
        processed_data.append(processed_item)

    logger.info("=" * 100)
    logger.info(f"ğŸ“Š PPTå›¾ç‰‡ä½¿ç”¨ç»Ÿè®¡: è£å‰ªå›¾ {cropped_count} ä¸ª, åŸå›¾ {original_count} ä¸ª, è¿‡æ»¤ {filtered_count} ä¸ª")
    logger.info("=" * 100)

    return processed_data


@CrewBase
class PPTGenerationCrew():
    """PPT generation crew - æ”¯æŒæœ¬åœ°ç”Ÿæˆå’ŒSuvalue APIä¸¤ç§æ¨¡å¼

    **ä¸¤ç§å·¥ä½œæµç¨‹**:
    1. **Agentæµç¨‹** (USE_AGENT_WORKFLOW=true)
       - ä½¿ç”¨CrewAI Agentè‡ªåŠ¨è°ƒç”¨å·¥å…·
       - é€‚åˆå¤æ‚çš„å¤šæ­¥éª¤ä»»åŠ¡
       - å¯èƒ½å‡ºç°JSONè§£æé—®é¢˜

    2. **ç›´æ¥LLMè°ƒç”¨** (USE_AGENT_WORKFLOW=false, é»˜è®¤)
       - LLMç”Ÿæˆæ•°æ® -> ç›´æ¥è°ƒç”¨å·¥å…·
       - æ›´ç¨³å®šã€æ›´å¯æ§
       - æ¨èä½¿ç”¨

    **ä½¿ç”¨æ–¹å¼**:
    ```python
    # æ–¹å¼1: é»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–
    crew = PPTGenerationCrew()

    # æ–¹å¼2: æ‰‹åŠ¨è®¾ç½®æ¨¡å¼
    PPTGenerationCrew.set_mode(
        use_suvalue_api=True,      # True=Suvalue API, False=æœ¬åœ°python-pptx
        use_agent_workflow=False   # False=ç›´æ¥LLMè°ƒç”¨(æ¨è), True=Agentæµç¨‹
    )
    ```

    **ç¯å¢ƒå˜é‡é…ç½®**:
    - USE_SUVALUE_PPT: true/false (é»˜è®¤true)
    - USE_AGENT_WORKFLOW: true/false (é»˜è®¤false)
    """

    agents_config = "config/agents.yaml"
    tasks_config = "config/tasks.yaml"

    # é»˜è®¤ä½¿ç”¨Suvalue APIï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
    _use_suvalue_api = os.getenv("USE_SUVALUE_PPT", "true").lower() in ("true", "1", "yes")

    # æ§åˆ¶æ˜¯å¦ä½¿ç”¨Agentæµç¨‹ï¼ˆé»˜è®¤Falseï¼Œä½¿ç”¨ç›´æ¥LLMè°ƒç”¨ï¼‰
    _use_agent_workflow = os.getenv("USE_AGENT_WORKFLOW", "false").lower() in ("true", "1", "yes")

    @classmethod
    def set_mode(cls, use_suvalue_api: bool, use_agent_workflow: bool = None):
        """è®¾ç½®PPTç”Ÿæˆæ¨¡å¼

        Args:
            use_suvalue_api: True=ä½¿ç”¨Suvalue API, False=ä½¿ç”¨æœ¬åœ°python-pptx
            use_agent_workflow: True=ä½¿ç”¨Agentæµç¨‹, False=ä½¿ç”¨ç›´æ¥LLMè°ƒç”¨ï¼ˆæ›´ç¨³å®šï¼‰ï¼ŒNone=ä¿æŒå½“å‰è®¾ç½®
        """
        cls._use_suvalue_api = use_suvalue_api
        if use_agent_workflow is not None:
            cls._use_agent_workflow = use_agent_workflow
        logger.info(f"PPTGenerationCrew æ¨¡å¼: {'Suvalue API' if use_suvalue_api else 'æœ¬åœ°python-pptx'}, "
                   f"å·¥ä½œæµ: {'Agentæµç¨‹' if cls._use_agent_workflow else 'ç›´æ¥LLMè°ƒç”¨'}")

    @agent
    def ppt_content_generator(self) -> Agent:
        """æœ¬åœ°æ¨¡å¼çš„agent"""
        return Agent(
            config=self.agents_config['ppt_content_generator'],
            llm=document_generation_llm,
            tools=[MedicalPPTGenerationTool()],
            verbose=True
        )

    @agent
    def suvalue_ppt_data_transformer(self) -> Agent:
        """Suvalue APIæ¨¡å¼çš„agent"""
        return Agent(
            config=self.agents_config['suvalue_ppt_data_transformer'],
            llm=document_generation_llm,
            tools=[SuvaluePPTTemplateTool(), SuvalueGeneratePPTTool()],
            verbose=True
        )

    def _generate_ppt_data_with_llm(self, patient_timeline, raw_files_data, patient_name,
                                     patient_journey_image_url, indicator_chart_images,
                                     treatment_gantt_chart_url, treatment_gantt_data=None, template_type=2,
                                     use_chunked_output=False):
        """
        ä½¿ç”¨LLMç›´æ¥ç”ŸæˆPPTæ•°æ®ï¼ˆä¸ä½¿ç”¨Agentæµç¨‹ï¼‰

        Args:
            patient_timeline: æ‚£è€…æ—¶é—´è½´æ•°æ®
            raw_files_data: åŸå§‹æ–‡ä»¶æ•°æ®
            patient_name: æ‚£è€…å§“å
            patient_journey_image_url: æ‚£è€…æ—¶é—´æ—…ç¨‹å›¾ç‰‡URL
            indicator_chart_images: æ ¸å¿ƒæŒ‡æ ‡è¶‹åŠ¿å›¾ç‰‡åˆ—è¡¨
            treatment_gantt_chart_url: æ²»ç–—ç”˜ç‰¹å›¾URL
            treatment_gantt_data: æ²»ç–—ç”˜ç‰¹å›¾æ•°æ®åˆ—è¡¨ï¼ˆsource_file å·²æ›¿æ¢ä¸ºæ–‡ä»¶åï¼‰
            template_type: æ¨¡æ¿ç±»å‹ï¼ˆé»˜è®¤2ï¼‰
            use_chunked_output: æ˜¯å¦ä½¿ç”¨åˆ†å—è¾“å‡ºï¼ˆé»˜è®¤Falseï¼‰

        Returns:
            dict: æ ¼å¼åŒ–çš„PPTæ•°æ®ï¼Œå¯ç›´æ¥ä¼ ç»™SuvalueGeneratePPTTool
        """
        try:
            logger.info("=" * 100)
            logger.info("ğŸ¤– ä½¿ç”¨LLMç›´æ¥ç”ŸæˆPPTæ•°æ®ï¼ˆç»•è¿‡Agentæµç¨‹ï¼‰")
            logger.info("=" * 100)

            # 1. è·å–æ¨¡æ¿ä¿¡æ¯
            template_tool = SuvaluePPTTemplateTool()
            template_info = template_tool._run(template_type=template_type)

            if not template_info or not template_info.get("success"):
                error_msg = template_info.get("error", "è·å–æ¨¡æ¿ä¿¡æ¯å¤±è´¥") if template_info else "è·å–æ¨¡æ¿ä¿¡æ¯å¤±è´¥"
                logger.error(f"âŒ è·å–Suvalueæ¨¡æ¿ä¿¡æ¯å¤±è´¥: {error_msg}")
                return None

            # æ¨¡æ¿JSONåœ¨ template_json å­—æ®µä¸­ï¼ˆåŒ…å«æ³¨é‡Šçš„åŸå§‹æ¨¡æ¿ï¼‰
            template_json_str = template_info.get("template_json", "{}")
            logger.info(f"âœ… æˆåŠŸè·å–æ¨¡æ¿JSONï¼Œé•¿åº¦: {len(template_json_str)} å­—ç¬¦")

            # ========== æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†å—è¾“å‡º ==========
            if use_chunked_output:
                logger.info("=" * 100)
                logger.info("ğŸ”€ ä½¿ç”¨åˆ†å—è¾“å‡ºæ¨¡å¼ï¼ˆå¸¦ä¸Šä¸‹æ–‡ä¼ é€’ï¼‰")
                logger.info("=" * 100)

                # å‡†å¤‡æ‚£è€…æ•°æ®
                patient_data = {
                    'patient_name': patient_name,
                    'patient_timeline': patient_timeline,
                    'raw_files_data': raw_files_data,
                    'treatment_gantt_data': treatment_gantt_data,
                    'patient_journey_image_url': patient_journey_image_url,
                    'indicator_chart_images': indicator_chart_images,
                    'treatment_gantt_chart_url': treatment_gantt_chart_url
                }

                # ğŸ†• ä½¿ç”¨æ–°ç‰ˆåˆ†å—ç”Ÿæˆå™¨ï¼ˆå¸¦ä¸Šä¸‹æ–‡ä¼ é€’ï¼‰
                token_manager = TokenManager(logger=logger)
                chunked_generator = UniversalChunkedGenerator(logger=logger, token_manager=token_manager)

                # ä½¿ç”¨ generate_in_chunks æ–¹æ³•ï¼ˆæ”¯æŒä¸Šä¸‹æ–‡ä¼ é€’ï¼‰
                ppt_data = chunked_generator.generate_in_chunks(
                    llm=document_generation_llm,
                    task_type='ppt_generation',
                    input_data=patient_data,
                    template_or_schema=template_json_str,
                    model_name='gemini-3-flash-preview'
                )

                return ppt_data

            # ä¸éœ€è¦è§£æJSONï¼Œç›´æ¥å°†åŸå§‹æ¨¡æ¿ï¼ˆåŒ…å«æ³¨é‡Šï¼‰ä¼ ç»™LLM
            # LLMèƒ½ç†è§£JSONä¸­çš„æ³¨é‡Šè¯´æ˜

            # 2. æ„å»ºæç¤ºè¯
            import json

            # æ„å»ºæ²»ç–—ç”˜ç‰¹å›¾æ•°æ®è¯´æ˜
            treatment_gantt_data_str = ""
            if treatment_gantt_data:
                treatment_gantt_data_str = f"\n\n**æ²»ç–—ç”˜ç‰¹å›¾æ•°æ®** (åŒ…å«æ¯æ¡æ²»ç–—çš„è¯¦ç»†ä¿¡æ¯ï¼Œsource_file å·²æ›¿æ¢ä¸ºæ–‡ä»¶å):\n{json.dumps(treatment_gantt_data, ensure_ascii=False, indent=2)}"

            prompt = f"""ä½ æ˜¯ä¸€ä¸ªåŒ»ç–—æ•°æ®è½¬æ¢ä¸“å®¶ï¼Œéœ€è¦å°†æ‚£è€…æ•°æ®è½¬æ¢ä¸ºSuvalue PPTæ¨¡æ¿æ ¼å¼ã€‚

**ä»»åŠ¡**: æ ¹æ®ä¸‹é¢çš„æ¨¡æ¿å­—æ®µè¯´æ˜å’Œæ‚£è€…æ•°æ®ï¼Œç”Ÿæˆç¬¦åˆæ¨¡æ¿è¦æ±‚çš„JSONæ•°æ®ã€‚

**æ¨¡æ¿å­—æ®µè¯´æ˜**ï¼ˆåŒ…å«æ³¨é‡Šè¯´æ˜æ¯ä¸ªå­—æ®µçš„ç”¨é€”ï¼‰:
{template_json_str}

**æ‚£è€…æ•°æ®**:
æ‚£è€…å§“å: {patient_name}
æ‚£è€…æ—¶é—´è½´æ•°æ®: {json.dumps(patient_timeline, ensure_ascii=False)}
åŸå§‹æ–‡ä»¶æ•°æ®: {json.dumps(raw_files_data, ensure_ascii=False)}\n\n{treatment_gantt_data_str}

**é¢„ç”Ÿæˆçš„å›¾è¡¨URL** (ä¼˜å…ˆä½¿ç”¨è¿™äº›):
- æ‚£è€…æ—¶é—´æ—…ç¨‹å›¾: {patient_journey_image_url or "æœªç”Ÿæˆ"}
- æ ¸å¿ƒæŒ‡æ ‡è¶‹åŠ¿å›¾: {json.dumps(indicator_chart_images, ensure_ascii=False) if indicator_chart_images else "æœªç”Ÿæˆ"}
- æ²»ç–—ç”˜ç‰¹å›¾: {treatment_gantt_chart_url or "æœªç”Ÿæˆ"}

**é‡è¦è¦æ±‚**:
1. ä¸¥æ ¼æŒ‰ç…§æ¨¡æ¿ç»“æ„è¾“å‡ºï¼Œä¸è¦æ·»åŠ æˆ–åˆ é™¤å­—æ®µ
2. åªä½¿ç”¨æ‚£è€…æ•°æ®ä¸­çœŸå®å­˜åœ¨çš„ä¿¡æ¯ï¼Œä¸è¦ç¼–é€ 
3. å¯¹äºåŒ»å­¦åŸå§‹æ–‡ä»¶çš„å›¾åƒï¼Œä»[åŸå§‹æ–‡ä»¶æ•°æ®]ä¸­é€‰æ‹©has_medical_image=trueçš„å›¾ç‰‡ï¼ˆä¼˜å…ˆé€‰æ‹©è£å‰ªå›¾ï¼‰
4. æ²»ç–—æ•°æ®å¯ä»[æ²»ç–—ç”˜ç‰¹å›¾æ•°æ®]ä¸­è·å–ï¼Œå…¶ä¸­source_fileå­—æ®µå·²æ˜¯æ–‡ä»¶åï¼ˆä¸æ˜¯UUIDï¼‰
5. ç¡®ä¿æ‰€æœ‰å›¾ç‰‡URLæ˜¯å®Œæ•´çš„ï¼ˆåŒ…å«http://æˆ–https://ï¼‰
6. ç›´æ¥è¾“å‡ºJSONæ ¼å¼ï¼Œå»é™¤æ¨¡æ¿ä¸­çš„æ‰€æœ‰æ³¨é‡Š
7. ä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ–‡å­—ã€Markdownä»£ç å—æ ‡è®°ï¼ˆå¦‚```jsonï¼‰

è¯·è¾“å‡ºç¬¦åˆæ¨¡æ¿è¦æ±‚çš„JSONæ•°æ®:"""

            # 3. è°ƒç”¨LLM
            logger.info("ğŸ“¤ å‡†å¤‡è°ƒç”¨LLMç”ŸæˆPPTæ•°æ®...")
            logger.info(f"  â”œâ”€ æ‚£è€…å§“å: {patient_name}")
            logger.info(f"  â”œâ”€ æ—¶é—´è½´è®°å½•æ•°: {len(patient_timeline) if isinstance(patient_timeline, list) else 'N/A'}")
            logger.info(f"  â”œâ”€ åŸå§‹æ–‡ä»¶æ•°: {len(raw_files_data) if isinstance(raw_files_data, list) else 'N/A'}")
            logger.info(f"  â”œâ”€ æ²»ç–—ç”˜ç‰¹å›¾æ•°æ®: {len(treatment_gantt_data) if treatment_gantt_data else 0} æ¡")
            logger.info(f"  â””â”€ é¢„ç”Ÿæˆå›¾è¡¨: æ—¶é—´æ—…ç¨‹å›¾={'æœ‰' if patient_journey_image_url else 'æ— '}, "
                       f"æŒ‡æ ‡è¶‹åŠ¿å›¾={len(indicator_chart_images) if indicator_chart_images else 0}ä¸ª")

            try:
                # CrewAI LLM å¯¹è±¡ç›´æ¥è°ƒç”¨
                response = document_generation_llm.call(prompt)
                response_text = str(response)
                logger.info(f"âœ… LLMè°ƒç”¨æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(response_text)} å­—ç¬¦")
            except AttributeError:
                # å¦‚æœæ˜¯ LangChain LLMï¼Œä½¿ç”¨ invoke
                try:
                    response = document_generation_llm.invoke(prompt)
                    response_text = response.content if hasattr(response, 'content') else str(response)
                    logger.info(f"âœ… LLMè°ƒç”¨æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(response_text)} å­—ç¬¦")
                except Exception as e:
                    logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
                    return None

            # 4. æå–JSON
            logger.info("ğŸ” å¼€å§‹è§£æLLMå“åº”...")
            logger.info(f"  â””â”€ å“åº”å‰500å­—ç¬¦: {response_text[:500]}")

            # ä½¿ç”¨JsonUtilsæå–JSON
            ppt_data = JsonUtils.safe_parse_json(response_text, debug_prefix="LLMç”ŸæˆPPTæ•°æ®")

            if not ppt_data:
                logger.error("âŒ æ— æ³•ä»LLMå“åº”ä¸­æå–æœ‰æ•ˆJSON")
                logger.error(f"  â””â”€ å“åº”å†…å®¹: {response_text[:1000]}")
                return None

            logger.info("âœ… JSONè§£ææˆåŠŸ")

            # æ£€æŸ¥LLMè¿”å›çš„ç»“æ„ï¼Œæå–å®é™…çš„PPTæ•°æ®
            # LLMå¯èƒ½è¿”å›åŒ…è£…ç»“æ„ï¼š{"success": true, "template_json": "..."}
            # æˆ–è€…ç›´æ¥è¿”å›PPTæ•°æ®ï¼š{"pptTemplate2Vm": {...}}
            logger.info("ğŸ” æ£€æŸ¥PPTæ•°æ®ç»“æ„...")
            if "template_json" in ppt_data:
                # å¦‚æœæœ‰template_jsonå­—æ®µï¼Œéœ€è¦å†è§£æä¸€æ¬¡
                logger.info("  â”œâ”€ æ£€æµ‹åˆ°template_jsonå­—æ®µï¼Œè¿›è¡ŒäºŒæ¬¡è§£æ...")
                template_json_str = ppt_data.get("template_json", "{}")
                ppt_data = JsonUtils.safe_parse_json(template_json_str, debug_prefix="äºŒæ¬¡è§£æPPTæ•°æ®")
                if not ppt_data:
                    logger.error("  â””â”€ âŒ äºŒæ¬¡è§£æå¤±è´¥")
                    return None
                logger.info("  â””â”€ âœ… äºŒæ¬¡è§£ææˆåŠŸ")

            # éªŒè¯æ˜¯å¦åŒ…å«pptTemplate2Vmå­—æ®µ
            if "pptTemplate2Vm" not in ppt_data:
                logger.warning(f"  âš ï¸ PPTæ•°æ®ç¼ºå°‘pptTemplate2Vmå­—æ®µï¼Œå½“å‰é¡¶å±‚å­—æ®µ: {list(ppt_data.keys())}")
                # å¦‚æœé¡¶å±‚å°±æ˜¯pptTemplate2Vmçš„å†…å®¹ï¼ŒåŒ…è£…ä¸€ä¸‹
                if any(key in ppt_data for key in ["title", "patient", "diag"]):
                    logger.info("  â”œâ”€ æ£€æµ‹åˆ°é¡¶å±‚åŒ…å«PPTå­—æ®µï¼Œè‡ªåŠ¨åŒ…è£…ä¸ºpptTemplate2Vmç»“æ„")
                    ppt_data = {"pptTemplate2Vm": ppt_data}
                    logger.info("  â””â”€ âœ… è‡ªåŠ¨åŒ…è£…æˆåŠŸ")
                else:
                    logger.error("  â””â”€ âŒ æ— æ³•è¯†åˆ«PPTæ•°æ®ç»“æ„")
                    return None

            logger.info("=" * 100)
            logger.info(f"âœ… æˆåŠŸç”ŸæˆPPTæ•°æ®ç»“æ„")
            logger.info(f"ğŸ“¦ pptTemplate2Vm åŒ…å«å­—æ®µ: {list(ppt_data.get('pptTemplate2Vm', {}).keys())[:10]}")
            logger.info("=" * 100)
            return ppt_data

        except Exception as e:
            logger.error(f"ä½¿ç”¨LLMç”ŸæˆPPTæ•°æ®æ—¶å‡ºé”™: {e}", exc_info=True)
            return None

    @task
    def generate_ppt_slides_task(self) -> Task:
        """æœ¬åœ°æ¨¡å¼çš„task"""
        return Task(
            config=self.tasks_config['generate_ppt_slides_task']
        )

    @task
    def transform_and_generate_ppt_task(self) -> Task:
        """Suvalue APIæ¨¡å¼çš„task"""
        return Task(
            config=self.tasks_config['transform_and_generate_ppt_task']
        )

    @crew
    def crew(self) -> Crew:
        """Creates the PPT generation crew"""
        # æ ¹æ®æ¨¡å¼é€‰æ‹©ä¸åŒçš„ agents å’Œ tasks
        if self._use_suvalue_api:
            agents = [self.suvalue_ppt_data_transformer()]
            tasks = [self.transform_and_generate_ppt_task()]
        else:
            agents = [self.ppt_content_generator()]
            tasks = [self.generate_ppt_slides_task()]

        return Crew(
            agents=agents,
            tasks=tasks,
            process=Process.sequential,
            verbose=True
        )

    def generate_ppt(self, patient_timeline, patient_journey, raw_files_data, agent_session_id,
                     auth_token=None, template_id="medical", filter_no_cropped_image=True):
        """
        Generate PPT from patient data (å¢å¼ºç‰ˆ - æ”¯æŒè‡ªåŠ¨å‹ç¼©å’Œåˆ†å—å¤„ç†)

        æ ¹æ®åˆå§‹åŒ–æ—¶çš„ use_suvalue_api å‚æ•°é€‰æ‹©ç”Ÿæˆæ–¹å¼ï¼š
        - True: ä½¿ç”¨Suvalue APIç”Ÿæˆï¼ˆéœ€è¦auth_tokenï¼‰
        - False: ä½¿ç”¨æœ¬åœ°python-pptxç”Ÿæˆï¼ˆéœ€è¦template_idï¼‰

        æ–°å¢åŠŸèƒ½ï¼š
        - è‡ªåŠ¨æ£€æµ‹tokenè¶…é™
        - æ™ºèƒ½æ•°æ®å‹ç¼©
        - åˆ†å—å¤„ç†è¶…å¤§æ•°æ®é›†
        - è¾“å‡ºå®Œæ•´æ€§ä¿æŠ¤

        Args:
            patient_timeline (dict or list): Patient timeline data for PPT content generation
            patient_journey (dict or list): Patient journey data for image generation (timeline chart, indicator trends)
            raw_files_data (list): Raw files data (will be processed to keep only required fields)
            agent_session_id (str): Session ID for file organization
            auth_token (str, optional): Bearer token for Suvalue API authentication (Suvalueæ¨¡å¼å¿…éœ€)
            template_id (str, optional): PPT template ID for local generation (æœ¬åœ°æ¨¡å¼å¿…éœ€, default: "medical")
            filter_no_cropped_image (bool, optional): æ˜¯å¦è¿‡æ»¤æ‰cropped_image_availableä¸ºfalseçš„æ–‡ä»¶ï¼Œé»˜è®¤True

        Returns:
            dict: PPT info
                - Suvalueæ¨¡å¼: {"success": bool, "ppt_url": str, "message": str}
                - æœ¬åœ°æ¨¡å¼: {"success": bool, "local_path": str, "file_uuid": str, "qiniu_url": str}
        """
        try:
            # ========== åˆå§‹åŒ–Tokenç®¡ç†å’Œæ•°æ®å‹ç¼©æ¨¡å— ==========
            token_manager = TokenManager(logger=logger)
            data_compressor = PatientDataCompressor(logger=logger, token_manager=token_manager)
            chunked_processor = ChunkedPPTProcessor(logger=logger, token_manager=token_manager)
            output_guard = OutputCompletenessGuard(logger=logger)
            output_chunked_generator = OutputChunkedGenerator(logger=logger, token_manager=token_manager)
            if self._use_suvalue_api:
                logger.info("Starting Suvalue PPT generation task (API mode)")
            else:
                logger.info("Starting Local PPT generation task (python-pptx mode)")
            current_date = datetime.now().strftime("%Y-%m-%d")

            # å¤„ç† raw_files_dataï¼Œåªä¿ç•™éœ€è¦çš„å­—æ®µ
            processed_raw_files_data = process_raw_files_data(raw_files_data, filter_no_cropped_image=filter_no_cropped_image)
            logger.info(f"å¤„ç†äº† {len(processed_raw_files_data)} ä¸ªæ–‡ä»¶çš„å…ƒæ•°æ®ï¼ˆä»…ä¿ç•™PPTæ‰€éœ€å­—æ®µï¼‰")

            # ========== Tokenæ£€æŸ¥å’Œæ•°æ®å‹ç¼© ==========
            model_name = 'gemini-3-flash-preview'  # ä»llms.pyè·å–
            enable_auto_compression = os.getenv('ENABLE_AUTO_COMPRESSION', 'true').lower() in ('true', '1', 'yes')

            # æ„å»ºè¾“å…¥æ•°æ®ç”¨äºtokenæ£€æŸ¥
            input_data_for_check = {
                'patient_timeline': patient_timeline,
                'raw_files_data': processed_raw_files_data,
                'patient_journey': patient_journey
            }

            logger.info("=" * 100)
            logger.info("ğŸ” å¼€å§‹Tokenæ£€æŸ¥å’Œæ•°æ®å‹ç¼©æµç¨‹")
            logger.info("=" * 100)

            # æ£€æŸ¥è¾“å…¥tokené™åˆ¶
            check_result = token_manager.check_input_limit(input_data_for_check, model_name)

            logger.info(f"ğŸ“Š è¾“å…¥æ•°æ®ç»Ÿè®¡:")
            logger.info(f"  â”œâ”€ æ‚£è€…æ—¶é—´è½´è®°å½•æ•°: {len(patient_timeline) if isinstance(patient_timeline, list) else 'N/A'}")
            logger.info(f"  â”œâ”€ åŸå§‹æ–‡ä»¶æ•°: {len(processed_raw_files_data)}")
            logger.info(f"  â”œâ”€ ä¼°ç®—æ€»tokens: {check_result['total_tokens']}")
            logger.info(f"  â”œâ”€ æ¨¡å‹é™åˆ¶: {check_result['limit']} tokens")
            logger.info(f"  â”œâ”€ å®‰å…¨é™åˆ¶: {check_result['safe_limit']} tokens")
            logger.info(f"  â”œâ”€ ä½¿ç”¨ç‡: {check_result['usage_ratio']:.1%}")
            logger.info(f"  â””â”€ éœ€è¦å‹ç¼©: {'æ˜¯ âš ï¸' if check_result['compression_needed'] else 'å¦ âœ…'}")

            # å¦‚æœéœ€è¦å‹ç¼©ä¸”å¯ç”¨äº†è‡ªåŠ¨å‹ç¼©
            if check_result['compression_needed'] and enable_auto_compression:
                logger.warning("=" * 100)
                logger.warning(f"âš ï¸ è¾“å…¥æ•°æ®è¶…è¿‡å®‰å…¨é™åˆ¶ï¼Œå¯åŠ¨è‡ªåŠ¨å‹ç¼©æµç¨‹")
                logger.warning(f"âš ï¸ å½“å‰: {check_result['total_tokens']} tokens > å®‰å…¨é™åˆ¶: {check_result['safe_limit']} tokens")
                logger.warning("=" * 100)

                # è®¡ç®—ç›®æ ‡tokenæ•°
                target_tokens = check_result['safe_limit']

                # è®°å½•å‹ç¼©å‰çš„æ•°æ®é‡
                original_timeline_count = len(patient_timeline) if isinstance(patient_timeline, list) else 0
                original_files_count = len(processed_raw_files_data)

                # åˆ†åˆ«å‹ç¼©ä¸åŒçš„æ•°æ®
                # 1. å‹ç¼©æ—¶é—´è½´æ•°æ®ï¼ˆåˆ†é…50%çš„ç›®æ ‡tokenï¼‰
                if patient_timeline:
                    logger.info(f"ğŸ“¦ å¼€å§‹å‹ç¼©æ—¶é—´è½´æ•°æ® (ç›®æ ‡: {int(target_tokens * 0.5)} tokens)...")
                    patient_timeline = data_compressor.compress_timeline(
                        patient_timeline,
                        target_tokens=int(target_tokens * 0.5)
                    )
                    compressed_timeline_count = len(patient_timeline) if isinstance(patient_timeline, list) else 0
                    logger.info(f"  âœ… æ—¶é—´è½´å‹ç¼©å®Œæˆ: {original_timeline_count} æ¡ â†’ {compressed_timeline_count} æ¡ "
                              f"(ä¿ç•™ç‡: {compressed_timeline_count/original_timeline_count:.1%})")

                # 2. å‹ç¼©åŸå§‹æ–‡ä»¶æ•°æ®ï¼ˆåˆ†é…30%çš„ç›®æ ‡tokenï¼‰
                if processed_raw_files_data:
                    logger.info(f"ğŸ“¦ å¼€å§‹å‹ç¼©åŸå§‹æ–‡ä»¶æ•°æ® (ç›®æ ‡: {int(target_tokens * 0.3)} tokens)...")
                    # ç»Ÿè®¡åŒ»å­¦å½±åƒæ–‡ä»¶æ•°
                    original_medical_count = sum(1 for f in processed_raw_files_data if f.get('has_medical_image', False))

                    processed_raw_files_data = data_compressor.compress_raw_files(
                        processed_raw_files_data,
                        target_tokens=int(target_tokens * 0.3)
                    )

                    compressed_files_count = len(processed_raw_files_data)
                    compressed_medical_count = sum(1 for f in processed_raw_files_data if f.get('has_medical_image', False))

                    logger.info(f"  âœ… æ–‡ä»¶å‹ç¼©å®Œæˆ: {original_files_count} ä¸ª â†’ {compressed_files_count} ä¸ª "
                              f"(ä¿ç•™ç‡: {compressed_files_count/original_files_count:.1%})")
                    logger.info(f"  âœ… åŒ»å­¦å½±åƒ: {original_medical_count} ä¸ª â†’ {compressed_medical_count} ä¸ª "
                              f"(ä¿ç•™ç‡: {compressed_medical_count/original_medical_count:.1%})" if original_medical_count > 0 else "  â„¹ï¸ æ— åŒ»å­¦å½±åƒæ–‡ä»¶")

                # 3. å‹ç¼©patient_journeyæ•°æ®ï¼ˆåˆ†é…20%çš„ç›®æ ‡tokenï¼‰
                if patient_journey and isinstance(patient_journey, dict):
                    logger.info(f"ğŸ“¦ å¼€å§‹å‹ç¼©patient_journeyæ•°æ® (ç›®æ ‡: {int(target_tokens * 0.2)} tokens)...")
                    patient_journey = data_compressor.compress_data(
                        patient_journey,
                        target_tokens=int(target_tokens * 0.2)
                    )
                    logger.info(f"  âœ… patient_journeyå‹ç¼©å®Œæˆ")

                # é‡æ–°æ£€æŸ¥å‹ç¼©åçš„tokenæ•°
                compressed_data = {
                    'patient_timeline': patient_timeline,
                    'raw_files_data': processed_raw_files_data,
                    'patient_journey': patient_journey
                }
                compressed_check = token_manager.check_input_limit(compressed_data, model_name)

                logger.info("=" * 100)
                logger.info(f"âœ… æ•°æ®å‹ç¼©å®Œæˆï¼")
                logger.info(f"ğŸ“Š å‹ç¼©æ•ˆæœ:")
                logger.info(f"  â”œâ”€ åŸå§‹tokens: {check_result['total_tokens']}")
                logger.info(f"  â”œâ”€ å‹ç¼©åtokens: {compressed_check['total_tokens']}")
                logger.info(f"  â”œâ”€ å‹ç¼©æ¯”ä¾‹: {compressed_check['total_tokens']/check_result['total_tokens']:.1%}")
                logger.info(f"  â”œâ”€ æ–°ä½¿ç”¨ç‡: {compressed_check['usage_ratio']:.1%}")
                logger.info(f"  â””â”€ åœ¨é™åˆ¶å†…: {'æ˜¯ âœ…' if compressed_check['within_limit'] else 'å¦ âŒ'}")
                logger.info("=" * 100)

                # å¦‚æœå‹ç¼©åä»è¶…é™ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†å—å¤„ç†
                if not compressed_check['within_limit']:
                    logger.error("=" * 100)
                    logger.error(f"âŒ æ•°æ®å‹ç¼©åä»è¶…è¿‡æ¨¡å‹é™åˆ¶")
                    logger.error(f"âŒ å½“å‰: {compressed_check['total_tokens']} tokens > é™åˆ¶: {compressed_check['limit']} tokens")
                    logger.error(f"âŒ å»ºè®®: 1) å‡å°‘æ•°æ®é‡  2) ä½¿ç”¨æ›´æ¿€è¿›çš„å‹ç¼©ç­–ç•¥  3) å¯ç”¨åˆ†å—å¤„ç†")
                    logger.error("=" * 100)
                    # è¿™é‡Œå¯ä»¥é€‰æ‹©ï¼š1) ä½¿ç”¨åˆ†å—å¤„ç†  2) è¿”å›é”™è¯¯
                    # æš‚æ—¶è¿”å›é”™è¯¯ï¼Œè®©ç”¨æˆ·çŸ¥é“æ•°æ®é‡è¿‡å¤§
                    return {
                        "success": False,
                        "error": f"æ‚£è€…æ•°æ®é‡è¿‡å¤§ï¼Œå³ä½¿å‹ç¼©åä»è¶…è¿‡æ¨¡å‹é™åˆ¶ ({compressed_check['total_tokens']} > {compressed_check['limit']} tokens)ã€‚"
                                f"å»ºè®®å‡å°‘æ•°æ®é‡æˆ–è”ç³»æŠ€æœ¯æ”¯æŒã€‚"
                    }
            elif not enable_auto_compression and check_result['compression_needed']:
                logger.warning("=" * 100)
                logger.warning(f"âš ï¸ è¾“å…¥æ•°æ®è¶…è¿‡å®‰å…¨é™åˆ¶ï¼Œä½†è‡ªåŠ¨å‹ç¼©å·²ç¦ç”¨")
                logger.warning(f"âš ï¸ å½“å‰: {check_result['total_tokens']} tokens > å®‰å…¨é™åˆ¶: {check_result['safe_limit']} tokens")
                logger.warning(f"âš ï¸ å»ºè®®: å¯ç”¨ ENABLE_AUTO_COMPRESSION=true")
                logger.warning("=" * 100)
            else:
                logger.info("=" * 100)
                logger.info(f"âœ… æ•°æ®é‡åœ¨å®‰å…¨èŒƒå›´å†…ï¼Œæ— éœ€å‹ç¼©")
                logger.info("=" * 100)


            # è·å–æ‚£è€…å§“å
            patient_name = 'æ‚£è€…'
            if isinstance(patient_journey, dict):
                # ä» patient_journey ä¸­è·å–æ‚£è€…å§“å
                if 'patient_info' in patient_journey:
                    patient_info = patient_journey.get('patient_info', {})
                    if isinstance(patient_info, dict):
                        basic_info = patient_info.get('basic', {})
                        if isinstance(basic_info, dict):
                            patient_name = basic_info.get('name', 'æ‚£è€…')
            elif isinstance(patient_journey, list) and patient_journey:
                # å¦‚æœæ˜¯åˆ—è¡¨æ ¼å¼ï¼Œå°è¯•ä»ç¬¬ä¸€ä¸ªæ¡ç›®è·å–æ‚£è€…å§“å
                first_entry = patient_journey[0]
                if isinstance(first_entry, dict):
                    patient_name = first_entry.get('patient_name', 'æ‚£è€…')

            logger.info(f"Patient name: {patient_name}")

            # ========== ç”Ÿæˆæ‚£è€…æ—¶é—´æ—…ç¨‹å›¾ç‰‡å¹¶ä¸Šä¼ åˆ°ä¸ƒç‰›äº‘ ==========
            patient_journey_image_url = None
            patient_journey_image_path = None

            # æå–æ—¶é—´çº¿æ•°æ®
            timeline_data = None
            if patient_journey:
                if isinstance(patient_journey, dict):
                    # å¦‚æœæ˜¯å­—å…¸ï¼Œå°è¯•æå–timeline_journeyå­—æ®µ
                    timeline_data = patient_journey.get('timeline_journey', patient_journey)
                elif isinstance(patient_journey, list):
                    # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œç›´æ¥ä½¿ç”¨
                    timeline_data = patient_journey

            if timeline_data:
                try:
                    # æ”¯æŒåˆ—è¡¨æ ¼å¼
                    if isinstance(timeline_data, list) and timeline_data:
                        logger.info("å¼€å§‹ç”Ÿæˆæ‚£è€…æ—¶é—´æ—…ç¨‹å›¾ç‰‡ï¼ˆç”¨äºPPTï¼‰...")

                        # ç”Ÿæˆå›¾ç‰‡æ–‡ä»¶åå’Œè·¯å¾„
                        image_uuid = str(uuid_lib.uuid4())
                        output_dir = Path("output/files_extract") / agent_session_id / "ppt_images"
                        output_dir.mkdir(parents=True, exist_ok=True)

                        image_filename = f"patient_journey_{image_uuid}.png"
                        image_path = output_dir / image_filename

                        # ç”Ÿæˆå›¾ç‰‡
                        success = generate_patient_journey_image_sync(
                            patient_journey_data=timeline_data,
                            output_path=str(image_path),
                            patient_name=patient_name
                        )

                        if success and image_path.exists():
                            patient_journey_image_path = str(image_path)
                            logger.info(f"æ‚£è€…æ—¶é—´æ—…ç¨‹å›¾ç‰‡ç”ŸæˆæˆåŠŸ: {patient_journey_image_path}")

                            # ä¸Šä¼ åˆ°ä¸ƒç‰›äº‘
                            try:
                                qiniu_service = QiniuUploadService()
                                qiniu_key = f"patient_journey_ppt/{image_uuid}.png"

                                upload_success, cloud_url, error = qiniu_service.upload_file(
                                    str(image_path),
                                    qiniu_key
                                )

                                if upload_success:
                                    patient_journey_image_url = cloud_url
                                    logger.info(f"æ‚£è€…æ—¶é—´æ—…ç¨‹å›¾ç‰‡å·²ä¸Šä¼ åˆ°ä¸ƒç‰›äº‘: {cloud_url}")
                                else:
                                    logger.error(f"ä¸Šä¼ æ‚£è€…æ—¶é—´æ—…ç¨‹å›¾ç‰‡åˆ°ä¸ƒç‰›äº‘å¤±è´¥: {error}")
                            except Exception as upload_error:
                                logger.error(f"ä¸Šä¼ æ‚£è€…æ—¶é—´æ—…ç¨‹å›¾ç‰‡åˆ°ä¸ƒç‰›äº‘æ—¶å‡ºé”™: {upload_error}")
                        else:
                            logger.warning("æ‚£è€…æ—¶é—´æ—…ç¨‹å›¾ç‰‡ç”Ÿæˆå¤±è´¥")
                    else:
                        logger.info("æ‚£è€…æ—…ç¨‹æ•°æ®ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡å›¾ç‰‡ç”Ÿæˆ")

                except Exception as e:
                    logger.error(f"ç”Ÿæˆæˆ–ä¸Šä¼ æ‚£è€…æ—¶é—´æ—…ç¨‹å›¾ç‰‡æ—¶å‡ºé”™: {e}")
                    import traceback
                    logger.error(traceback.format_exc())

            # ========== ç”Ÿæˆæ ¸å¿ƒæŒ‡æ ‡è¶‹åŠ¿å›¾ç‰‡å¹¶ä¸Šä¼ åˆ°ä¸ƒç‰›äº‘ ==========
            indicator_chart_images = []  # å­˜å‚¨å¤šä¸ªæŒ‡æ ‡å›¾ç‰‡çš„ä¿¡æ¯
            if patient_journey:
                try:
                    # æ£€æŸ¥æ˜¯å¦æœ‰indicator_seriesæ•°æ®
                    indicator_series = None
                    if isinstance(patient_journey, dict) and 'indicator_series' in patient_journey:
                        indicator_series = patient_journey.get('indicator_series')

                    if indicator_series and isinstance(indicator_series, list) and indicator_series:
                        logger.info(f"å¼€å§‹ç”Ÿæˆæ ¸å¿ƒæŒ‡æ ‡è¶‹åŠ¿å›¾ç‰‡ï¼ˆç”¨äºPPTï¼‰ï¼ŒåŒ…å« {len(indicator_series)} ä¸ªæŒ‡æ ‡ï¼Œæ¯ä¸ªæŒ‡æ ‡å•ç‹¬ç”Ÿæˆå›¾ç‰‡...")

                        # ç”Ÿæˆå›¾ç‰‡ç›®å½•
                        output_dir = Path("output/files_extract") / agent_session_id / "ppt_images" / "indicators"
                        output_dir.mkdir(parents=True, exist_ok=True)

                        # ä¸ºæ¯ä¸ªæŒ‡æ ‡ç”Ÿæˆç‹¬ç«‹çš„å›¾ç‰‡
                        results = generate_indicator_chart_images_multiple_sync(
                            indicator_series_data=indicator_series,
                            output_dir=str(output_dir),
                            patient_name=patient_name
                        )

                        # ä¸Šä¼ æ¯ä¸ªå›¾ç‰‡åˆ°ä¸ƒç‰›äº‘
                        qiniu_service = QiniuUploadService()
                        for result in results:
                            if result['success'] and result['file_path']:
                                try:
                                    # ç”Ÿæˆå”¯ä¸€çš„ä¸ƒç‰›äº‘key
                                    file_path = Path(result['file_path'])
                                    image_uuid = str(uuid_lib.uuid4())
                                    qiniu_key = f"indicator_chart_ppt/{image_uuid}_{file_path.name}"

                                    upload_success, cloud_url, error = qiniu_service.upload_file(
                                        result['file_path'],
                                        qiniu_key
                                    )

                                    if upload_success:
                                        indicator_chart_images.append({
                                            "indicator_name": result['indicator_name'],
                                            "local_path": result['file_path'],
                                            "cloud_url": cloud_url
                                        })
                                        logger.info(f"æŒ‡æ ‡ '{result['indicator_name']}' å›¾ç‰‡å·²ä¸Šä¼ åˆ°ä¸ƒç‰›äº‘: {cloud_url}")
                                    else:
                                        logger.error(f"ä¸Šä¼ æŒ‡æ ‡ '{result['indicator_name']}' å›¾ç‰‡åˆ°ä¸ƒç‰›äº‘å¤±è´¥: {error}")
                                        # å³ä½¿ä¸Šä¼ å¤±è´¥ï¼Œä¹Ÿä¿ç•™æœ¬åœ°è·¯å¾„
                                        indicator_chart_images.append({
                                            "indicator_name": result['indicator_name'],
                                            "local_path": result['file_path'],
                                            "cloud_url": None
                                        })
                                except Exception as upload_error:
                                    logger.error(f"ä¸Šä¼ æŒ‡æ ‡ '{result['indicator_name']}' å›¾ç‰‡åˆ°ä¸ƒç‰›äº‘æ—¶å‡ºé”™: {upload_error}")
                                    indicator_chart_images.append({
                                        "indicator_name": result['indicator_name'],
                                        "local_path": result['file_path'],
                                        "cloud_url": None
                                    })

                        logger.info(f"æ ¸å¿ƒæŒ‡æ ‡è¶‹åŠ¿å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼ŒæˆåŠŸç”Ÿæˆ {len(indicator_chart_images)} ä¸ªå›¾ç‰‡")
                    else:
                        logger.info("æŒ‡æ ‡åºåˆ—æ•°æ®ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡å›¾ç‰‡ç”Ÿæˆ")

                except Exception as e:
                    logger.error(f"ç”Ÿæˆæˆ–ä¸Šä¼ æ ¸å¿ƒæŒ‡æ ‡è¶‹åŠ¿å›¾ç‰‡æ—¶å‡ºé”™: {e}")
                    import traceback
                    logger.error(traceback.format_exc())

            # ========== å¤„ç†æ²»ç–—ç”˜ç‰¹å›¾æ•°æ®ï¼ˆä»…æ•°æ®å¤„ç†ï¼Œä¸ç”Ÿæˆå›¾ç‰‡ï¼‰==========
            #
            # æ²»ç–—ç”˜ç‰¹å›¾æ•°æ®å¤„ç†æµç¨‹ï¼š
            # 1. ä» patient_timeline æˆ– patient_journey ä¸­æå–æ²»ç–—æ•°æ®
            # 2. ä½¿ç”¨ TreatmentDataProcessor å¤„ç†æ•°æ®ï¼Œç”Ÿæˆç”˜ç‰¹å›¾æ‰€éœ€æ ¼å¼ (gantt_data)
            # 3. æ„å»º file_uuid -> filename æ˜ å°„ï¼Œå°† gantt_data ä¸­çš„ source_file (UUID) æ›¿æ¢ä¸ºæ–‡ä»¶å
            # 4. å°† gantt_data ä¼ é€’ç»™ PPT æ¨¡æ¿ï¼ˆPPT æ¨¡æ¿ä¼šè‡ªå·±ç”Ÿæˆç”˜ç‰¹å›¾ï¼‰
            #
            # gantt_data æ•°æ®ç»“æ„ç¤ºä¾‹ï¼ˆå¤„ç†åçš„åŸå§‹æ•°æ®ï¼Œå¯ç›´æ¥ä¼ ç»™PPTæ¨¡æ¿ï¼‰ï¼š
            # [
            #   {
            #     "treatment_name": "æ²»ç–—åç§°",           # æ²»ç–—æ–¹æ¡ˆåç§°
            #     "start_date": "2024-01-01",           # å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            #     "end_date": "2024-01-15",             # ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            #     "category": "åŒ–ç–—/æ”¾ç–—/æ‰‹æœ¯/é¶å‘æ²»ç–—ç­‰", # æ²»ç–—ç±»åˆ«
            #     "source_file": "æ¥æºæ–‡ä»¶å.pdf",       # æ¥æºæ–‡ä»¶åï¼ˆå·²ä»UUIDè½¬æ¢ï¼‰
            #     "details": "æ²»ç–—è¯¦æƒ…æè¿°"              # æ²»ç–—è¯¦ç»†ä¿¡æ¯
            #   },
            #   ...
            # ]
            #
            treatment_gantt_chart_url = None
            treatment_gantt_chart_path = None
            source_file_mapping = {}  # å­˜å‚¨ file_uuid -> filename çš„æ˜ å°„

            if patient_timeline or patient_journey:
                try:
                    logger.info("å¼€å§‹å¤„ç†æ‚£è€…æ²»ç–—æ•°æ®ï¼ˆä»…æå–æ•°æ®ï¼Œä¸ç”Ÿæˆå›¾ç‰‡ï¼‰...")

                    # åˆå§‹åŒ–æ²»ç–—æ•°æ®å¤„ç†å™¨
                    treatment_processor = TreatmentDataProcessor()

                    # ä»patient_timelineæˆ–patient_journeyä¸­æå–æ²»ç–—æ•°æ®
                    # ä¼˜å…ˆä½¿ç”¨patient_timelineï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨patient_journey
                    source_data = patient_timeline if patient_timeline else patient_journey

                    # è°ƒè¯•ï¼šæ£€æŸ¥æ•°æ®ç±»å‹å’Œå†…å®¹
                    logger.info(f"æ²»ç–—æ•°æ®æºç±»å‹: {type(source_data)}")
                    if isinstance(source_data, str):
                        logger.info(f"æ•°æ®æ˜¯å­—ç¬¦ä¸²ï¼Œé•¿åº¦: {len(source_data)}, å‰200å­—ç¬¦: {source_data[:200]}")
                    elif isinstance(source_data, dict):
                        logger.info(f"æ•°æ®æ˜¯å­—å…¸ï¼Œé”®: {list(source_data.keys())[:10]}")
                    elif isinstance(source_data, list):
                        logger.info(f"æ•°æ®æ˜¯åˆ—è¡¨ï¼Œé•¿åº¦: {len(source_data)}")

                    # å¤„ç†æ²»ç–—æ•°æ®ç”Ÿæˆç”˜ç‰¹å›¾æ‰€éœ€æ ¼å¼
                    gantt_data = treatment_processor.process_patient_treatments(source_data)

                    if gantt_data and len(gantt_data) > 0:
                        logger.info(f"æˆåŠŸæå– {len(gantt_data)} æ¡æ²»ç–—è®°å½•")
                        logger.info(f"ç”˜ç‰¹å›¾æ•°æ®: {gantt_data}")

                        # æ„å»º source_file (file_uuid) -> filename çš„æ˜ å°„
                        if raw_files_data and isinstance(raw_files_data, list):
                            for file_item in raw_files_data:
                                if isinstance(file_item, dict):
                                    file_uuid = file_item.get("file_uuid")
                                    filename = file_item.get("filename")
                                    if file_uuid and filename:
                                        source_file_mapping[file_uuid] = filename
                            logger.info(f"æ„å»ºäº† {len(source_file_mapping)} ä¸ªæ–‡ä»¶æ˜ å°„å…³ç³»")

                        # æ›¿æ¢ gantt_data ä¸­çš„ source_file (UUID) ä¸º source_file_name (æ–‡ä»¶å)
                        for treatment in gantt_data:
                            source_file_uuid = treatment.get("source_file", "")
                            if source_file_uuid and source_file_uuid in source_file_mapping:
                                treatment["source_file"] = source_file_mapping[source_file_uuid]
                            else:
                                treatment["source_file"] = ""

                        logger.info("å·²å°†æ²»ç–—è®°å½•çš„ source_file ä» UUID æ›¿æ¢ä¸ºæ–‡ä»¶å")
                        logger.info("æ²»ç–—ç”˜ç‰¹å›¾æ•°æ®å¤„ç†å®Œæˆï¼Œå°†ä¼ é€’ç»™ PPT æ¨¡æ¿è‡ªè¡Œç”Ÿæˆå›¾è¡¨")

                        # ========== ä»¥ä¸‹ä»£ç å·²æ³¨é‡Šï¼šä¸å†ç”Ÿæˆç”˜ç‰¹å›¾å›¾ç‰‡ï¼ŒPPTæ¨¡æ¿ä¼šè‡ªå·±ç”Ÿæˆ ==========
                        # # ç”Ÿæˆå›¾ç‰‡æ–‡ä»¶åå’Œè·¯å¾„
                        # gantt_uuid = str(uuid_lib.uuid4())
                        # output_dir = Path("output/files_extract") / agent_session_id / "ppt_images"
                        # output_dir.mkdir(parents=True, exist_ok=True)
                        #
                        # gantt_filename = f"treatment_gantt_{gantt_uuid}.png"
                        # gantt_path = output_dir / gantt_filename
                        #
                        # # ç”Ÿæˆç”˜ç‰¹å›¾å›¾ç‰‡ï¼ˆä½¿ç”¨ECharts - æœ¬åœ°æ¸²æŸ“ï¼Œæ— éœ€è”ç½‘ï¼‰
                        # success = generate_treatment_gantt_chart_sync(
                        #     gantt_data=gantt_data,
                        #     output_path=str(gantt_path),
                        #     patient_name=patient_name,
                        #     use_google_charts=False  # ä½¿ç”¨EChartsï¼Œæ¯æ¡æ²»ç–—è®°å½•ç‹¬ç«‹æ˜¾ç¤º
                        # )
                        #
                        # if success and gantt_path.exists():
                        #     treatment_gantt_chart_path = str(gantt_path)
                        #     logger.info(f"æ²»ç–—ç”˜ç‰¹å›¾ç”ŸæˆæˆåŠŸ: {treatment_gantt_chart_path}")
                        #
                        #     # ä¸Šä¼ åˆ°ä¸ƒç‰›äº‘
                        #     try:
                        #         qiniu_service = QiniuUploadService()
                        #         qiniu_key = f"treatment_gantt_ppt/{gantt_uuid}.png"
                        #
                        #         upload_success, cloud_url, error = qiniu_service.upload_file(
                        #             str(gantt_path),
                        #             qiniu_key
                        #         )
                        #
                        #         if upload_success:
                        #             treatment_gantt_chart_url = cloud_url
                        #             logger.info(f"æ²»ç–—ç”˜ç‰¹å›¾å·²ä¸Šä¼ åˆ°ä¸ƒç‰›äº‘: {cloud_url}")
                        #         else:
                        #             logger.error(f"ä¸Šä¼ æ²»ç–—ç”˜ç‰¹å›¾åˆ°ä¸ƒç‰›äº‘å¤±è´¥: {error}")
                        #     except Exception as upload_error:
                        #         logger.error(f"ä¸Šä¼ æ²»ç–—ç”˜ç‰¹å›¾åˆ°ä¸ƒç‰›äº‘æ—¶å‡ºé”™: {upload_error}")
                        # else:
                        #     logger.warning("æ²»ç–—ç”˜ç‰¹å›¾ç”Ÿæˆå¤±è´¥")
                    else:
                        logger.info("æœªæå–åˆ°æ²»ç–—æ•°æ®ï¼Œè·³è¿‡ç”˜ç‰¹å›¾å¤„ç†")

                except Exception as e:
                    logger.error(f"å¤„ç†æ²»ç–—ç”˜ç‰¹å›¾æ•°æ®æ—¶å‡ºé”™: {e}")
                    import traceback
                    logger.error(traceback.format_exc())

            # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯JSONå¯åºåˆ—åŒ–çš„
            import json

            # è½¬æ¢æ•°æ®ä¸ºJSONå­—ç¬¦ä¸²ä»¥ç¡®ä¿å¯åºåˆ—åŒ–
            try:
                patient_timeline_json = json.dumps(patient_timeline, ensure_ascii=False, default=str)
                processed_files_json = json.dumps(processed_raw_files_data, ensure_ascii=False, default=str)
                # æ–°å¢ï¼šåºåˆ—åŒ– treatment_gantt_data (å·²åŒ…å« source_file æ–‡ä»¶å)
                treatment_gantt_data_json = json.dumps(gantt_data if 'gantt_data' in locals() else [], ensure_ascii=False, default=str)
            except Exception as e:
                logger.error(f"JSONåºåˆ—åŒ–å¤±è´¥: {str(e)}")
                return {"success": False, "error": f"æ•°æ®åºåˆ—åŒ–å¤±è´¥: {str(e)}"}

            # æ ¹æ®æ¨¡å¼å‡†å¤‡ä¸åŒçš„è¾“å…¥å‚æ•°
            if self._use_suvalue_api:
                # Suvalue APIæ¨¡å¼
                ppt_inputs = {
                    "current_date": current_date,
                    "patient_timeline": patient_timeline_json,
                    "raw_files_data": processed_files_json,
                    "patient_name": patient_name,
                    "auth_token": auth_token or "",  # å…è®¸ä¸ºç©º
                    "session_id": agent_session_id,
                    "patient_journey_image_url": patient_journey_image_url or "",
                    "indicator_chart_images": json.dumps(indicator_chart_images, ensure_ascii=False),
                    # "treatment_gantt_chart_url": treatment_gantt_chart_url or "",  # å·²ç§»é™¤ï¼šä¸å†ç”Ÿæˆç”˜ç‰¹å›¾å›¾ç‰‡
                    "treatment_gantt_data": treatment_gantt_data_json  # æ²»ç–—ç”˜ç‰¹å›¾æ•°æ®ï¼ˆsource_file å·²æ›¿æ¢ä¸ºæ–‡ä»¶åï¼‰ï¼ŒPPTæ¨¡æ¿ä¼šç”¨æ­¤æ•°æ®è‡ªè¡Œç”Ÿæˆç”˜ç‰¹å›¾
                }
            else:
                # æœ¬åœ°æ¨¡å¼
                logger.info(f"Retrieving medical template information for: {template_id}")
                template_info = get_template_by_id(template_id)

                if not template_info:
                    logger.error(f"Failed to retrieve template: {template_id}")
                    return {"success": False, "error": f"Template not found: {template_id}"}

                logger.info(f"Successfully retrieved template: {template_info.get('name')}")

                try:
                    template_info_json = json.dumps(template_info, ensure_ascii=False, default=str)
                except Exception as e:
                    logger.error(f"æ¨¡æ¿ä¿¡æ¯JSONåºåˆ—åŒ–å¤±è´¥: {str(e)}")
                    return {"success": False, "error": f"æ¨¡æ¿ä¿¡æ¯åºåˆ—åŒ–å¤±è´¥: {str(e)}"}

                ppt_inputs = {
                    "current_date": current_date,
                    "patient_structured_data": patient_timeline_json,
                    "patient_timeline": patient_timeline_json,
                    "raw_files_data": processed_files_json,
                    "template_info": template_info_json,
                    "session_id": agent_session_id,
                    "patient_name": patient_name,
                    "patient_journey_image_url": patient_journey_image_url or "",
                    "patient_journey_image_path": patient_journey_image_path,
                    "indicator_chart_images": json.dumps(indicator_chart_images, ensure_ascii=False),
                    # "treatment_gantt_chart_url": treatment_gantt_chart_url or "",  # å·²ç§»é™¤ï¼šä¸å†ç”Ÿæˆç”˜ç‰¹å›¾å›¾ç‰‡
                    # "treatment_gantt_chart_path": treatment_gantt_chart_path,  # å·²ç§»é™¤ï¼šä¸å†ç”Ÿæˆç”˜ç‰¹å›¾å›¾ç‰‡
                    "treatment_gantt_data": treatment_gantt_data_json  # æ²»ç–—ç”˜ç‰¹å›¾æ•°æ®ï¼ˆsource_file å·²æ›¿æ¢ä¸ºæ–‡ä»¶åï¼‰ï¼ŒPPTæ¨¡æ¿ä¼šç”¨æ­¤æ•°æ®è‡ªè¡Œç”Ÿæˆç”˜ç‰¹å›¾
                }

            # æ ¹æ®æ¨¡å¼é€‰æ‹©æ‰§è¡Œä¸åŒçš„ä»»åŠ¡
            mode_name = "Suvalue API" if self._use_suvalue_api else "Local python-pptx"
            workflow_name = "Agentæµç¨‹" if self._use_agent_workflow else "ç›´æ¥LLMè°ƒç”¨"
            logger.info(f"Starting PPT generation task ({mode_name} mode, {workflow_name})")

            # ğŸ†• åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ç›´æ¥LLMè°ƒç”¨æµç¨‹ï¼ˆä»…Suvalue APIæ¨¡å¼æ”¯æŒï¼‰
            if self._use_suvalue_api and not self._use_agent_workflow:
                logger.info("=" * 80)
                logger.info("ä½¿ç”¨ç›´æ¥LLMè°ƒç”¨æµç¨‹ç”ŸæˆPPTï¼ˆç»•è¿‡Agentï¼‰")
                logger.info("=" * 80)

                # ========== æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†å—è¾“å‡º ==========
                model_name = 'gemini-3-flash-preview'

                # ğŸ†• ä¼˜å…ˆä½¿ç”¨ä¸»å¼€å…³ ENABLE_NEW_FEATURESï¼Œå¦‚æœæœªè®¾ç½®åˆ™ä½¿ç”¨ ENABLE_CHUNKED_OUTPUT
                enable_new_features = os.getenv('ENABLE_NEW_FEATURES', '').lower()

                if enable_new_features in ('true', '1', 'yes'):
                    # ä¸»å¼€å…³å¯ç”¨ - å¯ç”¨åˆ†å—è¾“å‡º
                    use_chunked_output = True
                    logger.info("âœ… ä¸»å¼€å…³å·²å¯ç”¨ (ENABLE_NEW_FEATURES=true)ï¼Œå°†ä½¿ç”¨åˆ†å—è¾“å‡º")
                elif enable_new_features in ('false', '0', 'no'):
                    # ä¸»å¼€å…³ç¦ç”¨ - ä½¿ç”¨åŸæœ‰é€»è¾‘
                    use_chunked_output = False
                    logger.info("â„¹ï¸ ä¸»å¼€å…³å·²ç¦ç”¨ (ENABLE_NEW_FEATURES=false)ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘")
                else:
                    # æœªè®¾ç½®ä¸»å¼€å…³ - ä½¿ç”¨ç»†ç²’åº¦æ§åˆ¶
                    enable_chunked_output = os.getenv('ENABLE_CHUNKED_OUTPUT', 'false').lower()

                    use_chunked_output = False
                    if enable_chunked_output == 'true' or enable_chunked_output == '1' or enable_chunked_output == 'yes':
                        # å¼ºåˆ¶å¯ç”¨åˆ†å—è¾“å‡º
                        use_chunked_output = True
                        logger.info("â„¹ï¸ åˆ†å—è¾“å‡ºå·²å¼ºåˆ¶å¯ç”¨ï¼ˆENABLE_CHUNKED_OUTPUT=trueï¼‰")
                    elif enable_chunked_output == 'false' or enable_chunked_output == '0' or enable_chunked_output == 'no':
                        # å¼ºåˆ¶ç¦ç”¨åˆ†å—è¾“å‡º
                        use_chunked_output = False
                        logger.info("â„¹ï¸ åˆ†å—è¾“å‡ºå·²ç¦ç”¨ï¼ˆENABLE_CHUNKED_OUTPUT=falseï¼‰ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘")
                    else:
                        # è‡ªåŠ¨æ£€æµ‹ï¼ˆé»˜è®¤è¡Œä¸ºï¼‰
                        # ä¼°ç®—è¾“å‡ºå¤§å°
                        estimated_output_size = output_chunked_generator.estimate_output_size({
                            'patient_timeline': patient_timeline,
                            'raw_files_data': processed_raw_files_data
                        })

                        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†å—è¾“å‡º
                        use_chunked_output = output_chunked_generator.should_use_chunked_output(
                            model_name=model_name,
                            expected_output_size=estimated_output_size
                        )
                        logger.info(f"â„¹ï¸ è‡ªåŠ¨æ£€æµ‹åˆ†å—è¾“å‡ºéœ€æ±‚: {use_chunked_output} (é¢„æœŸè¾“å‡º: {estimated_output_size} tokens)")

                if use_chunked_output:
                    logger.warning("=" * 100)
                    logger.warning(f"âš ï¸ å¯ç”¨åˆ†å—è¾“å‡ºæ¨¡å¼ï¼ˆå¸¦ä¸Šä¸‹æ–‡ä¼ é€’ï¼‰")
                    logger.warning("=" * 100)

                # 1. ä½¿ç”¨LLMç”ŸæˆPPTæ•°æ®
                ppt_data = self._generate_ppt_data_with_llm(
                    patient_timeline=patient_timeline,
                    raw_files_data=processed_raw_files_data,
                    patient_name=patient_name,
                    patient_journey_image_url=patient_journey_image_url,
                    indicator_chart_images=indicator_chart_images,
                    treatment_gantt_chart_url=treatment_gantt_chart_url,
                    treatment_gantt_data=gantt_data if 'gantt_data' in locals() else None,
                    template_type=2,
                    use_chunked_output=use_chunked_output  # ä¼ é€’åˆ†å—è¾“å‡ºæ ‡å¿—
                )

                if not ppt_data:
                    return {"success": False, "error": "LLMç”ŸæˆPPTæ•°æ®å¤±è´¥"}

                # 2. ç›´æ¥è°ƒç”¨å·¥å…·ç”ŸæˆPPT
                logger.info("è°ƒç”¨SuvalueGeneratePPTToolç”ŸæˆPPT...")
                ppt_tool = SuvalueGeneratePPTTool()
                ppt_info = ppt_tool._run(template_type=2, ppt_data=ppt_data)

                if ppt_info and ppt_info.get("success"):
                    logger.info(f"âœ… ç›´æ¥LLMè°ƒç”¨æµç¨‹æˆåŠŸ: ppt_url={ppt_info.get('ppt_url')}")
                    # æ·»åŠ  treatment_gantt_data å’Œ ppt_data åˆ°è¿”å›ç»“æœ
                    ppt_info["treatment_gantt_data"] = gantt_data if 'gantt_data' in locals() else []
                    ppt_info["ppt_data"] = ppt_data
                    return ppt_info
                else:
                    error_msg = ppt_info.get("error", "PPTç”Ÿæˆå¤±è´¥") if ppt_info else "PPTç”Ÿæˆå¤±è´¥"
                    logger.error(f"âŒ ç›´æ¥LLMè°ƒç”¨æµç¨‹å¤±è´¥: {error_msg}")
                    return {"success": False, "error": error_msg}

            # åŸæœ‰çš„Agentæµç¨‹
            if self._use_suvalue_api:
                # Suvalue API æ¨¡å¼ï¼šå•ç‹¬æ‰§è¡Œç‰¹å®šä»»åŠ¡
                logger.info("Step 1: Executing Suvalue PPT transformation and generation task (Agentæµç¨‹)")
                task = self.transform_and_generate_ppt_task()
                task.interpolate_inputs_and_add_conversation_history(ppt_inputs)
                result = self.suvalue_ppt_data_transformer().execute_task(task)
                logger.info("Suvalue PPT generation completed")
            else:
                # æœ¬åœ°æ¨¡å¼ï¼šå•ç‹¬æ‰§è¡Œç‰¹å®šä»»åŠ¡
                logger.info("Step 1: Executing local PPT slides generation task")
                task = self.generate_ppt_slides_task()
                task.interpolate_inputs_and_add_conversation_history(ppt_inputs)
                result = self.ppt_content_generator().execute_task(task)
                logger.info("Local PPT generation completed")

            # è®°å½•åŸå§‹è¿”å›ç»“æœç”¨äºè°ƒè¯•
            logger.info(f"Crewæ‰§è¡Œç»“æœç±»å‹: {type(result)}")
            logger.info(f"Crewæ‰§è¡Œç»“æœå†…å®¹: {str(result)[:500]}")

            # ä»CrewOutputå¯¹è±¡ä¸­æå–ç»“æœ
            ppt_info = None

            if hasattr(result, 'json_dict') and result.json_dict:
                ppt_info = result.json_dict
                logger.info("ä»CrewOutput.json_dictæå–ç»“æœ")
            elif hasattr(result, 'pydantic') and result.pydantic:
                ppt_info = result.pydantic if isinstance(result.pydantic, dict) else result.pydantic.dict()
                logger.info("ä»CrewOutput.pydanticæå–ç»“æœ")
            elif hasattr(result, 'raw'):
                if isinstance(result.raw, dict):
                    ppt_info = result.raw
                    logger.info("ä»CrewOutput.rawæå–ç»“æœï¼ˆå­—å…¸ï¼‰")
                elif isinstance(result.raw, str):
                    # å…ˆå°è¯•ä»æ–‡æœ¬ä¸­æå–JSONï¼ˆå¤„ç†åŒ…å«Thoughtç­‰éJSONæ–‡æœ¬çš„æƒ…å†µï¼‰
                    json_str = JsonUtils.extract_json_from_text(result.raw)
                    if json_str:
                        logger.info("ä»CrewOutput.rawä¸­æå–åˆ°JSONå­—ç¬¦ä¸²")
                        ppt_info = JsonUtils.safe_parse_json(json_str, debug_prefix="PPT generation")
                        if ppt_info:
                            logger.info("ä»CrewOutput.rawæå–ç»“æœï¼ˆJSONè§£æï¼‰")
                    else:
                        # å¦‚æœæå–ä¸åˆ°JSONï¼Œå°è¯•ast.literal_eval
                        import ast
                        try:
                            ppt_info = ast.literal_eval(result.raw)
                            logger.info("ä»CrewOutput.rawæå–ç»“æœï¼ˆä½¿ç”¨ast.literal_evalï¼‰")
                        except (ValueError, SyntaxError) as e:
                            logger.warning(f"ast.literal_evalå¤±è´¥ä¸”æ— æ³•æå–JSON: {e}")
                            logger.warning(f"åŸå§‹å†…å®¹: {result.raw[:500]}")

            if not ppt_info:
                # æœ€åå°è¯•ä»æ•´ä¸ªresultå­—ç¬¦ä¸²ä¸­æå–JSON
                result_str = str(result)
                json_str = JsonUtils.extract_json_from_text(result_str)
                if json_str:
                    logger.info("ä»resultå­—ç¬¦ä¸²ä¸­æå–åˆ°JSON")
                    ppt_info = JsonUtils.safe_parse_json(json_str, debug_prefix="PPT generation")
                else:
                    logger.warning(f"æ— æ³•ä»resultä¸­æå–JSONï¼ŒåŸå§‹å†…å®¹: {result_str[:500]}")

            if ppt_info and ppt_info.get("success"):
                # æ ¹æ®æ¨¡å¼æ·»åŠ é¢å¤–ä¿¡æ¯
                if not self._use_suvalue_api:
                    # æœ¬åœ°æ¨¡å¼ï¼šæ·»åŠ å›¾ç‰‡URLå’Œè·¯å¾„
                    if patient_journey_image_url:
                        ppt_info["patient_journey_image_url"] = patient_journey_image_url
                        logger.info(f"æ‚£è€…æ—¶é—´æ—…ç¨‹å›¾ç‰‡URLå·²æ·»åŠ åˆ°PPTç»“æœ: {patient_journey_image_url}")
                    if patient_journey_image_path:
                        ppt_info["patient_journey_image_path"] = patient_journey_image_path
                        logger.info(f"æ‚£è€…æ—¶é—´æ—…ç¨‹å›¾ç‰‡æœ¬åœ°è·¯å¾„å·²æ·»åŠ åˆ°PPTç»“æœ: {patient_journey_image_path}")
                    if indicator_chart_images:
                        ppt_info["indicator_chart_images"] = indicator_chart_images
                        logger.info(f"æ ¸å¿ƒæŒ‡æ ‡è¶‹åŠ¿å›¾ç‰‡ä¿¡æ¯å·²æ·»åŠ åˆ°PPTç»“æœ: {len(indicator_chart_images)} ä¸ªå›¾ç‰‡")
                    if treatment_gantt_chart_url:
                        ppt_info["treatment_gantt_chart_url"] = treatment_gantt_chart_url
                        logger.info(f"æ²»ç–—ç”˜ç‰¹å›¾URLå·²æ·»åŠ åˆ°PPTç»“æœ: {treatment_gantt_chart_url}")
                    if treatment_gantt_chart_path:
                        ppt_info["treatment_gantt_chart_path"] = treatment_gantt_chart_path
                        logger.info(f"æ²»ç–—ç”˜ç‰¹å›¾æœ¬åœ°è·¯å¾„å·²æ·»åŠ åˆ°PPTç»“æœ: {treatment_gantt_chart_path}")

                    logger.info(f"Local PPTç”ŸæˆæˆåŠŸ: local_path={ppt_info.get('local_path')}, "
                              f"file_uuid={ppt_info.get('file_uuid')}, qiniu_url={ppt_info.get('qiniu_url')}")
                    if not ppt_info.get('file_uuid'):
                        logger.warning("Local PPTç”ŸæˆæˆåŠŸä½†ç¼ºå°‘file_uuidå­—æ®µï¼ˆå¯èƒ½æœªä¸Šä¼ åˆ°ä¸ƒç‰›äº‘ï¼‰")
                else:
                    # Suvalue APIæ¨¡å¼
                    logger.info(f"Suvalue PPTç”ŸæˆæˆåŠŸ: ppt_url={ppt_info.get('ppt_url')}")

                return ppt_info
            else:
                if ppt_info is None or not ppt_info:
                    error_msg = f"æ— æ³•è§£æCrewè¿”å›ç»“æœã€‚åŸå§‹ç»“æœ: {str(result)[:200]}"
                elif not isinstance(ppt_info, dict):
                    error_msg = f"Crewè¿”å›ç»“æœä¸æ˜¯å­—å…¸ç±»å‹: {type(ppt_info)}, å†…å®¹: {str(ppt_info)[:200]}"
                else:
                    error_msg = ppt_info.get("error", f"PPTç”Ÿæˆå¤±è´¥ä½†æœªæä¾›é”™è¯¯ä¿¡æ¯ã€‚è¿”å›å†…å®¹: {str(ppt_info)[:200]}")

                logger.error(f"PPTç”Ÿæˆå¤±è´¥ ({mode_name} mode): {error_msg}")
                return {"success": False, "error": error_msg}

        except Exception as e:
            mode_name = "Suvalue API" if self._use_suvalue_api else "Local python-pptx"
            logger.error(f"Error in PPT generation ({mode_name} mode): {e}", exc_info=True)
            return {"success": False, "error": str(e)}
