import os
from dotenv import load_dotenv
load_dotenv()
from crewai import Agent, Crew, Process, Task, LLM
from crewai.project import CrewBase, agent, crew, task
from src.llms import *
from src.utils.json_utils import JsonUtils
from src.utils.logger import BeijingLogger
from datetime import datetime
import re
import unicodedata
import concurrent.futures
from src.custom_tools.get_disease_list_tool import get_disease_list_tool
from src.custom_tools.query_disease_config_tool import query_disease_config_tool
from pathlib import Path
import time
import json
import uuid as uuid_lib
from src.custom_tools.patient_journey_image_generator import generate_patient_journey_image_sync
from src.custom_tools.indicator_chart_image_generator import generate_indicator_chart_image_sync
from app.utils.qiniu_upload_service import QiniuUploadService
from app.utils.file_metadata_builder import FileMetadataBuilder  # æ–°å¢å¯¼å…¥
from src.utils.data_compressor import PatientDataCompressor  # æ•°æ®å‹ç¼©
from src.utils.token_manager import TokenManager  # Tokenç®¡ç†

# åˆå§‹åŒ– logger
logger = BeijingLogger().get_logger()

@CrewBase
class PatientDataCrew():
    """Patient data processing crew"""

    agents_config = "config/agents.yaml"
    tasks_config = "config/tasks.yaml"
    max_concurrency = 10  # é»˜è®¤æœ€å¤§å¹¶å‘æ•°
    
    def __init__(self, max_concurrency=None):
        """
        åˆå§‹åŒ–PatientDataCrew

        Args:
            max_concurrency (int, optional): æœ€å¤§å¹¶å‘å¤„ç†æ•°ï¼Œé»˜è®¤ä¸ºNoneï¼Œå°†ä½¿ç”¨ç±»å˜é‡é»˜è®¤å€¼
        """
        if max_concurrency is not None:
            self.max_concurrency = max_concurrency

    @staticmethod
    def estimate_tokens(text):
        """
        ä¼°ç®—æ–‡æœ¬ä¸­çš„tokenæ•°é‡ï¼ŒåŸºäºä»¥ä¸‹è§„åˆ™ï¼š
        - è‹±æ–‡å­—ç¬¦: çº¦ 0.3 ä¸ªtoken/å­—ç¬¦
        - ä¸­æ–‡å­—ç¬¦: çº¦ 0.6 ä¸ªtoken/å­—ç¬¦
        - å…¶ä»–å­—ç¬¦: çº¦ 0.5 ä¸ªtoken/å­—ç¬¦ï¼ˆä¿å®ˆä¼°è®¡ï¼‰
        
        Args:
            text (str): è¾“å…¥æ–‡æœ¬
            
        Returns:
            float: ä¼°ç®—çš„tokenæ•°é‡
        """
        if not text:
            return 0
            
        # è®¡æ•°å™¨åˆå§‹åŒ–
        english_chars = 0
        chinese_chars = 0
        other_chars = 0
        
        # éå†æ–‡æœ¬ä¸­çš„æ¯ä¸ªå­—ç¬¦
        for char in text:
            # è·³è¿‡ç©ºç™½å­—ç¬¦
            if char.isspace():
                continue
                
            # æ£€æŸ¥æ˜¯å¦ä¸ºASCIIèŒƒå›´å†…çš„è‹±æ–‡å­—ç¬¦
            if ord(char) < 128 and (char.isalpha() or char.isdigit() or char in ",.!?;:'\"()[]{}"):
                english_chars += 1
            # æ£€æŸ¥æ˜¯å¦ä¸ºä¸­æ–‡å­—ç¬¦
            elif any([
                'CJK' in unicodedata.name(char, ''),
                'HIRAGANA' in unicodedata.name(char, ''),
                'KATAKANA' in unicodedata.name(char, ''),
                'IDEOGRAPHIC' in unicodedata.name(char, '')
            ]):
                chinese_chars += 1
            # å…¶ä»–å­—ç¬¦
            else:
                other_chars += 1
        
        # æ ¹æ®ä¸åŒå­—ç¬¦ç±»å‹è®¡ç®—tokenä¼°ç®—å€¼
        estimated_tokens = (
            english_chars * 0.3 +  # è‹±æ–‡å­—ç¬¦
            chinese_chars * 0.6 +  # ä¸­æ–‡å­—ç¬¦
            other_chars * 0.5      # å…¶ä»–å­—ç¬¦
        )
        
        # ä¿å®ˆèµ·è§ï¼Œå‘ä¸Šå–æ•´å¹¶æ·»åŠ ä¸€ç‚¹é¢å¤–ç¼“å†²
        return int(estimated_tokens * 1.1) + 1

    def _save_patient_data_to_output(self, session_id, patient_content, full_structure_data, patient_journey=None, mdt_simple_report=None):
        """å°†æ‚£è€…æ•°æ®ä¿å­˜åˆ°è¾“å‡ºç›®å½•"""
        try:
            if not session_id:
                logger.warning("No session_id provided, skipping patient data save")
                return None
            
            # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„ï¼ˆä¸intent_determine_crewç›¸åŒçš„ç›®å½•ç»“æ„ï¼‰
            output_dir = Path("output/files_extract") / session_id
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # ç¡®ä¿æ•°æ®ä¸­çš„Unicodeç¼–ç è¢«æ­£ç¡®è§£ç 
            def decode_unicode_recursive(obj):
                """é€’å½’è§£ç å¯¹è±¡ä¸­çš„Unicodeè½¬ä¹‰åºåˆ—"""
                if isinstance(obj, dict):
                    return {key: decode_unicode_recursive(value) for key, value in obj.items()}
                elif isinstance(obj, list):
                    return [decode_unicode_recursive(item) for item in obj]
                elif isinstance(obj, str):
                    try:
                        # å¤„ç†Unicodeè½¬ä¹‰åºåˆ—
                        if '\\u' in obj:
                            return obj.encode().decode('unicode_escape')
                        return obj
                    except Exception:
                        return obj
                else:
                    return obj
            
            # ğŸš¨ ç®€åŒ–ï¼šç›´æ¥å‡†å¤‡è¦ä¿å­˜çš„æ•°æ®ï¼Œä¸éœ€è¦å¤æ‚çš„å†å²è®°å½•
            patient_data = {
                "session_id": session_id,
                "timestamp": time.time(),
                "processing_date": datetime.now().isoformat(),
                "patient_content": decode_unicode_recursive(patient_content) if isinstance(patient_content, str) else patient_content,
                "full_structure_data": decode_unicode_recursive(full_structure_data),
                "patient_journey": decode_unicode_recursive(patient_journey) if patient_journey is not None else None,
                "mdt_simple_report": decode_unicode_recursive(mdt_simple_report) if mdt_simple_report is not None else None
            }
            
            # å¦‚æœå·²å­˜åœ¨æ–‡ä»¶ï¼Œå…ˆå¤‡ä»½
            output_file = output_dir / "patient_data.json"
            if output_file.exists():
                # åˆ›å»ºå¤‡ä»½æ–‡ä»¶
                backup_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                backup_file = output_dir / f"patient_data_backup_{backup_timestamp}.json"
                import shutil
                shutil.copy2(output_file, backup_file)
                logger.info(f"å·²åˆ›å»ºå¤‡ä»½æ–‡ä»¶: {backup_file}")
            
            # ä¿å­˜åˆ°JSONæ–‡ä»¶
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(patient_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æ‚£è€…æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
            
            return str(output_file)
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ‚£è€…æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            return None
    
    @agent
    def file_preprocessor(self) -> Agent:
        return Agent(
            config=self.agents_config['file_preprocessor'],
            llm=general_llm,
            verbose=True
        )

    @agent
    def disease_config_agent(self) -> Agent:
        return Agent(
            config=self.agents_config['disease_config_agent'],
            tools=[get_disease_list_tool, query_disease_config_tool],
            llm=document_generation_llm,
            verbose=True
        )

    @agent
    def patient_data_processor(self) -> Agent:
        return Agent(
            config=self.agents_config['patient_data_processor'],
            llm=document_generation_llm,
            verbose=True
        )

    @agent
    def timeline_summary_generator(self) -> Agent:
        return Agent(
            config=self.agents_config['timeline_summary_generator'],
            llm=document_generation_llm,
            verbose=True
        )

    @agent
    def timeline_details_generator(self) -> Agent:
        return Agent(
            config=self.agents_config['timeline_details_generator'],
            llm=document_generation_llm,
            verbose=True
        )

    @agent
    def core_points_extractor(self) -> Agent:
        return Agent(
            config=self.agents_config['core_points_extractor'],
            llm=document_generation_llm,
            verbose=True
        )

    @agent
    def patient_journey_extractor(self) -> Agent:
        return Agent(
            config=self.agents_config['patient_journey_extractor'],
            llm=document_generation_llm,
            verbose=True
        )

    @agent
    def patient_journey_summary_generator(self) -> Agent:
        return Agent(
            config=self.agents_config['patient_journey_summary_generator'],
            llm=document_generation_llm,
            verbose=True
        )

    @agent
    def patient_journey_details_generator(self) -> Agent:
        return Agent(
            config=self.agents_config['patient_journey_details_generator'],
            llm=document_generation_llm,
            verbose=True
        )

    @agent
    def indicator_series_extractor(self) -> Agent:
        return Agent(
            config=self.agents_config['indicator_series_extractor'],
            llm=document_generation_llm,
            verbose=True
        )

    @agent
    def mdt_report_generator(self) -> Agent:
        return Agent(
            config=self.agents_config['mdt_report_generator'],
            llm=document_generation_llm,
            verbose=True
        )
    
    @task
    def preprocess_files_task(self) -> Task:
        return Task(
            config=self.tasks_config['preprocess_files_task']
        )

    @task
    def get_disease_config_task(self) -> Task:
        return Task(
            config=self.tasks_config['get_disease_config_task']
        )

    @task
    def process_patient_data_task(self) -> Task:
        return Task(
            config=self.tasks_config['process_patient_data_task'],
            context=[self.get_disease_config_task()]  # ä¾èµ–ç–¾ç—…é…ç½®ä»»åŠ¡çš„è¾“å‡º
        )

    @task
    def generate_timeline_summary_task(self) -> Task:
        return Task(
            config=self.tasks_config['generate_timeline_summary_task'],
            context=[self.get_disease_config_task()]  # ä¾èµ–ç–¾ç—…é…ç½®ä»»åŠ¡çš„è¾“å‡º
        )

    @task
    def generate_timeline_details_task(self) -> Task:
        return Task(
            config=self.tasks_config['generate_timeline_details_task'],
            context=[self.get_disease_config_task()]  # ä¾èµ–ç–¾ç—…é…ç½®ä»»åŠ¡çš„è¾“å‡º
        )

    @task
    def extract_core_points_task(self) -> Task:
        return Task(
            config=self.tasks_config['extract_core_points_task'],
            context=[self.get_disease_config_task()]  # ä¾èµ–ç–¾ç—…é…ç½®ä»»åŠ¡çš„è¾“å‡º
        )

    @task
    def extract_patient_journey_task(self) -> Task:
        return Task(
            config=self.tasks_config['extract_patient_journey_task'],
            context=[self.get_disease_config_task()]  # ä¾èµ–ç–¾ç—…é…ç½®ä»»åŠ¡çš„è¾“å‡º
        )

    @task
    def generate_patient_journey_summary_task(self) -> Task:
        return Task(
            config=self.tasks_config['generate_patient_journey_summary_task'],
            context=[self.get_disease_config_task()]  # ä¾èµ–ç–¾ç—…é…ç½®ä»»åŠ¡çš„è¾“å‡º
        )

    @task
    def generate_patient_journey_details_task(self) -> Task:
        return Task(
            config=self.tasks_config['generate_patient_journey_details_task'],
            context=[self.get_disease_config_task()]  # ä¾èµ–ç–¾ç—…é…ç½®ä»»åŠ¡çš„è¾“å‡º
        )

    @task
    def extract_indicator_series_task(self) -> Task:
        return Task(
            config=self.tasks_config['extract_indicator_series_task'],
            context=[self.get_disease_config_task()]  # ä¾èµ–ç–¾ç—…é…ç½®ä»»åŠ¡çš„è¾“å‡º
        )

    @task
    def generate_mdt_report_task(self) -> Task:
        return Task(
            config=self.tasks_config['generate_mdt_report_task'],
            context=[self.get_disease_config_task()]  # ä¾èµ–ç–¾ç—…é…ç½®ä»»åŠ¡çš„è¾“å‡º
        )

    @crew
    def crew(self) -> Crew:
        """Creates the patient data processing crew with 30-minute timeout"""
        return Crew(
            agents=self.agents,
            tasks=self.tasks,
            process=Process.sequential,
            verbose=True,
            max_execution_time=1800  # 30 minutes timeout (30 * 60 = 1800 seconds)
        )

    def get_structured_patient_data(self, patient_info, patient_timeline, messages, files, agent_session_id, existing_patient_data=None):
        """
        Process patient information into a structured timeline with detailed categorized information.
        æ”¯æŒå¢é‡æ›´æ–°ï¼šå¦‚æœå­˜åœ¨ç°æœ‰æ‚£è€…æ•°æ®ï¼Œå°†æ–°ä¿¡æ¯ä¸ç°æœ‰ä¿¡æ¯åˆå¹¶æ›´æ–°ã€‚

        Args:
            patient_info (str): Raw patient information text
            patient_timeline (str): Current patient timeline (may be empty for new patients)
            messages (list): Conversation messages history
            files (list): List of file objects with their content
            agent_session_id (str): The session ID for the agent
            existing_patient_data (dict): ç°æœ‰æ‚£è€…æ•°æ®ï¼ŒåŒ…å«timelineã€journeyã€mdt_reportç­‰
        Returns:
            dict: Structured patient data with timeline and categorized details
        """
        # å°†ç”Ÿæˆå™¨ç‰ˆæœ¬çš„ç»“æœæ”¶é›†èµ·æ¥è¿”å›
        result = None
        for progress_data in self.get_structured_patient_data_stream(
            patient_info=patient_info,
            patient_timeline=patient_timeline,
            messages=messages,
            files=files,
            agent_session_id=agent_session_id,
            existing_patient_data=existing_patient_data
        ):
            if progress_data.get("type") == "result":
                result = progress_data.get("data")
        return result if result else {"error": "No result returned"}

    def get_structured_patient_data_stream(self, patient_info, patient_timeline, messages, files, agent_session_id, existing_patient_data=None):
        """
        Process patient information into a structured timeline (generator version).
        å®æ—¶è¿”å›å¤„ç†è¿›åº¦å’Œæœ€ç»ˆç»“æœã€‚

        Args:
            patient_info (str): Raw patient information text
            patient_timeline (str): Current patient timeline (may be empty for new patients)
            messages (list): Conversation messages history
            files (list): List of file objects with their content
            agent_session_id (str): The session ID for the agent
            existing_patient_data (dict): ç°æœ‰æ‚£è€…æ•°æ®ï¼ŒåŒ…å«timelineã€journeyã€mdt_reportç­‰

        Yields:
            dict: Progress updates or final result
                - type: "progress" or "result"
                - For progress: stage, message, progress (0-100)
                - For result: data (final result dict)
        """
        try:
            # ========== æ€»ä½“å¼€å§‹æ—¶é—´ ==========
            overall_start_time = time.time()
            logger.info("=" * 80)
            logger.info("å¼€å§‹æ‚£è€…æ•°æ®å¤„ç†æµç¨‹")
            logger.info("=" * 80)
            
            # è®¾ç½®å½“å‰æ—¥æœŸ
            current_date = datetime.now().strftime("%Y-%m-%d")

            # ğŸ†• åˆå§‹åŒ–æ•°æ®å‹ç¼©å·¥å…·ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰
            enable_compression = os.getenv('ENABLE_DATA_COMPRESSION', 'false').lower() in ('true', '1', 'yes')

            if enable_compression:
                logger.info("âœ… æ•°æ®å‹ç¼©åŠŸèƒ½å·²å¯ç”¨ (ENABLE_DATA_COMPRESSION=true)")
            else:
                logger.info("â„¹ï¸ æ•°æ®å‹ç¼©åŠŸèƒ½æœªå¯ç”¨ï¼Œå¯é€šè¿‡ ENABLE_DATA_COMPRESSION=true å¯ç”¨")

            token_manager = None
            data_compressor = None

            if enable_compression:
                token_manager = TokenManager(logger=logger)
                data_compressor = PatientDataCompressor(logger=logger, token_manager=token_manager)
                logger.info("âœ… å·²åˆå§‹åŒ–æ•°æ®å‹ç¼©å·¥å…·")
            else:
                logger.info("â„¹ï¸ æ•°æ®å‹ç¼©å·¥å…·æœªåˆå§‹åŒ–")

            # ğŸš¨ ä¿®æ”¹ï¼šä½¿ç”¨ä¼ å…¥çš„existing_patient_dataå‚æ•°è€Œä¸æ˜¯ä»æœ¬åœ°æ–‡ä»¶åŠ è½½
            existing_timeline = None
            existing_patient_journey = None
            existing_mdt_report = None

            if existing_patient_data:
                logger.info("Found existing patient data from database, will perform incremental update")

                # å®‰å…¨åœ°è·å–ç°æœ‰æ•°æ®ï¼Œå¤„ç†å¯èƒ½ä¸ºNoneçš„æƒ…å†µ
                patient_timeline_data = existing_patient_data.get("patient_timeline")
                existing_timeline = patient_timeline_data.get("timeline", []) if patient_timeline_data else []

                existing_patient_journey = existing_patient_data.get("patient_journey")
                if existing_patient_journey is None:
                    existing_patient_journey = {}

                # ğŸš¨ ä¿®å¤ï¼šå¦‚æœ existing_patient_journey æ˜¯åˆ—è¡¨æ ¼å¼ï¼ˆå†å²æ•°æ®æ ¼å¼é”™è¯¯ï¼‰ï¼Œä¿®æ­£ä¸ºå­—å…¸æ ¼å¼
                if isinstance(existing_patient_journey, list):
                    logger.warning(f"âš ï¸ ä»æ•°æ®åº“è¯»å–çš„ existing_patient_journey æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œå°†å…¶ä¿®æ­£ä¸ºå­—å…¸æ ¼å¼")
                    existing_patient_journey = {
                        "timeline_journey": existing_patient_journey,
                        "indicator_series": []
                    }
                elif isinstance(existing_patient_journey, dict):
                    # ç¡®ä¿åŒ…å«å¿…éœ€çš„å­—æ®µ
                    if "timeline_journey" not in existing_patient_journey:
                        existing_patient_journey["timeline_journey"] = []
                        logger.warning(f"âš ï¸ existing_patient_journey ç¼ºå°‘ timeline_journey å­—æ®µï¼Œå·²æ·»åŠ ç©ºæ•°ç»„")
                    if "indicator_series" not in existing_patient_journey:
                        existing_patient_journey["indicator_series"] = []
                        logger.warning(f"âš ï¸ existing_patient_journey ç¼ºå°‘ indicator_series å­—æ®µï¼Œå·²æ·»åŠ ç©ºæ•°ç»„")

                existing_mdt_report = existing_patient_data.get("mdt_simple_report")
                if existing_mdt_report is None:
                    existing_mdt_report = {}

                logger.info(f"Existing data contains {len(existing_timeline)} timeline entries")

                # è®°å½•ç°æœ‰æ•°æ®çš„è¯¦ç»†ä¿¡æ¯
                if existing_patient_journey and "timeline_journey" in existing_patient_journey:
                    logger.info(f"Existing patient journey contains {len(existing_patient_journey['timeline_journey'])} journey events")
                if existing_patient_journey and "indicator_series" in existing_patient_journey:
                    logger.info(f"Existing patient journey contains {len(existing_patient_journey['indicator_series'])} indicator series")
                if existing_mdt_report:
                    logger.info(f"Existing MDT report contains data")
            else:
                logger.info("No existing patient data found, will create new patient record")
            
            # ========== é˜¶æ®µ1: æ–‡ä»¶é¢„å¤„ç† ==========
            file_preprocessing_start_time = time.time()
            logger.info("-" * 80)
            logger.info("ã€é˜¶æ®µ1ã€‘å¼€å§‹æ–‡ä»¶é¢„å¤„ç†")
            logger.info("-" * 80)

            # å‘é€è¿›åº¦æ›´æ–°
            yield {"type": "progress", "stage": "file_preprocessing", "message": "æ­£åœ¨é¢„å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶", "progress": 10}

            # ç¡®å®šæ˜¯å¦éœ€è¦æ–‡ä»¶é¢„å¤„ç†
            if not files or len(files) == 0:
                logger.info("No files to process, skipping file preprocessing step")
                # å°†messagesè½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼
                if isinstance(messages, list):
                    messages_text = "\n".join([str(msg) for msg in messages if msg])
                    preprocessed_info = f"{patient_info}\n\nå¯¹è¯å†å²:\n{messages_text}" if messages_text else patient_info
                else:
                    preprocessed_info = str(messages) if messages else patient_info
            else:
                # è¿‡æ»¤æ–‡ä»¶ï¼šè·³è¿‡ä»PDFæå–çš„å›¾ç‰‡ï¼Œé¿å…é‡å¤
                # å› ä¸ºPDFçš„extracted_textå·²ç»åŒ…å«äº†å›¾ç‰‡æè¿°
                filtered_files = FileMetadataBuilder.filter_for_llm_input(files)
                logger.info(f"è¿‡æ»¤åç”¨äºLLMçš„æ–‡ä»¶æ•°: {len(filtered_files)} (åŸå§‹: {len(files)})")

                # é¦–å…ˆè®¡ç®—æ‰€æœ‰æ–‡ä»¶çš„æ€»tokenæ•°
                total_file_tokens = 0
                valid_file_count = 0
                for file in filtered_files:
                    # ä¼˜å…ˆä½¿ç”¨file_contentï¼Œå…¼å®¹extracted_text
                    file_content = file.get('file_content') or file.get('extracted_text', '')
                    # åªè®¡ç®—æœ‰å†…å®¹æ–‡ä»¶çš„tokenæ•°
                    if file_content and file_content.strip():
                        file_tokens = self.estimate_tokens(file_content)
                        total_file_tokens += file_tokens
                        valid_file_count += 1
                    else:
                        logger.warning(f"Skipping empty file in token calculation: {file.get('file_name', 'æœªå‘½åæ–‡ä»¶')}")

                logger.info(f"Total tokens for {valid_file_count} valid files: {total_file_tokens} (out of {len(filtered_files)} total files)")

                # å¦‚æœæ€»tokenæ•°ä¸è¶…è¿‡50000ï¼Œè·³è¿‡æ–‡ä»¶é¢„å¤„ç†æ­¥éª¤
                if total_file_tokens <= 50000:
                    logger.info("Files token count doesn't exceed 50000, skipping file preprocessing step")
                    # ç›´æ¥åˆå¹¶æ‰€æœ‰æ–‡ä»¶å†…å®¹
                    files_content = []
                    for file in filtered_files:
                        file_name = file.get('file_name', 'æœªå‘½åæ–‡ä»¶')
                        # ä¼˜å…ˆä½¿ç”¨file_contentï¼Œå…¼å®¹extracted_text
                        file_content = file.get('file_content') or file.get('extracted_text', '')
                        file_uuid = file.get('file_uuid', '')

                        # åªå¤„ç†æœ‰å†…å®¹çš„æ–‡ä»¶ï¼Œè·³è¿‡ç©ºå†…å®¹æ–‡ä»¶
                        if file_content and file_content.strip():
                            files_content.append(f"æ–‡ä»¶UUID: {file_uuid}\nå†…å®¹:\n{file_content}")
                            logger.info(f"Added file content: {file_name} (UUID: {file_uuid}) ({len(file_content)} chars)")
                        else:
                            logger.warning(f"Skipping file with empty content: {file_name} (UUID: {file_uuid})")

                    if files_content:
                        preprocessed_info = f"{patient_info}\n\næ–‡ä»¶æå–çš„æ‚£è€…ä¿¡æ¯:\n" + "\n\n".join(files_content)
                    else:
                        logger.info("No valid file content found, using only patient_info and messages")
                        preprocessed_info = patient_info
                else:
                    logger.info(f"Preprocessing {len(filtered_files)} files (total tokens: {total_file_tokens})")
                    
                    # æ–‡ä»¶é¢„å¤„ç†æ­¥éª¤
                    preprocessed_info = patient_info
                    
                    # å‡†å¤‡æ–‡ä»¶é¢„å¤„ç†ä»»åŠ¡çš„è¾“å…¥
                    max_tokens_per_batch = 88000 # æ¯æ‰¹æ¬¡å¤„ç†çš„æœ€å¤§tokenæ•° ï¼Œç°åœ¨ç”¨qwen 128k çš„æ¨¡å‹ï¼Œæ‰€ä»¥è®¾ç½®ä¸º75000
                    max_tokens_per_chunk = 88000  # å•ä¸ªæ–‡ä»¶å—çš„æœ€å¤§tokenæ•°ï¼Œç°åœ¨ç”¨qwen 128k çš„æ¨¡å‹ï¼Œæ‰€ä»¥è®¾ç½®ä¸º75000

                    # éå†å¤„ç†æ‰€æœ‰æ–‡ä»¶ï¼Œæ”¶é›†æ‰€æœ‰æ‰¹æ¬¡
                    all_batches = []
                    current_batch = []
                    current_batch_tokens = 0

                    # ä½¿ç”¨è¿‡æ»¤åçš„æ–‡ä»¶åˆ—è¡¨
                    for file in filtered_files:
                        file_name = file.get('file_name', 'æœªå‘½åæ–‡ä»¶')
                        # ä¼˜å…ˆä½¿ç”¨file_contentï¼Œå…¼å®¹extracted_text
                        file_content = file.get('file_content') or file.get('extracted_text', '')
                        file_uuid = file.get('file_uuid', '')

                        # è·³è¿‡ç©ºå†…å®¹æ–‡ä»¶
                        if not file_content or not file_content.strip():
                            logger.warning(f"Skipping file with empty content during preprocessing: {file_name} (UUID: {file_uuid})")
                            continue
                            
                        file_tokens = self.estimate_tokens(file_content)
                        
                        logger.info(f"File '{file_name}' (UUID: {file_uuid}): {len(file_content)} chars, estimated {file_tokens} tokens")
                        
                        # å¤„ç†å¤§æ–‡ä»¶ - åˆ‡åˆ†ä¸ºå¤šä¸ªå—
                        if file_tokens > max_tokens_per_chunk:
                            logger.info(f"Splitting large file '{file_name}' ({file_tokens} tokens) into chunks")
                            
                            # ä¼°è®¡æ¯ä¸ªå­—ç¬¦çš„å¹³å‡tokenæ•°
                            avg_tokens_per_char = file_tokens / len(file_content) if len(file_content) > 0 else 0.5
                            # ä¼°ç®—æ¯ä¸ªå—çš„æœ€å¤§å­—ç¬¦æ•°
                            chars_per_chunk = int(max_tokens_per_chunk / avg_tokens_per_char) if avg_tokens_per_char > 0 else 20000
                            
                            # è®¡ç®—éœ€è¦çš„å—æ•°
                            num_chunks = (len(file_content) + chars_per_chunk - 1) // chars_per_chunk
                            
                            for chunk_idx in range(num_chunks):
                                start_pos = chunk_idx * chars_per_chunk
                                end_pos = min((chunk_idx + 1) * chars_per_chunk, len(file_content))
                                chunk_content = file_content[start_pos:end_pos]
                                chunk_tokens = self.estimate_tokens(chunk_content)
                                
                                logger.info(f"  Chunk {chunk_idx+1}/{num_chunks}: {len(chunk_content)} chars, estimated {chunk_tokens} tokens")
                                
                                # æ£€æŸ¥å½“å‰æ‰¹æ¬¡æ˜¯å¦ä¼šè¶…å‡ºtokené™åˆ¶
                                if current_batch_tokens + chunk_tokens > max_tokens_per_batch:
                                    # æ·»åŠ å½“å‰æ‰¹æ¬¡åˆ°æ‰€æœ‰æ‰¹æ¬¡åˆ—è¡¨
                                    if current_batch:
                                        all_batches.append(list(current_batch))
                                    
                                    # é‡ç½®æ‰¹æ¬¡
                                    current_batch = []
                                    current_batch_tokens = 0
                                
                                # æ·»åŠ æ–‡ä»¶å—åˆ°å½“å‰æ‰¹æ¬¡
                                current_batch.append({
                                    "file_name": f"{file_name} (Part {chunk_idx+1}/{num_chunks})",
                                    "file_content": chunk_content,
                                    "file_uuid": file_uuid
                                })
                                current_batch_tokens += chunk_tokens
                        else:
                            # å¤„ç†æ ‡å‡†å¤§å°æ–‡ä»¶
                            # æ£€æŸ¥å½“å‰æ‰¹æ¬¡æ˜¯å¦ä¼šè¶…å‡ºtokené™åˆ¶
                            if current_batch_tokens + file_tokens > max_tokens_per_batch:
                                # æ·»åŠ å½“å‰æ‰¹æ¬¡åˆ°æ‰€æœ‰æ‰¹æ¬¡åˆ—è¡¨
                                if current_batch:
                                    all_batches.append(list(current_batch))
                                
                                # é‡ç½®æ‰¹æ¬¡
                                current_batch = []
                                current_batch_tokens = 0
                            
                            # æ·»åŠ åˆ°å½“å‰æ‰¹æ¬¡
                            current_batch.append({
                                "file_name": file_name,
                                "file_content": file_content,
                                "file_uuid": file_uuid
                            })
                            current_batch_tokens += file_tokens
                    
                    # æ·»åŠ æœ€åä¸€ä¸ªæ‰¹æ¬¡
                    if current_batch:
                        all_batches.append(list(current_batch))
                    
                    logger.info(f"Prepared {len(all_batches)} batches for processing")
                    
                    # ä½¿ç”¨å¹¶å‘å¤„ç†æ‰¹æ¬¡
                    all_preprocessed_content = []
                    all_batch_inputs = []
                    
                    # å®šä¹‰æ‰¹æ¬¡å¤„ç†å‡½æ•°
                    def process_batch(batch):
                        batch_input = {
                            "files_batch": batch,
                            "patient_info": patient_info,
                            "current_date": current_date
                        }
                        all_batch_inputs.append(batch_input)  # ä¿å­˜batch_input
                        self.preprocess_files_task().interpolate_inputs_and_add_conversation_history(batch_input)
                        return self.file_preprocessor().execute_task(self.preprocess_files_task())
                    
                    # è·å–æœ€å¤§å¹¶å‘æ•°
                    max_concurrent = min(self.max_concurrency, len(all_batches))
                    logger.info(f"Processing {len(all_batches)} batches with maximum {max_concurrent} concurrent workers")
                    
                    # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¹¶å‘å¤„ç†
                    completed_batches = 0
                    with concurrent.futures.ThreadPoolExecutor(max_workers=max_concurrent) as executor:
                        # æäº¤æ‰€æœ‰æ‰¹æ¬¡å¤„ç†ä»»åŠ¡
                        future_to_batch = {executor.submit(process_batch, batch): i for i, batch in enumerate(all_batches)}

                        # æ”¶é›†æ‰€æœ‰ç»“æœ
                        for future in concurrent.futures.as_completed(future_to_batch):
                            batch_idx = future_to_batch[future]
                            try:
                                result = future.result()
                                all_preprocessed_content.append(result)
                                completed_batches += 1
                                logger.info(f"Completed processing batch {batch_idx+1}/{len(all_batches)}")

                                # å‘é€æ–‡ä»¶æ‰¹æ¬¡å¤„ç†è¿›åº¦ï¼ˆ10-30%ä¹‹é—´ï¼‰
                                batch_progress = 10 + int(20 * completed_batches / len(all_batches))
                                yield {"type": "progress", "stage": "file_preprocessing", "message": f"æ­£åœ¨å¤„ç†æ–‡ä»¶æ‰¹æ¬¡ {completed_batches}/{len(all_batches)}", "progress": batch_progress}
                            except Exception as e:
                                logger.error(f"Error processing batch {batch_idx+1}: {str(e)}")
                    
                    # åˆå¹¶æ‰€æœ‰é¢„å¤„ç†ç»“æœ
                    if all_preprocessed_content:
                        # ä¿å­˜é¢„å¤„ç†ç»“æœåˆ°æœ¬åœ°æ–‡ä»¶
                        try:
                            # ç¡®ä¿ç›®å½•å­˜åœ¨
                            log_dir = "logs/patient_data_preprocessed"
                            os.makedirs(log_dir, exist_ok=True)
                            
                            # åˆ›å»ºå¸¦æœ‰æ—¶é—´æˆ³çš„æ–‡ä»¶å
                            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                            
                            # ä¿å­˜é¢„å¤„ç†è¾“å‡ºç»“æœ
                            output_filename = f"{log_dir}/patient_data_preprocessed_{timestamp}.json"
                            # å‡†å¤‡JSONæ•°æ®
                            output_json_data = {
                                "timestamp": datetime.now().isoformat(),
                                "content_count": len(all_preprocessed_content),
                                "preprocessed_content": all_preprocessed_content
                            }
                            # å†™å…¥JSONæ–‡ä»¶
                            JsonUtils.safe_json_dump(output_json_data, output_filename)
                            
                            # ä¿å­˜é¢„å¤„ç†è¾“å…¥æ•°æ®
                            input_filename = f"{log_dir}/patient_data_input_{timestamp}.json"
                            # å‡†å¤‡JSONæ•°æ®
                            input_json_data = {
                                "timestamp": datetime.now().isoformat(),
                                "batch_count": len(all_batch_inputs),
                                "batch_inputs": all_batch_inputs
                            }
                            # å†™å…¥JSONæ–‡ä»¶
                            JsonUtils.safe_json_dump(input_json_data, input_filename)
                            
                            logger.info(f"Preprocessed content saved to {output_filename}")
                            logger.info(f"Batch input data saved to {input_filename}")
                        except Exception as e:
                            logger.error(f"Failed to save preprocessed content: {e}")
                        
                        preprocessed_info = f"{patient_info}\n\næ–‡ä»¶æå–çš„æ‚£è€…ä¿¡æ¯:\n" + "\n\n".join(all_preprocessed_content)
                        preprocessed_tokens = self.estimate_tokens(preprocessed_info)
                        logger.info(f"Files preprocessing completed, combined result: {len(preprocessed_info)} chars, estimated {preprocessed_tokens} tokens")

            # è®°å½•æ–‡ä»¶é¢„å¤„ç†è€—æ—¶
            file_preprocessing_duration = time.time() - file_preprocessing_start_time
            logger.info("-" * 80)
            logger.info(f"ã€é˜¶æ®µ1ã€‘æ–‡ä»¶é¢„å¤„ç†å®Œæˆï¼Œè€—æ—¶: {file_preprocessing_duration:.2f} ç§’ ({file_preprocessing_duration/60:.2f} åˆ†é’Ÿ)")
            logger.info("-" * 80)

            # å‘é€æ–‡ä»¶é¢„å¤„ç†å®Œæˆè¿›åº¦
            yield {"type": "progress", "stage": "file_preprocessing_completed", "message": "æ–‡ä»¶é¢„å¤„ç†å®Œæˆ", "progress": 30}

            # ä½¿ç”¨é¢„å¤„ç†åçš„æ‚£è€…ä¿¡æ¯æ‰§è¡ŒåŸå§‹ä»»åŠ¡
            # ========== é˜¶æ®µ2: ç–¾ç—…é…ç½®è¯†åˆ« ==========
            disease_config_start_time = time.time()
            logger.info("-" * 80)
            logger.info("ã€é˜¶æ®µ2ã€‘å¼€å§‹ç–¾ç—…é…ç½®è¯†åˆ«")
            logger.info("-" * 80)

            # å‘é€è¿›åº¦æ›´æ–°
            yield {"type": "progress", "stage": "disease_config", "message": "æ­£åœ¨è¯†åˆ«ç–¾ç—…é…ç½®", "progress": 35}

            # ğŸ†• å‹ç¼©æ‚£è€…ä¿¡æ¯æ•°æ®ï¼ˆå¯é€‰åŠŸèƒ½ï¼Œé»˜è®¤ä¸å¯ç”¨ï¼‰
            compressed_patient_info = preprocessed_info  # é»˜è®¤ä½¿ç”¨åŸå§‹æ•°æ®
            if enable_compression and data_compressor:
                try:
                    compressed_patient_info = data_compressor.compress_data(
                        preprocessed_info,
                        max_tokens=50000,
                        model_name='deepseek-chat'
                    )
                    logger.info(f"âœ… æ‚£è€…ä¿¡æ¯å‹ç¼©å®Œæˆ: {len(preprocessed_info)} â†’ {len(compressed_patient_info)} å­—ç¬¦")
                except Exception as e:
                    logger.warning(f"âš ï¸ æ•°æ®å‹ç¼©å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ•°æ®: {e}")
                    compressed_patient_info = preprocessed_info

            disease_config_inputs = {
                "patient_info": compressed_patient_info  # ä½¿ç”¨å‹ç¼©åçš„æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰æˆ–åŸå§‹æ•°æ®
            }
            self.get_disease_config_task().interpolate_inputs_and_add_conversation_history(disease_config_inputs)
            disease_config_result = self.disease_config_agent().execute_task(self.get_disease_config_task())

            # è®°å½•ç–¾ç—…é…ç½®è¯†åˆ«è€—æ—¶
            disease_config_duration = time.time() - disease_config_start_time
            logger.info("-" * 80)
            logger.info(f"ã€é˜¶æ®µ2ã€‘ç–¾ç—…é…ç½®è¯†åˆ«å®Œæˆï¼Œè€—æ—¶: {disease_config_duration:.2f} ç§’ ({disease_config_duration/60:.2f} åˆ†é’Ÿ)")
            logger.info("-" * 80)

            # è§£æç–¾ç—…é…ç½®ç»“æœ
            disease_config_data = JsonUtils.safe_parse_json(disease_config_result, debug_prefix="Disease config identification")
            if disease_config_data:
                logger.info(f"Identified diseases config: {disease_config_data.get('status', 'unknown')}")
                if disease_config_data.get('configs'):
                    logger.info(f"Found {len(disease_config_data['configs'])} disease configurations")
            else:
                logger.warning("Failed to parse disease config result, will proceed without specific disease config")
                disease_config_data = {"status": "error", "configs": []}

            # å‘é€ç–¾ç—…é…ç½®è¯†åˆ«å®Œæˆè¿›åº¦
            yield {"type": "progress", "stage": "disease_config_completed", "message": "ç–¾ç—…é…ç½®è¯†åˆ«å®Œæˆ", "progress": 45}

            # ========== é˜¶æ®µ3: æ‚£è€…æ•°æ®å¤„ç†ï¼ˆæ—¶é—´è½´ç”Ÿæˆ - åˆ†å±‚å¤„ç†ï¼‰ ==========
            patient_data_processing_start_time = time.time()
            logger.info("-" * 80)
            logger.info("ã€é˜¶æ®µ3ã€‘å¼€å§‹æ‚£è€…æ•°æ®å¤„ç†ï¼ˆæ—¶é—´è½´ç”Ÿæˆ - åˆ†å±‚å¤„ç†ï¼‰")
            logger.info("-" * 80)

            # å‘é€è¿›åº¦æ›´æ–°
            yield {"type": "progress", "stage": "timeline_generation", "message": "æ­£åœ¨ç”Ÿæˆæ‚£è€…æ—¶é—´è½´æ‘˜è¦", "progress": 50}

            # ========== é˜¶æ®µ3.1: ç”Ÿæˆæ—¶é—´è½´æ‘˜è¦ ==========
            logger.info("-" * 80)
            logger.info("ã€é˜¶æ®µ3.1ã€‘å¼€å§‹ç”Ÿæˆæ—¶é—´è½´æ‘˜è¦")
            logger.info("-" * 80)

            # æå–ç°æœ‰æ—¶é—´è½´çš„æ‘˜è¦ï¼ˆåªä¿ç•™å…³é”®å­—æ®µï¼Œä¸åŒ…å«data_blocksï¼‰
            existing_timeline_summary = []
            if existing_timeline and len(existing_timeline) > 0:
                for entry in existing_timeline:
                    summary_entry = {
                        "id": entry.get("id"),
                        "time_period": entry.get("time_period"),
                        "title": entry.get("title"),
                        "type": entry.get("type"),
                        "location": entry.get("location"),
                        "summary": entry.get("summary"),
                        "key_indicators": entry.get("key_indicators"),
                        "has_details": True  # æ ‡è®°å·²æœ‰è¯¦ç»†æ•°æ®
                    }
                    existing_timeline_summary.append(summary_entry)
                logger.info(f"ä»ç°æœ‰æ—¶é—´è½´ä¸­æå–äº† {len(existing_timeline_summary)} ä¸ªæ‘˜è¦æ¡ç›®")
            else:
                logger.info("æ²¡æœ‰ç°æœ‰æ—¶é—´è½´æ•°æ®ï¼Œå°†åˆ›å»ºæ–°çš„æ—¶é—´è½´")

            # æ‰§è¡Œæ—¶é—´è½´æ‘˜è¦ç”Ÿæˆä»»åŠ¡
            summary_inputs = {
                "patient_info": compressed_patient_info,
                "current_date": current_date,
                "existing_timeline_summary": existing_timeline_summary,
                "disease_config": disease_config_data
            }
            self.generate_timeline_summary_task().interpolate_inputs_and_add_conversation_history(summary_inputs)
            timeline_summary_result = self.timeline_summary_generator().execute_task(self.generate_timeline_summary_task())

            # è§£ææ—¶é—´è½´æ‘˜è¦ç»“æœ
            timeline_summary_data = JsonUtils.safe_parse_json(timeline_summary_result, debug_prefix="Timeline summary generation")
            if timeline_summary_data:
                timeline_summary_data = JsonUtils._decode_unicode_in_dict(timeline_summary_data)
                logger.info(f"æˆåŠŸç”Ÿæˆæ—¶é—´è½´æ‘˜è¦ï¼ŒåŒ…å« {len(timeline_summary_data.get('timeline', []))} ä¸ªæ¡ç›®")
            else:
                logger.error("æ—¶é—´è½´æ‘˜è¦ç”Ÿæˆå¤±è´¥")
                timeline_summary_data = {"patient_info": {}, "timeline": []}

            # å‘é€æ‘˜è¦ç”Ÿæˆå®Œæˆè¿›åº¦
            yield {"type": "progress", "stage": "timeline_summary_completed", "message": "æ—¶é—´è½´æ‘˜è¦ç”Ÿæˆå®Œæˆ", "progress": 55}

            # ========== é˜¶æ®µ3.2: å¹¶å‘ç”Ÿæˆè¯¦ç»†æ•°æ® ==========
            logger.info("-" * 80)
            logger.info("ã€é˜¶æ®µ3.2ã€‘å¼€å§‹å¹¶å‘ç”Ÿæˆæ—¶é—´è½´è¯¦ç»†æ•°æ®")
            logger.info("-" * 80)

            timeline_entries = timeline_summary_data.get("timeline", [])

            # è¯†åˆ«éœ€è¦ç”Ÿæˆè¯¦ç»†æ•°æ®çš„æ¡ç›®ï¼ˆæ–°å¢çš„æ¡ç›®ï¼‰
            entries_need_details = []
            existing_timeline_ids = [e.get("id") for e in existing_timeline_summary]

            for entry in timeline_entries:
                timeline_id = entry.get("id")
                # åªä¸ºæ–°å¢çš„æ¡ç›®ç”Ÿæˆè¯¦ç»†æ•°æ®ï¼ˆä¸åœ¨existing_timeline_summaryä¸­çš„æ¡ç›®ï¼‰
                if timeline_id not in existing_timeline_ids:
                    entries_need_details.append(entry)
                    logger.debug(f"æ¡ç›® {timeline_id} æ˜¯æ–°å¢æ¡ç›®ï¼Œéœ€è¦ç”Ÿæˆè¯¦ç»†æ•°æ®")
                else:
                    logger.debug(f"æ¡ç›® {timeline_id} å·²å­˜åœ¨ï¼Œè·³è¿‡è¯¦ç»†æ•°æ®ç”Ÿæˆ")

            logger.info(f"å…±æœ‰ {len(timeline_entries)} ä¸ªæ—¶é—´è½´æ¡ç›®ï¼Œå…¶ä¸­ {len(entries_need_details)} ä¸ªéœ€è¦ç”Ÿæˆè¯¦ç»†æ•°æ®")

            # åˆ†æ‰¹å¹¶å‘å¤„ç†è¯¦ç»†æ•°æ®ç”Ÿæˆ
            batch_size = 3  # æ¯æ‰¹å¤„ç†3ä¸ªæ¡ç›®
            all_details = {}  # å­˜å‚¨æ‰€æœ‰è¯¦ç»†æ•°æ®ï¼Œkeyä¸ºtimeline_id

            if entries_need_details:
                # å®šä¹‰å•ä¸ªæ¡ç›®çš„è¯¦ç»†æ•°æ®ç”Ÿæˆå‡½æ•°
                def generate_details_for_entry(entry):
                    try:
                        timeline_id = entry.get("id")
                        logger.info(f"å¼€å§‹ç”Ÿæˆæ¡ç›® {timeline_id} çš„è¯¦ç»†æ•°æ®")

                        # ğŸš¨ é‡è¦ï¼šä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºæ–°çš„ Task å®ä¾‹ï¼Œé¿å…çº¿ç¨‹å®‰å…¨é—®é¢˜
                        detail_task = Task(
                            config=self.tasks_config['generate_timeline_details_task'],
                            context=[self.get_disease_config_task()]
                        )

                        detail_inputs = {
                            "patient_info": compressed_patient_info,
                            "current_date": current_date,
                            "timeline_summary": timeline_summary_data,
                            "target_timeline_id": timeline_id,
                            "disease_config": disease_config_data
                        }
                        detail_task.interpolate_inputs_and_add_conversation_history(detail_inputs)
                        detail_result = self.timeline_details_generator().execute_task(detail_task)

                        # è§£æè¯¦ç»†æ•°æ®
                        detail_data = JsonUtils.safe_parse_json(detail_result, debug_prefix=f"Timeline details for {timeline_id}")
                        if detail_data:
                            detail_data = JsonUtils._decode_unicode_in_dict(detail_data)
                            logger.info(f"æˆåŠŸç”Ÿæˆæ¡ç›® {timeline_id} çš„è¯¦ç»†æ•°æ®ï¼ŒåŒ…å« {len(detail_data.get('data_blocks', []))} ä¸ªæ•°æ®å—")
                            return (timeline_id, detail_data.get("data_blocks", []))
                        else:
                            logger.warning(f"æ¡ç›® {timeline_id} çš„è¯¦ç»†æ•°æ®ç”Ÿæˆå¤±è´¥")
                            return (timeline_id, [])
                    except Exception as e:
                        logger.error(f"ç”Ÿæˆæ¡ç›® {entry.get('id')} çš„è¯¦ç»†æ•°æ®æ—¶å‡ºé”™: {e}")
                        import traceback
                        logger.error(traceback.format_exc())
                        return (entry.get("id"), [])

                # è·å–æœ€å¤§å¹¶å‘æ•°
                max_concurrent = min(self.max_concurrency, len(entries_need_details))
                logger.info(f"ä½¿ç”¨ {max_concurrent} ä¸ªå¹¶å‘workerå¤„ç† {len(entries_need_details)} ä¸ªæ¡ç›®")

                # åˆ†æ‰¹å¤„ç†
                completed_count = 0
                for batch_start in range(0, len(entries_need_details), batch_size):
                    batch_entries = entries_need_details[batch_start:batch_start + batch_size]
                    batch_num = batch_start // batch_size + 1
                    total_batches = (len(entries_need_details) + batch_size - 1) // batch_size

                    logger.info(f"å¤„ç†ç¬¬ {batch_num}/{total_batches} æ‰¹ï¼ŒåŒ…å« {len(batch_entries)} ä¸ªæ¡ç›®")

                    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†å½“å‰æ‰¹æ¬¡
                    with concurrent.futures.ThreadPoolExecutor(max_workers=min(max_concurrent, len(batch_entries))) as executor:
                        future_to_entry = {executor.submit(generate_details_for_entry, entry): entry for entry in batch_entries}

                        for future in concurrent.futures.as_completed(future_to_entry):
                            entry = future_to_entry[future]
                            try:
                                timeline_id, data_blocks = future.result()
                                all_details[timeline_id] = data_blocks
                                completed_count += 1
                                logger.info(f"å®Œæˆ {completed_count}/{len(entries_need_details)} ä¸ªæ¡ç›®çš„è¯¦ç»†æ•°æ®ç”Ÿæˆ")

                                # å‘é€è¯¦ç»†æ•°æ®ç”Ÿæˆè¿›åº¦ï¼ˆ55-65%ä¹‹é—´ï¼‰
                                detail_progress = 55 + int(10 * completed_count / len(entries_need_details))
                                yield {"type": "progress", "stage": "timeline_details_generation",
                                       "message": f"æ­£åœ¨ç”Ÿæˆè¯¦ç»†æ•°æ® {completed_count}/{len(entries_need_details)}",
                                       "progress": detail_progress}
                            except Exception as e:
                                logger.error(f"å¤„ç†æ¡ç›® {entry.get('id')} çš„è¯¦ç»†æ•°æ®æ—¶å‡ºé”™: {e}")

            # ========== é˜¶æ®µ3.3: åˆå¹¶æ‘˜è¦å’Œè¯¦ç»†æ•°æ® ==========
            logger.info("-" * 80)
            logger.info("ã€é˜¶æ®µ3.3ã€‘å¼€å§‹åˆå¹¶æ‘˜è¦å’Œè¯¦ç»†æ•°æ®")
            logger.info("-" * 80)

            # åˆå¹¶æ‘˜è¦å’Œè¯¦ç»†æ•°æ®
            final_timeline = []
            for entry in timeline_entries:
                timeline_id = entry.get("id")
                # åˆ›å»ºå®Œæ•´çš„æ—¶é—´è½´æ¡ç›®
                full_entry = dict(entry)  # å¤åˆ¶æ‘˜è¦æ•°æ®

                # æ·»åŠ è¯¦ç»†æ•°æ®
                if timeline_id in all_details:
                    # æ–°ç”Ÿæˆçš„è¯¦ç»†æ•°æ®
                    full_entry["data_blocks"] = all_details[timeline_id]
                elif existing_timeline and timeline_id in [e.get("id") for e in existing_timeline]:
                    # å¦‚æœæ˜¯ç°æœ‰æ¡ç›®ï¼Œä»existing_timelineä¸­è·å–data_blocks
                    for existing_entry in existing_timeline:
                        if existing_entry.get("id") == timeline_id:
                            full_entry["data_blocks"] = existing_entry.get("data_blocks", [])
                            break
                else:
                    # æ²¡æœ‰è¯¦ç»†æ•°æ®
                    full_entry["data_blocks"] = []

                final_timeline.append(full_entry)

            # æ„å»ºæœ€ç»ˆçš„parsed_result
            parsed_result = {
                "patient_info": timeline_summary_data.get("patient_info", {}),
                "timeline": final_timeline
            }

            logger.info(f"æˆåŠŸåˆå¹¶æ—¶é—´è½´æ•°æ®ï¼Œæœ€ç»ˆåŒ…å« {len(final_timeline)} ä¸ªå®Œæ•´æ¡ç›®")

            # è®°å½•æ‚£è€…æ•°æ®å¤„ç†è€—æ—¶
            patient_data_processing_duration = time.time() - patient_data_processing_start_time
            logger.info("-" * 80)
            logger.info(f"ã€é˜¶æ®µ3ã€‘æ‚£è€…æ•°æ®å¤„ç†å®Œæˆï¼Œè€—æ—¶: {patient_data_processing_duration:.2f} ç§’ ({patient_data_processing_duration/60:.2f} åˆ†é’Ÿ)")
            logger.info("-" * 80)

            # å‘é€æ—¶é—´è½´ç”Ÿæˆå®Œæˆè¿›åº¦
            yield {"type": "progress", "stage": "timeline_generation_completed", "message": "æ‚£è€…æ—¶é—´è½´ç”Ÿæˆå®Œæˆ", "progress": 65}

            # ========== é˜¶æ®µ4: æ‚£è€…æ—…ç¨‹æå–ï¼ˆåˆ†å±‚å¤„ç†ï¼‰ ==========
            patient_journey_start_time = time.time()
            logger.info("-" * 80)
            logger.info("ã€é˜¶æ®µ4ã€‘å¼€å§‹æ‚£è€…æ—…ç¨‹æå–ï¼ˆåˆ†å±‚å¤„ç†ï¼‰")
            logger.info("-" * 80)

            # å‘é€è¿›åº¦æ›´æ–°
            yield {"type": "progress", "stage": "patient_journey", "message": "æ­£åœ¨æå–æ‚£è€…æ—…ç¨‹æ•°æ®", "progress": 70}

            # ğŸ†• å‹ç¼©ç°æœ‰æ‚£è€…æ—…ç¨‹æ•°æ®ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰
            compressed_journey = existing_patient_journey  # é»˜è®¤ä½¿ç”¨åŸå§‹æ•°æ®
            if enable_compression and data_compressor and existing_patient_journey and len(existing_patient_journey) > 0:
                try:
                    compressed_journey = data_compressor.compress_data(
                        existing_patient_journey,
                        max_tokens=20000,
                        model_name='deepseek-chat'
                    )
                    logger.info(f"âœ… æ‚£è€…æ—…ç¨‹å‹ç¼©å®Œæˆ")
                except Exception as e:
                    logger.warning(f"âš ï¸ æ‚£è€…æ—…ç¨‹å‹ç¼©å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ•°æ®: {e}")
                    compressed_journey = existing_patient_journey

            # æ‹†åˆ†ç°æœ‰æ‚£è€…æ—…ç¨‹æ•°æ®ä¸º timeline_journey å’Œ indicator_series
            existing_timeline_journey = []
            existing_indicator_series = []
            if compressed_journey:
                if isinstance(compressed_journey, dict):
                    existing_timeline_journey = compressed_journey.get("timeline_journey", [])
                    existing_indicator_series = compressed_journey.get("indicator_series", [])
                elif isinstance(compressed_journey, list):
                    # å…¼å®¹æ—§æ ¼å¼ï¼šå¦‚æœæ˜¯åˆ—è¡¨ï¼Œå‡è®¾æ˜¯ timeline_journey
                    existing_timeline_journey = compressed_journey
                    logger.warning("ç°æœ‰æ‚£è€…æ—…ç¨‹æ•°æ®æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œå°†å…¶è§†ä¸º timeline_journey")

            # ========== é˜¶æ®µ4.1: ç”Ÿæˆæ‚£è€…æ—…ç¨‹æ‘˜è¦ ==========
            logger.info("-" * 80)
            logger.info("ã€é˜¶æ®µ4.1ã€‘å¼€å§‹ç”Ÿæˆæ‚£è€…æ—…ç¨‹æ‘˜è¦")
            logger.info("-" * 80)

            # æå–ç°æœ‰æ‚£è€…æ—…ç¨‹çš„æ‘˜è¦ï¼ˆåªä¿ç•™idã€dateã€typeã€event_descriptionï¼‰
            existing_journey_summary = []
            if existing_timeline_journey and len(existing_timeline_journey) > 0:
                for event in existing_timeline_journey:
                    # ä»ç°æœ‰äº‹ä»¶çš„textä¸­æå–ç®€è¦æè¿°ä½œä¸ºevent_description
                    # å¦‚æœtextå¤ªé•¿ï¼Œæˆªå–å‰30å­—ä½œä¸ºäº‹ä»¶æè¿°
                    event_text = event.get("text", "")
                    event_description = event_text[:30] if event_text else ""

                    summary_event = {
                        "id": event.get("id"),
                        "date": event.get("date"),
                        "type": event.get("type"),
                        "event_description": event_description
                    }
                    existing_journey_summary.append(summary_event)
                logger.info(f"ä»ç°æœ‰æ‚£è€…æ—…ç¨‹ä¸­æå–äº† {len(existing_journey_summary)} ä¸ªæ‘˜è¦äº‹ä»¶")
            else:
                logger.info("æ²¡æœ‰ç°æœ‰æ‚£è€…æ—…ç¨‹æ•°æ®ï¼Œå°†åˆ›å»ºæ–°çš„æ‚£è€…æ—…ç¨‹")

            # æ‰§è¡Œæ‚£è€…æ—…ç¨‹æ‘˜è¦ç”Ÿæˆä»»åŠ¡
            journey_summary_result = None
            try:
                summary_inputs = {
                    "current_date": current_date,
                    "patient_content": compressed_patient_info,
                    "full_structure_data": parsed_result if parsed_result else {},
                    "existing_journey_summary": existing_journey_summary,
                    "disease_config": disease_config_data
                }

                # ğŸš¨ é‡è¦ï¼šä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºæ–°çš„ Task å®ä¾‹
                journey_summary_task = Task(
                    config=self.tasks_config['generate_patient_journey_summary_task'],
                    context=[self.get_disease_config_task()]
                )

                journey_summary_task.interpolate_inputs_and_add_conversation_history(summary_inputs)
                journey_summary_result_raw = self.patient_journey_summary_generator().execute_task(journey_summary_task)

                # è§£ææ‚£è€…æ—…ç¨‹æ‘˜è¦ç»“æœ
                journey_summary_result = JsonUtils.safe_parse_json(journey_summary_result_raw, debug_prefix="Patient journey summary generation")
                if journey_summary_result:
                    journey_summary_result = JsonUtils._decode_unicode_in_dict(journey_summary_result)
                    # éªŒè¯ç»“æœæ˜¯å¦ä¸ºåˆ—è¡¨
                    if isinstance(journey_summary_result, list):
                        logger.info(f"æˆåŠŸç”Ÿæˆæ‚£è€…æ—…ç¨‹æ‘˜è¦ï¼ŒåŒ…å« {len(journey_summary_result)} ä¸ªäº‹ä»¶")
                    else:
                        logger.warning("æ‚£è€…æ—…ç¨‹æ‘˜è¦è§£æç»“æœæ ¼å¼ä¸æ­£ç¡®ï¼Œåº”ä¸ºåˆ—è¡¨")
                        journey_summary_result = []
                else:
                    logger.warning("æ‚£è€…æ—…ç¨‹æ‘˜è¦è§£æç»“æœä¸ºç©º")
                    journey_summary_result = []
            except Exception as e:
                logger.error(f"Error in patient journey summary generation: {e}")
                import traceback
                logger.error(traceback.format_exc())
                journey_summary_result = []

            # å‘é€æ‘˜è¦ç”Ÿæˆå®Œæˆè¿›åº¦
            yield {"type": "progress", "stage": "journey_summary_completed", "message": "æ‚£è€…æ—…ç¨‹æ‘˜è¦ç”Ÿæˆå®Œæˆ", "progress": 72}

            # ========== é˜¶æ®µ4.2: å¹¶å‘ç”Ÿæˆè¯¦ç»†æ–‡æœ¬ ==========
            logger.info("-" * 80)
            logger.info("ã€é˜¶æ®µ4.2ã€‘å¼€å§‹å¹¶å‘ç”Ÿæˆæ‚£è€…æ—…ç¨‹è¯¦ç»†æ–‡æœ¬")
            logger.info("-" * 80)

            # è¯†åˆ«éœ€è¦ç”Ÿæˆè¯¦ç»†æ–‡æœ¬çš„äº‹ä»¶ï¼ˆæ–°å¢çš„äº‹ä»¶ï¼‰
            events_need_details = []
            existing_journey_ids = [e.get("id") for e in existing_journey_summary]

            for event in journey_summary_result:
                event_id = event.get("id")
                # åªä¸ºæ–°å¢çš„äº‹ä»¶ç”Ÿæˆè¯¦ç»†æ–‡æœ¬ï¼ˆä¸åœ¨existing_journey_summaryä¸­çš„äº‹ä»¶ï¼‰
                if event_id not in existing_journey_ids:
                    events_need_details.append(event)
                    logger.debug(f"äº‹ä»¶ {event_id} æ˜¯æ–°å¢äº‹ä»¶ï¼Œéœ€è¦ç”Ÿæˆè¯¦ç»†æ–‡æœ¬")
                else:
                    logger.debug(f"äº‹ä»¶ {event_id} å·²å­˜åœ¨ï¼Œè·³è¿‡è¯¦ç»†æ–‡æœ¬ç”Ÿæˆ")

            logger.info(f"å…±æœ‰ {len(journey_summary_result)} ä¸ªæ‚£è€…æ—…ç¨‹äº‹ä»¶ï¼Œå…¶ä¸­ {len(events_need_details)} ä¸ªéœ€è¦ç”Ÿæˆè¯¦ç»†æ–‡æœ¬")

            # åˆ†æ‰¹å¹¶å‘å¤„ç†è¯¦ç»†æ–‡æœ¬ç”Ÿæˆ
            batch_size = 8  # æ¯æ‰¹å¤„ç†8ä¸ªäº‹ä»¶ï¼ˆæ ¹æ®ç”¨æˆ·å»ºè®®ï¼‰
            all_details = {}  # å­˜å‚¨æ‰€æœ‰è¯¦ç»†æ–‡æœ¬ï¼Œkeyä¸ºevent_id

            if events_need_details:
                # å®šä¹‰æ‰¹é‡ç”Ÿæˆè¯¦ç»†æ–‡æœ¬çš„å‡½æ•°
                def generate_details_for_batch(event_ids):
                    try:
                        logger.info(f"å¼€å§‹ç”Ÿæˆæ‰¹æ¬¡äº‹ä»¶çš„è¯¦ç»†æ–‡æœ¬ï¼Œäº‹ä»¶ID: {event_ids}")

                        # ğŸš¨ é‡è¦ï¼šä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºæ–°çš„ Task å®ä¾‹ï¼Œé¿å…çº¿ç¨‹å®‰å…¨é—®é¢˜
                        detail_task = Task(
                            config=self.tasks_config['generate_patient_journey_details_task'],
                            context=[self.get_disease_config_task()]
                        )

                        detail_inputs = {
                            "current_date": current_date,
                            "patient_content": compressed_patient_info,
                            "full_structure_data": parsed_result if parsed_result else {},
                            "journey_summary": journey_summary_result,
                            "target_event_ids": event_ids,
                            "disease_config": disease_config_data
                        }
                        detail_task.interpolate_inputs_and_add_conversation_history(detail_inputs)
                        detail_result = self.patient_journey_details_generator().execute_task(detail_task)

                        # è§£æè¯¦ç»†æ–‡æœ¬
                        detail_data = JsonUtils.safe_parse_json(detail_result, debug_prefix=f"Patient journey details for {event_ids}")
                        if detail_data:
                            detail_data = JsonUtils._decode_unicode_in_dict(detail_data)
                            # éªŒè¯ç»“æœæ˜¯å¦ä¸ºåˆ—è¡¨
                            if isinstance(detail_data, list):
                                logger.info(f"æˆåŠŸç”Ÿæˆæ‰¹æ¬¡äº‹ä»¶çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…å« {len(detail_data)} ä¸ªäº‹ä»¶")
                                # è¿”å›å­—å…¸ï¼Œkeyä¸ºevent_idï¼Œvalueä¸ºè¯¦ç»†ä¿¡æ¯å¯¹è±¡
                                result = {}
                                for item in detail_data:
                                    event_id = item.get("id")
                                    result[event_id] = {
                                        "text": item.get("text", ""),
                                        "chief_surgeon": item.get("chief_surgeon", ""),
                                        "examination_hospital": item.get("examination_hospital", ""),
                                        "sources": item.get("sources", [])
                                    }
                                return result
                            else:
                                logger.warning(f"æ‰¹æ¬¡äº‹ä»¶çš„è¯¦ç»†ä¿¡æ¯è§£æç»“æœæ ¼å¼ä¸æ­£ç¡®")
                                return {}
                        else:
                            logger.warning(f"æ‰¹æ¬¡äº‹ä»¶çš„è¯¦ç»†ä¿¡æ¯ç”Ÿæˆå¤±è´¥")
                            return {}
                    except Exception as e:
                        logger.error(f"ç”Ÿæˆæ‰¹æ¬¡äº‹ä»¶ {event_ids} çš„è¯¦ç»†ä¿¡æ¯æ—¶å‡ºé”™: {e}")
                        import traceback
                        logger.error(traceback.format_exc())
                        return {}

                # è·å–æœ€å¤§å¹¶å‘æ•°
                max_concurrent = min(self.max_concurrency, (len(events_need_details) + batch_size - 1) // batch_size)
                logger.info(f"ä½¿ç”¨ {max_concurrent} ä¸ªå¹¶å‘workerå¤„ç† {len(events_need_details)} ä¸ªäº‹ä»¶")

                # åˆ†æ‰¹å¤„ç†
                completed_count = 0
                total_batches = (len(events_need_details) + batch_size - 1) // batch_size

                for batch_start in range(0, len(events_need_details), batch_size):
                    batch_events = events_need_details[batch_start:batch_start + batch_size]
                    batch_num = batch_start // batch_size + 1
                    batch_event_ids = [e.get("id") for e in batch_events]

                    logger.info(f"å¤„ç†ç¬¬ {batch_num}/{total_batches} æ‰¹ï¼ŒåŒ…å« {len(batch_events)} ä¸ªäº‹ä»¶")

                    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†å½“å‰æ‰¹æ¬¡
                    with concurrent.futures.ThreadPoolExecutor(max_workers=min(max_concurrent, 1)) as executor:
                        future = executor.submit(generate_details_for_batch, batch_event_ids)

                        try:
                            batch_details = future.result()
                            all_details.update(batch_details)
                            completed_count += len(batch_events)
                            logger.info(f"å®Œæˆ {completed_count}/{len(events_need_details)} ä¸ªäº‹ä»¶çš„è¯¦ç»†æ–‡æœ¬ç”Ÿæˆ")

                            # å‘é€è¯¦ç»†æ–‡æœ¬ç”Ÿæˆè¿›åº¦ï¼ˆ72-78%ä¹‹é—´ï¼‰
                            detail_progress = 72 + int(6 * completed_count / len(events_need_details))
                            yield {"type": "progress", "stage": "journey_details_generation",
                                   "message": f"æ­£åœ¨ç”Ÿæˆè¯¦ç»†æ–‡æœ¬ {completed_count}/{len(events_need_details)}",
                                   "progress": detail_progress}
                        except Exception as e:
                            logger.error(f"å¤„ç†æ‰¹æ¬¡ {batch_num} çš„è¯¦ç»†æ–‡æœ¬æ—¶å‡ºé”™: {e}")

            # ========== é˜¶æ®µ4.3: åˆå¹¶æ‘˜è¦å’Œè¯¦ç»†ä¿¡æ¯ ==========
            logger.info("-" * 80)
            logger.info("ã€é˜¶æ®µ4.3ã€‘å¼€å§‹åˆå¹¶æ‘˜è¦å’Œè¯¦ç»†ä¿¡æ¯")
            logger.info("-" * 80)

            # åˆå¹¶æ‘˜è¦å’Œè¯¦ç»†ä¿¡æ¯
            final_timeline_journey = []
            for event in journey_summary_result:
                event_id = event.get("id")
                # åˆ›å»ºå®Œæ•´çš„æ‚£è€…æ—…ç¨‹äº‹ä»¶
                full_event = {
                    "date": event.get("date"),
                    "type": event.get("type")
                }

                # æ·»åŠ è¯¦ç»†ä¿¡æ¯
                if event_id in all_details:
                    # æ–°ç”Ÿæˆçš„è¯¦ç»†ä¿¡æ¯
                    detail_info = all_details[event_id]
                    full_event["text"] = detail_info.get("text", "")
                    full_event["chief_surgeon"] = detail_info.get("chief_surgeon", "")
                    full_event["examination_hospital"] = detail_info.get("examination_hospital", "")
                    full_event["sources"] = detail_info.get("sources", [])
                elif existing_timeline_journey and event_id in [e.get("id") for e in existing_timeline_journey]:
                    # å¦‚æœæ˜¯ç°æœ‰äº‹ä»¶ï¼Œä»existing_timeline_journeyä¸­è·å–è¯¦ç»†ä¿¡æ¯
                    for existing_event in existing_timeline_journey:
                        if existing_event.get("id") == event_id:
                            full_event["text"] = existing_event.get("text", "")
                            full_event["chief_surgeon"] = existing_event.get("chief_surgeon", "")
                            full_event["examination_hospital"] = existing_event.get("examination_hospital", "")
                            full_event["sources"] = existing_event.get("sources", [])
                            break
                else:
                    # æ²¡æœ‰è¯¦ç»†ä¿¡æ¯ï¼Œä½¿ç”¨ç©ºå€¼
                    full_event["text"] = ""
                    full_event["chief_surgeon"] = ""
                    full_event["examination_hospital"] = ""
                    full_event["sources"] = []

                final_timeline_journey.append(full_event)

            logger.info(f"æˆåŠŸåˆå¹¶æ‚£è€…æ—…ç¨‹æ•°æ®ï¼Œæœ€ç»ˆåŒ…å« {len(final_timeline_journey)} ä¸ªå®Œæ•´äº‹ä»¶")

            # æ‰§è¡Œ"æ‚£è€…æ—¶é—´æ—…ç¨‹"ä»»åŠ¡ï¼ˆåªæå–æ—¶é—´è½´ï¼‰
            timeline_journey_result = final_timeline_journey

            # ========== é˜¶æ®µ4.5: æŒ‡æ ‡åºåˆ—æå– ==========
            logger.info("-" * 80)
            logger.info("ã€é˜¶æ®µ4.5ã€‘å¼€å§‹æŒ‡æ ‡åºåˆ—æå–")
            logger.info("-" * 80)

            # å‘é€è¿›åº¦æ›´æ–°
            yield {"type": "progress", "stage": "indicator_series", "message": "æ­£åœ¨æå–å…³é”®æŒ‡æ ‡åºåˆ—", "progress": 77}

            # æ‰§è¡Œ"æŒ‡æ ‡åºåˆ—æå–"ä»»åŠ¡
            indicator_series_result = None
            try:
                indicator_inputs = {
                    "current_date": current_date,
                    "patient_content": compressed_patient_info,  # ğŸ†• ä½¿ç”¨å‹ç¼©åçš„æ•°æ®
                    "full_structure_data": parsed_result if parsed_result else {},
                    "existing_indicator_series": existing_indicator_series,  # ğŸ†• åªä¼ å…¥æŒ‡æ ‡åºåˆ—æ•°æ®
                    "disease_config": disease_config_data  # ä¼ é€’ç–¾ç—…é…ç½®
                }
                self.extract_indicator_series_task().interpolate_inputs_and_add_conversation_history(indicator_inputs)
                indicator_result = self.indicator_series_extractor().execute_task(self.extract_indicator_series_task())
                indicator_series_result = JsonUtils.safe_parse_json(indicator_result, debug_prefix="Indicator series extraction")

                # é¢å¤–çš„Unicodeæ¸…ç†æ­¥éª¤å’Œç»“æ„éªŒè¯
                if indicator_series_result:
                    indicator_series_result = JsonUtils._decode_unicode_in_dict(indicator_series_result)
                    # éªŒè¯ç»“æœæ˜¯å¦ä¸ºåˆ—è¡¨
                    if isinstance(indicator_series_result, list):
                        logger.info(f"æˆåŠŸæå–æŒ‡æ ‡åºåˆ—ï¼ŒåŒ…å«{len(indicator_series_result)}ä¸ªæŒ‡æ ‡")
                    elif isinstance(indicator_series_result, dict):
                        # å¦‚æœè¿”å›çš„æ˜¯å­—å…¸ï¼Œå°è¯•æå– indicator_series å­—æ®µ
                        if "indicator_series" in indicator_series_result:
                            indicator_series_result = indicator_series_result["indicator_series"]
                            logger.info(f"ä»å­—å…¸ä¸­æå–æŒ‡æ ‡åºåˆ—ï¼ŒåŒ…å«{len(indicator_series_result)}ä¸ªæŒ‡æ ‡")
                        else:
                            logger.warning("æŒ‡æ ‡åºåˆ—è§£æç»“æœæ˜¯å­—å…¸ä½†ç¼ºå°‘ indicator_series å­—æ®µ")
                            indicator_series_result = []
                    else:
                        logger.warning("æŒ‡æ ‡åºåˆ—è§£æç»“æœæ ¼å¼ä¸æ­£ç¡®")
                        indicator_series_result = []
                else:
                    logger.warning("æŒ‡æ ‡åºåˆ—è§£æç»“æœä¸ºç©º")
                    indicator_series_result = []
            except Exception as e:
                logger.error(f"Error in indicator series extraction: {e}")
                indicator_series_result = []

            # åˆå¹¶æ‚£è€…æ—…ç¨‹å’ŒæŒ‡æ ‡åºåˆ—ç»“æœ
            special_parsed_result = {
                "timeline_journey": timeline_journey_result if timeline_journey_result else [],
                "indicator_series": indicator_series_result if indicator_series_result else []
            }

            # è®°å½•æ‚£è€…æ—…ç¨‹æå–è€—æ—¶
            patient_journey_duration = time.time() - patient_journey_start_time
            logger.info("-" * 80)
            logger.info(f"ã€é˜¶æ®µ4ã€‘æ‚£è€…æ—…ç¨‹æå–å®Œæˆï¼Œè€—æ—¶: {patient_journey_duration:.2f} ç§’ ({patient_journey_duration/60:.2f} åˆ†é’Ÿ)")
            logger.info("-" * 80)

            # å‘é€æ‚£è€…æ—…ç¨‹æå–å®Œæˆè¿›åº¦
            yield {"type": "progress", "stage": "patient_journey_completed", "message": "æ‚£è€…æ—…ç¨‹æ•°æ®æå–å®Œæˆ", "progress": 80}

            # ========== é˜¶æ®µ5: MDTæŠ¥å‘Šç”Ÿæˆ ==========
            mdt_report_start_time = time.time()
            logger.info("-" * 80)
            logger.info("ã€é˜¶æ®µ5ã€‘å¼€å§‹MDTæŠ¥å‘Šç”Ÿæˆ")
            logger.info("-" * 80)

            # å‘é€è¿›åº¦æ›´æ–°
            yield {"type": "progress", "stage": "mdt_report", "message": "æ­£åœ¨ç”ŸæˆMDTæŠ¥å‘Š", "progress": 85}

            # ğŸ†• å‹ç¼©ç°æœ‰MDTæŠ¥å‘Šæ•°æ®ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰
            compressed_mdt_report = existing_mdt_report  # é»˜è®¤ä½¿ç”¨åŸå§‹æ•°æ®
            if enable_compression and data_compressor and existing_mdt_report and len(existing_mdt_report) > 0:
                try:
                    compressed_mdt_report = data_compressor.compress_data(
                        existing_mdt_report,
                        max_tokens=20000,
                        model_name='deepseek-chat'
                    )
                    logger.info(f"âœ… MDTæŠ¥å‘Šå‹ç¼©å®Œæˆ")
                except Exception as e:
                    logger.warning(f"âš ï¸ MDTæŠ¥å‘Šå‹ç¼©å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ•°æ®: {e}")
                    compressed_mdt_report = existing_mdt_report

            # æ‰§è¡ŒMDTæŠ¥å‘Šç”Ÿæˆä»»åŠ¡
            mdt_report_result = None
            try:
                mdt_inputs = {
                    "current_date": current_date,
                    "patient_content": compressed_patient_info,  # ğŸ†• ä½¿ç”¨å‹ç¼©åçš„æ•°æ®
                    "patient_structured_data": parsed_result if parsed_result else {},
                    "existing_mdt_report": compressed_mdt_report,  # ğŸ†• ä½¿ç”¨å‹ç¼©åçš„æŠ¥å‘Š
                    "disease_config": disease_config_data  # ä¼ é€’ç–¾ç—…é…ç½®
                }
                self.generate_mdt_report_task().interpolate_inputs_and_add_conversation_history(mdt_inputs)
                mdt_result = self.mdt_report_generator().execute_task(self.generate_mdt_report_task())
                mdt_parsed_result = JsonUtils.safe_parse_json(mdt_result, debug_prefix="MDT report generation")
                
                # ğŸš¨ ä¿®å¤ï¼šæ­£ç¡®æå–mdt_simple_reportå­—æ®µ
                if mdt_parsed_result:
                    mdt_parsed_result = JsonUtils._decode_unicode_in_dict(mdt_parsed_result)
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«mdt_simple_reportå­—æ®µ
                    if isinstance(mdt_parsed_result, dict) and "mdt_simple_report" in mdt_parsed_result:
                        mdt_report_result = mdt_parsed_result["mdt_simple_report"]
                        logger.info(f"æˆåŠŸæå–MDTæŠ¥å‘Šï¼ŒåŒ…å«{len(mdt_report_result)}ä¸ªæ¡ç›®")
                    else:
                        # å¦‚æœæ²¡æœ‰mdt_simple_reportå­—æ®µï¼Œä½¿ç”¨æ•´ä¸ªè§£æç»“æœ
                        logger.warning("MDTæŠ¥å‘ŠJSONä¸­æœªæ‰¾åˆ°mdt_simple_reportå­—æ®µï¼Œä½¿ç”¨æ•´ä¸ªè§£æç»“æœ")
                        mdt_report_result = mdt_parsed_result
                else:
                    logger.warning("MDTæŠ¥å‘Šè§£æç»“æœä¸ºç©º")
            except Exception as e:
                logger.error(f"Error in MDT report generation: {e}")

            # è®°å½•MDTæŠ¥å‘Šç”Ÿæˆè€—æ—¶
            mdt_report_duration = time.time() - mdt_report_start_time
            logger.info("-" * 80)
            logger.info(f"ã€é˜¶æ®µ5ã€‘MDTæŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {mdt_report_duration:.2f} ç§’ ({mdt_report_duration/60:.2f} åˆ†é’Ÿ)")
            logger.info("-" * 80)

            # å‘é€MDTæŠ¥å‘Šç”Ÿæˆå®Œæˆè¿›åº¦
            yield {"type": "progress", "stage": "mdt_report_completed", "message": "MDTæŠ¥å‘Šç”Ÿæˆå®Œæˆ", "progress": 90}

            if parsed_result:
                timeline_count = len(parsed_result.get('timeline', []))
                logger.info(f"Successfully processed patient data with {timeline_count} timeline entries")
            else:
                logger.warning("Failed to parse patient data processing result")
            
            # å‡†å¤‡è¿”å›çš„ç»“æœ
            result_data = {
                "patient_content": preprocessed_info,
                "full_structure_data": parsed_result if parsed_result else {"error": "Failed to parse patient data", "raw": patient_data_result},
                "patient_journey": special_parsed_result if special_parsed_result else {},
                "mdt_simple_report": mdt_report_result if mdt_report_result else {}
            }

            # ğŸš¨ ç®€åŒ–ï¼šç§»é™¤å¤æ‚çš„éªŒè¯é€»è¾‘ï¼Œç›´æ¥ä¿å­˜å¤„ç†ç»“æœ

            # ========== ç”Ÿæˆæ‚£è€…æ—¶é—´æ—…ç¨‹å›¾ç‰‡å¹¶ä¸Šä¼ åˆ°ä¸ƒç‰›äº‘ ==========
            # ğŸš¨ ä¸´æ—¶ç¦ç”¨ï¼šå› ä¸ºPlaywrightåŠ è½½è¶…æ—¶é—®é¢˜
            if False and special_parsed_result and agent_session_id:
                try:
                    logger.info("å¼€å§‹ç”Ÿæˆæ‚£è€…æ—¶é—´æ—…ç¨‹å›¾ç‰‡...")

                    # ä»patient_journeyä¸­æå–æ•°æ®
                    journey_list = special_parsed_result if isinstance(special_parsed_result, list) else []

                    if journey_list:
                        # ç”Ÿæˆå›¾ç‰‡æ–‡ä»¶åå’Œè·¯å¾„
                        image_uuid = str(uuid_lib.uuid4())
                        output_dir = Path("output/files_extract") / agent_session_id / "patient_journey_images"
                        output_dir.mkdir(parents=True, exist_ok=True)

                        image_filename = f"patient_journey_{image_uuid}.png"
                        image_path = output_dir / image_filename

                        # æå–æ‚£è€…å§“åï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                        patient_name = "æ‚£è€…"
                        if parsed_result and isinstance(parsed_result, dict):
                            patient_name = parsed_result.get("patient_name", "æ‚£è€…")

                        # ç”Ÿæˆå›¾ç‰‡
                        success = generate_patient_journey_image_sync(
                            patient_journey_data=journey_list,
                            output_path=str(image_path),
                            patient_name=patient_name
                        )

                        if success and image_path.exists():
                            logger.info(f"æ‚£è€…æ—¶é—´æ—…ç¨‹å›¾ç‰‡ç”ŸæˆæˆåŠŸ: {image_path}")

                            # ä¸Šä¼ åˆ°ä¸ƒç‰›äº‘
                            try:
                                qiniu_service = QiniuUploadService()
                                qiniu_key = f"patient_journey/{image_uuid}.png"

                                upload_success, cloud_url, error = qiniu_service.upload_file(
                                    str(image_path),
                                    qiniu_key
                                )

                                if upload_success:
                                    logger.info(f"æ‚£è€…æ—¶é—´æ—…ç¨‹å›¾ç‰‡å·²ä¸Šä¼ åˆ°ä¸ƒç‰›äº‘: {cloud_url}")

                                    # å°†å›¾ç‰‡URLæ·»åŠ åˆ°patient_journey JSONä¸­
                                    if isinstance(result_data["patient_journey"], dict):
                                        result_data["patient_journey"]["image_url"] = cloud_url
                                    elif isinstance(result_data["patient_journey"], list):
                                        # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå­—å…¸ç»“æ„
                                        result_data["patient_journey"] = {
                                            "timeline_journey": result_data["patient_journey"],
                                            "image_url": cloud_url
                                        }
                                else:
                                    logger.error(f"ä¸Šä¼ æ‚£è€…æ—¶é—´æ—…ç¨‹å›¾ç‰‡åˆ°ä¸ƒç‰›äº‘å¤±è´¥: {error}")
                            except Exception as upload_error:
                                logger.error(f"ä¸Šä¼ æ‚£è€…æ—¶é—´æ—…ç¨‹å›¾ç‰‡åˆ°ä¸ƒç‰›äº‘æ—¶å‡ºé”™: {upload_error}")
                        else:
                            logger.warning("æ‚£è€…æ—¶é—´æ—…ç¨‹å›¾ç‰‡ç”Ÿæˆå¤±è´¥")
                    else:
                        logger.info("æ‚£è€…æ—…ç¨‹æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡å›¾ç‰‡ç”Ÿæˆ")

                except Exception as e:
                    logger.error(f"ç”Ÿæˆæˆ–ä¸Šä¼ æ‚£è€…æ—¶é—´æ—…ç¨‹å›¾ç‰‡æ—¶å‡ºé”™: {e}")
                    import traceback
                    logger.error(traceback.format_exc())

            # ========== ç”Ÿæˆæ ¸å¿ƒæŒ‡æ ‡è¶‹åŠ¿å›¾ç‰‡å¹¶ä¸Šä¼ åˆ°ä¸ƒç‰›äº‘ ==========
            # ğŸš¨ ä¸´æ—¶ç¦ç”¨ï¼šå› ä¸ºPlaywrightåŠ è½½è¶…æ—¶é—®é¢˜
            indicator_chart_image_url = None
            if False and special_parsed_result and agent_session_id:
                try:
                    # ä»patient_journeyä¸­æå–indicator_seriesæ•°æ®
                    indicator_series = None
                    if isinstance(special_parsed_result, dict) and 'indicator_series' in special_parsed_result:
                        indicator_series = special_parsed_result.get('indicator_series')

                    if indicator_series and isinstance(indicator_series, list) and indicator_series:
                        logger.info(f"å¼€å§‹ç”Ÿæˆæ ¸å¿ƒæŒ‡æ ‡è¶‹åŠ¿å›¾ç‰‡ï¼ŒåŒ…å« {len(indicator_series)} ä¸ªæŒ‡æ ‡...")

                        # ç”Ÿæˆå›¾ç‰‡æ–‡ä»¶åå’Œè·¯å¾„
                        image_uuid = str(uuid_lib.uuid4())
                        output_dir = Path("output/files_extract") / agent_session_id / "indicator_chart_images"
                        output_dir.mkdir(parents=True, exist_ok=True)

                        image_filename = f"indicator_chart_{image_uuid}.png"
                        image_path = output_dir / image_filename

                        # æå–æ‚£è€…å§“å
                        patient_name = "æ‚£è€…"
                        if parsed_result and isinstance(parsed_result, dict):
                            patient_name = parsed_result.get("patient_name", "æ‚£è€…")

                        # ç”Ÿæˆå›¾ç‰‡
                        success = generate_indicator_chart_image_sync(
                            indicator_series_data=indicator_series,
                            output_path=str(image_path),
                            patient_name=patient_name
                        )

                        if success and image_path.exists():
                            logger.info(f"æ ¸å¿ƒæŒ‡æ ‡è¶‹åŠ¿å›¾ç‰‡ç”ŸæˆæˆåŠŸ: {image_path}")

                            # ä¸Šä¼ åˆ°ä¸ƒç‰›äº‘
                            try:
                                qiniu_service = QiniuUploadService()
                                qiniu_key = f"indicator_chart/{image_uuid}.png"

                                upload_success, cloud_url, error = qiniu_service.upload_file(
                                    str(image_path),
                                    qiniu_key
                                )

                                if upload_success:
                                    indicator_chart_image_url = cloud_url
                                    # å°†URLæ·»åŠ åˆ°patient_journey JSONä¸­
                                    if isinstance(special_parsed_result, dict):
                                        special_parsed_result["indicator_chart_image_url"] = cloud_url
                                        result_data["patient_journey"] = special_parsed_result
                                    logger.info(f"æ ¸å¿ƒæŒ‡æ ‡è¶‹åŠ¿å›¾ç‰‡å·²ä¸Šä¼ åˆ°ä¸ƒç‰›äº‘: {cloud_url}")
                                else:
                                    logger.error(f"ä¸Šä¼ æ ¸å¿ƒæŒ‡æ ‡è¶‹åŠ¿å›¾ç‰‡åˆ°ä¸ƒç‰›äº‘å¤±è´¥: {error}")
                            except Exception as upload_error:
                                logger.error(f"ä¸Šä¼ æ ¸å¿ƒæŒ‡æ ‡è¶‹åŠ¿å›¾ç‰‡åˆ°ä¸ƒç‰›äº‘æ—¶å‡ºé”™: {upload_error}")
                        else:
                            logger.warning("æ ¸å¿ƒæŒ‡æ ‡è¶‹åŠ¿å›¾ç‰‡ç”Ÿæˆå¤±è´¥")
                    else:
                        logger.info("æŒ‡æ ‡åºåˆ—æ•°æ®ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡å›¾ç‰‡ç”Ÿæˆ")

                except Exception as e:
                    logger.error(f"ç”Ÿæˆæˆ–ä¸Šä¼ æ ¸å¿ƒæŒ‡æ ‡è¶‹åŠ¿å›¾ç‰‡æ—¶å‡ºé”™: {e}")
                    import traceback
                    logger.error(traceback.format_exc())

            # ä¿å­˜æ‚£è€…æ•°æ®åˆ°è¾“å‡ºç›®å½•ï¼ˆä¸intent_determine_crewç›¸åŒçš„sessionç›®å½•ï¼‰
            if agent_session_id:
                output_file_path = self._save_patient_data_to_output(
                    agent_session_id,
                    preprocessed_info,
                    result_data["full_structure_data"],
                    result_data.get("patient_journey"),
                    result_data.get("mdt_simple_report")
                )
                if output_file_path:
                    logger.info(f"æ‚£è€…æ•°æ®å·²ä¿å­˜åˆ°è¾“å‡ºç›®å½•: {output_file_path}")
                else:
                    logger.warning("ä¿å­˜æ‚£è€…æ•°æ®åˆ°è¾“å‡ºç›®å½•å¤±è´¥")
            else:
                logger.warning("No agent_session_id provided, skipping patient data save")

            # ========== æ€»ä½“è€—æ—¶ç»Ÿè®¡ ==========
            overall_duration = time.time() - overall_start_time
            logger.info("=" * 80)
            logger.info("æ‚£è€…æ•°æ®å¤„ç†æµç¨‹å®Œæˆ - è€—æ—¶ç»Ÿè®¡")
            logger.info("=" * 80)
            logger.info(f"ã€é˜¶æ®µ1ã€‘æ–‡ä»¶é¢„å¤„ç†:        {file_preprocessing_duration:.2f} ç§’ ({file_preprocessing_duration/60:.2f} åˆ†é’Ÿ) - {(file_preprocessing_duration/overall_duration*100):.1f}%")
            logger.info(f"ã€é˜¶æ®µ2ã€‘ç–¾ç—…é…ç½®è¯†åˆ«:      {disease_config_duration:.2f} ç§’ ({disease_config_duration/60:.2f} åˆ†é’Ÿ) - {(disease_config_duration/overall_duration*100):.1f}%")
            logger.info(f"ã€é˜¶æ®µ3ã€‘æ‚£è€…æ•°æ®å¤„ç†:      {patient_data_processing_duration:.2f} ç§’ ({patient_data_processing_duration/60:.2f} åˆ†é’Ÿ) - {(patient_data_processing_duration/overall_duration*100):.1f}%")
            logger.info(f"ã€é˜¶æ®µ4ã€‘æ‚£è€…æ—…ç¨‹æå–:      {patient_journey_duration:.2f} ç§’ ({patient_journey_duration/60:.2f} åˆ†é’Ÿ) - {(patient_journey_duration/overall_duration*100):.1f}%")
            logger.info(f"ã€é˜¶æ®µ5ã€‘MDTæŠ¥å‘Šç”Ÿæˆ:       {mdt_report_duration:.2f} ç§’ ({mdt_report_duration/60:.2f} åˆ†é’Ÿ) - {(mdt_report_duration/overall_duration*100):.1f}%")
            logger.info("-" * 80)
            logger.info(f"ã€æ€»è®¡ã€‘æ•´ä½“å¤„ç†æ—¶é—´:       {overall_duration:.2f} ç§’ ({overall_duration/60:.2f} åˆ†é’Ÿ)")
            logger.info("=" * 80)

            # å‘é€æœ€ç»ˆå¤„ç†å®Œæˆè¿›åº¦
            yield {"type": "progress", "stage": "finalizing", "message": "å¤„ç†å®Œæˆï¼Œæ­£åœ¨æ•´ç†ç»“æœ", "progress": 95}

            # yield æœ€ç»ˆç»“æœ
            yield {"type": "result", "data": result_data}

        except Exception as e:
            logger.error(f"Error in patient data processing: {e}")
            yield {"type": "result", "data": {"error": str(e)}} 

