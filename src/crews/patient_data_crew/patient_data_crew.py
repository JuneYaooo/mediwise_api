import os
from dotenv import load_dotenv
load_dotenv()
from crewai import Agent, Crew, Process, Task, LLM
from crewai.project import CrewBase, agent, crew, task
from src.llms import *
from src.utils.json_utils import JsonUtils
from src.utils.logger import BeijingLogger
from datetime import datetime
import re
import unicodedata
import concurrent.futures
from src.custom_tools.get_disease_list_tool import get_disease_list_tool
from src.custom_tools.query_disease_config_tool import query_disease_config_tool
from pathlib import Path
import time
import json
import uuid as uuid_lib
from src.custom_tools.patient_journey_image_generator import generate_patient_journey_image_sync
from src.custom_tools.indicator_chart_image_generator import generate_indicator_chart_image_sync
from app.utils.qiniu_upload_service import QiniuUploadService
from app.utils.file_metadata_builder import FileMetadataBuilder  # æ–°å¢å¯¼å…¥

# åˆå§‹åŒ– logger
logger = BeijingLogger().get_logger()

@CrewBase
class PatientDataCrew():
    """Patient data processing crew"""

    agents_config = "config/agents.yaml"
    tasks_config = "config/tasks.yaml"
    max_concurrency = 10  # é»˜è®¤æœ€å¤§å¹¶å‘æ•°
    
    def __init__(self, max_concurrency=None):
        """
        åˆå§‹åŒ–PatientDataCrew

        Args:
            max_concurrency (int, optional): æœ€å¤§å¹¶å‘å¤„ç†æ•°ï¼Œé»˜è®¤ä¸ºNoneï¼Œå°†ä½¿ç”¨ç±»å˜é‡é»˜è®¤å€¼
        """
        if max_concurrency is not None:
            self.max_concurrency = max_concurrency

    @staticmethod
    def estimate_tokens(text):
        """
        ä¼°ç®—æ–‡æœ¬ä¸­çš„tokenæ•°é‡ï¼ŒåŸºäºä»¥ä¸‹è§„åˆ™ï¼š
        - è‹±æ–‡å­—ç¬¦: çº¦ 0.3 ä¸ªtoken/å­—ç¬¦
        - ä¸­æ–‡å­—ç¬¦: çº¦ 0.6 ä¸ªtoken/å­—ç¬¦
        - å…¶ä»–å­—ç¬¦: çº¦ 0.5 ä¸ªtoken/å­—ç¬¦ï¼ˆä¿å®ˆä¼°è®¡ï¼‰
        
        Args:
            text (str): è¾“å…¥æ–‡æœ¬
            
        Returns:
            float: ä¼°ç®—çš„tokenæ•°é‡
        """
        if not text:
            return 0
            
        # è®¡æ•°å™¨åˆå§‹åŒ–
        english_chars = 0
        chinese_chars = 0
        other_chars = 0
        
        # éå†æ–‡æœ¬ä¸­çš„æ¯ä¸ªå­—ç¬¦
        for char in text:
            # è·³è¿‡ç©ºç™½å­—ç¬¦
            if char.isspace():
                continue
                
            # æ£€æŸ¥æ˜¯å¦ä¸ºASCIIèŒƒå›´å†…çš„è‹±æ–‡å­—ç¬¦
            if ord(char) < 128 and (char.isalpha() or char.isdigit() or char in ",.!?;:'\"()[]{}"):
                english_chars += 1
            # æ£€æŸ¥æ˜¯å¦ä¸ºä¸­æ–‡å­—ç¬¦
            elif any([
                'CJK' in unicodedata.name(char, ''),
                'HIRAGANA' in unicodedata.name(char, ''),
                'KATAKANA' in unicodedata.name(char, ''),
                'IDEOGRAPHIC' in unicodedata.name(char, '')
            ]):
                chinese_chars += 1
            # å…¶ä»–å­—ç¬¦
            else:
                other_chars += 1
        
        # æ ¹æ®ä¸åŒå­—ç¬¦ç±»å‹è®¡ç®—tokenä¼°ç®—å€¼
        estimated_tokens = (
            english_chars * 0.3 +  # è‹±æ–‡å­—ç¬¦
            chinese_chars * 0.6 +  # ä¸­æ–‡å­—ç¬¦
            other_chars * 0.5      # å…¶ä»–å­—ç¬¦
        )
        
        # ä¿å®ˆèµ·è§ï¼Œå‘ä¸Šå–æ•´å¹¶æ·»åŠ ä¸€ç‚¹é¢å¤–ç¼“å†²
        return int(estimated_tokens * 1.1) + 1

    def _save_patient_data_to_output(self, session_id, patient_content, full_structure_data, patient_journey=None, mdt_simple_report=None):
        """å°†æ‚£è€…æ•°æ®ä¿å­˜åˆ°è¾“å‡ºç›®å½•"""
        try:
            if not session_id:
                logger.warning("No session_id provided, skipping patient data save")
                return None
            
            # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„ï¼ˆä¸intent_determine_crewç›¸åŒçš„ç›®å½•ç»“æ„ï¼‰
            output_dir = Path("output/files_extract") / session_id
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # ç¡®ä¿æ•°æ®ä¸­çš„Unicodeç¼–ç è¢«æ­£ç¡®è§£ç 
            def decode_unicode_recursive(obj):
                """é€’å½’è§£ç å¯¹è±¡ä¸­çš„Unicodeè½¬ä¹‰åºåˆ—"""
                if isinstance(obj, dict):
                    return {key: decode_unicode_recursive(value) for key, value in obj.items()}
                elif isinstance(obj, list):
                    return [decode_unicode_recursive(item) for item in obj]
                elif isinstance(obj, str):
                    try:
                        # å¤„ç†Unicodeè½¬ä¹‰åºåˆ—
                        if '\\u' in obj:
                            return obj.encode().decode('unicode_escape')
                        return obj
                    except Exception:
                        return obj
                else:
                    return obj
            
            # ğŸš¨ ç®€åŒ–ï¼šç›´æ¥å‡†å¤‡è¦ä¿å­˜çš„æ•°æ®ï¼Œä¸éœ€è¦å¤æ‚çš„å†å²è®°å½•
            patient_data = {
                "session_id": session_id,
                "timestamp": time.time(),
                "processing_date": datetime.now().isoformat(),
                "patient_content": decode_unicode_recursive(patient_content) if isinstance(patient_content, str) else patient_content,
                "full_structure_data": decode_unicode_recursive(full_structure_data),
                "patient_journey": decode_unicode_recursive(patient_journey) if patient_journey is not None else None,
                "mdt_simple_report": decode_unicode_recursive(mdt_simple_report) if mdt_simple_report is not None else None
            }
            
            # å¦‚æœå·²å­˜åœ¨æ–‡ä»¶ï¼Œå…ˆå¤‡ä»½
            output_file = output_dir / "patient_data.json"
            if output_file.exists():
                # åˆ›å»ºå¤‡ä»½æ–‡ä»¶
                backup_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                backup_file = output_dir / f"patient_data_backup_{backup_timestamp}.json"
                import shutil
                shutil.copy2(output_file, backup_file)
                logger.info(f"å·²åˆ›å»ºå¤‡ä»½æ–‡ä»¶: {backup_file}")
            
            # ä¿å­˜åˆ°JSONæ–‡ä»¶
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(patient_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æ‚£è€…æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
            
            return str(output_file)
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ‚£è€…æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            return None
    
    @agent
    def file_preprocessor(self) -> Agent:
        return Agent(
            config=self.agents_config['file_preprocessor'],
            llm=general_llm,
            verbose=True
        )

    @agent
    def disease_config_agent(self) -> Agent:
        return Agent(
            config=self.agents_config['disease_config_agent'],
            tools=[get_disease_list_tool, query_disease_config_tool],
            llm=document_generation_llm,
            verbose=True
        )

    @agent
    def patient_data_processor(self) -> Agent:
        return Agent(
            config=self.agents_config['patient_data_processor'],
            llm=document_generation_llm,
            verbose=True
        )

    @agent
    def core_points_extractor(self) -> Agent:
        return Agent(
            config=self.agents_config['core_points_extractor'],
            llm=document_generation_llm,
            verbose=True
        )

    @agent
    def mdt_report_generator(self) -> Agent:
        return Agent(
            config=self.agents_config['mdt_report_generator'],
            llm=document_generation_llm,
            verbose=True
        )
    
    @task
    def preprocess_files_task(self) -> Task:
        return Task(
            config=self.tasks_config['preprocess_files_task']
        )

    @task
    def get_disease_config_task(self) -> Task:
        return Task(
            config=self.tasks_config['get_disease_config_task']
        )

    @task
    def process_patient_data_task(self) -> Task:
        return Task(
            config=self.tasks_config['process_patient_data_task'],
            context=[self.get_disease_config_task()]  # ä¾èµ–ç–¾ç—…é…ç½®ä»»åŠ¡çš„è¾“å‡º
        )

    @task
    def extract_core_points_task(self) -> Task:
        return Task(
            config=self.tasks_config['extract_core_points_task'],
            context=[self.get_disease_config_task()]  # ä¾èµ–ç–¾ç—…é…ç½®ä»»åŠ¡çš„è¾“å‡º
        )

    @task
    def generate_mdt_report_task(self) -> Task:
        return Task(
            config=self.tasks_config['generate_mdt_report_task'],
            context=[self.get_disease_config_task()]  # ä¾èµ–ç–¾ç—…é…ç½®ä»»åŠ¡çš„è¾“å‡º
        )

    @crew
    def crew(self) -> Crew:
        """Creates the patient data processing crew with 30-minute timeout"""
        return Crew(
            agents=self.agents,
            tasks=self.tasks,
            process=Process.sequential,
            verbose=True,
            max_execution_time=1800  # 30 minutes timeout (30 * 60 = 1800 seconds)
        )

    def get_structured_patient_data(self, patient_info, patient_timeline, messages, files, agent_session_id, existing_patient_data=None):
        """
        Process patient information into a structured timeline with detailed categorized information.
        æ”¯æŒå¢é‡æ›´æ–°ï¼šå¦‚æœå­˜åœ¨ç°æœ‰æ‚£è€…æ•°æ®ï¼Œå°†æ–°ä¿¡æ¯ä¸ç°æœ‰ä¿¡æ¯åˆå¹¶æ›´æ–°ã€‚

        Args:
            patient_info (str): Raw patient information text
            patient_timeline (str): Current patient timeline (may be empty for new patients)
            messages (list): Conversation messages history
            files (list): List of file objects with their content
            agent_session_id (str): The session ID for the agent
            existing_patient_data (dict): ç°æœ‰æ‚£è€…æ•°æ®ï¼ŒåŒ…å«timelineã€journeyã€mdt_reportç­‰
        Returns:
            dict: Structured patient data with timeline and categorized details
        """
        # å°†ç”Ÿæˆå™¨ç‰ˆæœ¬çš„ç»“æœæ”¶é›†èµ·æ¥è¿”å›
        result = None
        for progress_data in self.get_structured_patient_data_stream(
            patient_info=patient_info,
            patient_timeline=patient_timeline,
            messages=messages,
            files=files,
            agent_session_id=agent_session_id,
            existing_patient_data=existing_patient_data
        ):
            if progress_data.get("type") == "result":
                result = progress_data.get("data")
        return result if result else {"error": "No result returned"}

    def get_structured_patient_data_stream(self, patient_info, patient_timeline, messages, files, agent_session_id, existing_patient_data=None):
        """
        Process patient information into a structured timeline (generator version).
        å®æ—¶è¿”å›å¤„ç†è¿›åº¦å’Œæœ€ç»ˆç»“æœã€‚

        Args:
            patient_info (str): Raw patient information text
            patient_timeline (str): Current patient timeline (may be empty for new patients)
            messages (list): Conversation messages history
            files (list): List of file objects with their content
            agent_session_id (str): The session ID for the agent
            existing_patient_data (dict): ç°æœ‰æ‚£è€…æ•°æ®ï¼ŒåŒ…å«timelineã€journeyã€mdt_reportç­‰

        Yields:
            dict: Progress updates or final result
                - type: "progress" or "result"
                - For progress: stage, message, progress (0-100)
                - For result: data (final result dict)
        """
        try:
            # ========== æ€»ä½“å¼€å§‹æ—¶é—´ ==========
            overall_start_time = time.time()
            logger.info("=" * 80)
            logger.info("å¼€å§‹æ‚£è€…æ•°æ®å¤„ç†æµç¨‹")
            logger.info("=" * 80)
            
            # è®¾ç½®å½“å‰æ—¥æœŸ
            current_date = datetime.now().strftime("%Y-%m-%d")
            
            # ğŸš¨ ä¿®æ”¹ï¼šä½¿ç”¨ä¼ å…¥çš„existing_patient_dataå‚æ•°è€Œä¸æ˜¯ä»æœ¬åœ°æ–‡ä»¶åŠ è½½
            existing_timeline = None
            existing_patient_journey = None
            existing_mdt_report = None
            
            if existing_patient_data:
                logger.info("Found existing patient data from database, will perform incremental update")
                
                # å®‰å…¨åœ°è·å–ç°æœ‰æ•°æ®ï¼Œå¤„ç†å¯èƒ½ä¸ºNoneçš„æƒ…å†µ
                patient_timeline_data = existing_patient_data.get("patient_timeline")
                existing_timeline = patient_timeline_data.get("timeline", []) if patient_timeline_data else []
                
                existing_patient_journey = existing_patient_data.get("patient_journey")
                if existing_patient_journey is None:
                    existing_patient_journey = {}
                
                existing_mdt_report = existing_patient_data.get("mdt_simple_report")
                if existing_mdt_report is None:
                    existing_mdt_report = {}
                
                logger.info(f"Existing data contains {len(existing_timeline)} timeline entries")
                
                # è®°å½•ç°æœ‰æ•°æ®çš„è¯¦ç»†ä¿¡æ¯
                if existing_patient_journey and "timeline_journey" in existing_patient_journey:
                    logger.info(f"Existing patient journey contains {len(existing_patient_journey['timeline_journey'])} journey events")
                if existing_patient_journey and "indicator_series" in existing_patient_journey:
                    logger.info(f"Existing patient journey contains {len(existing_patient_journey['indicator_series'])} indicator series")
                if existing_mdt_report:
                    logger.info(f"Existing MDT report contains data")
            else:
                logger.info("No existing patient data found, will create new patient record")
            
            # ========== é˜¶æ®µ1: æ–‡ä»¶é¢„å¤„ç† ==========
            file_preprocessing_start_time = time.time()
            logger.info("-" * 80)
            logger.info("ã€é˜¶æ®µ1ã€‘å¼€å§‹æ–‡ä»¶é¢„å¤„ç†")
            logger.info("-" * 80)

            # å‘é€è¿›åº¦æ›´æ–°
            yield {"type": "progress", "stage": "file_preprocessing", "message": "æ­£åœ¨é¢„å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶", "progress": 10}

            # ç¡®å®šæ˜¯å¦éœ€è¦æ–‡ä»¶é¢„å¤„ç†
            if not files or len(files) == 0:
                logger.info("No files to process, skipping file preprocessing step")
                # å°†messagesè½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼
                if isinstance(messages, list):
                    messages_text = "\n".join([str(msg) for msg in messages if msg])
                    preprocessed_info = f"{patient_info}\n\nå¯¹è¯å†å²:\n{messages_text}" if messages_text else patient_info
                else:
                    preprocessed_info = str(messages) if messages else patient_info
            else:
                # è¿‡æ»¤æ–‡ä»¶ï¼šè·³è¿‡ä»PDFæå–çš„å›¾ç‰‡ï¼Œé¿å…é‡å¤
                # å› ä¸ºPDFçš„extracted_textå·²ç»åŒ…å«äº†å›¾ç‰‡æè¿°
                filtered_files = FileMetadataBuilder.filter_for_llm_input(files)
                logger.info(f"è¿‡æ»¤åç”¨äºLLMçš„æ–‡ä»¶æ•°: {len(filtered_files)} (åŸå§‹: {len(files)})")

                # é¦–å…ˆè®¡ç®—æ‰€æœ‰æ–‡ä»¶çš„æ€»tokenæ•°
                total_file_tokens = 0
                valid_file_count = 0
                for file in filtered_files:
                    # ä¼˜å…ˆä½¿ç”¨file_contentï¼Œå…¼å®¹extracted_text
                    file_content = file.get('file_content') or file.get('extracted_text', '')
                    # åªè®¡ç®—æœ‰å†…å®¹æ–‡ä»¶çš„tokenæ•°
                    if file_content and file_content.strip():
                        file_tokens = self.estimate_tokens(file_content)
                        total_file_tokens += file_tokens
                        valid_file_count += 1
                    else:
                        logger.warning(f"Skipping empty file in token calculation: {file.get('file_name', 'æœªå‘½åæ–‡ä»¶')}")

                logger.info(f"Total tokens for {valid_file_count} valid files: {total_file_tokens} (out of {len(filtered_files)} total files)")

                # å¦‚æœæ€»tokenæ•°ä¸è¶…è¿‡50000ï¼Œè·³è¿‡æ–‡ä»¶é¢„å¤„ç†æ­¥éª¤
                if total_file_tokens <= 50000:
                    logger.info("Files token count doesn't exceed 50000, skipping file preprocessing step")
                    # ç›´æ¥åˆå¹¶æ‰€æœ‰æ–‡ä»¶å†…å®¹
                    files_content = []
                    for file in filtered_files:
                        file_name = file.get('file_name', 'æœªå‘½åæ–‡ä»¶')
                        # ä¼˜å…ˆä½¿ç”¨file_contentï¼Œå…¼å®¹extracted_text
                        file_content = file.get('file_content') or file.get('extracted_text', '')
                        file_uuid = file.get('file_uuid', '')

                        # åªå¤„ç†æœ‰å†…å®¹çš„æ–‡ä»¶ï¼Œè·³è¿‡ç©ºå†…å®¹æ–‡ä»¶
                        if file_content and file_content.strip():
                            files_content.append(f"æ–‡ä»¶UUID: {file_uuid}\nå†…å®¹:\n{file_content}")
                            logger.info(f"Added file content: {file_name} (UUID: {file_uuid}) ({len(file_content)} chars)")
                        else:
                            logger.warning(f"Skipping file with empty content: {file_name} (UUID: {file_uuid})")

                    if files_content:
                        preprocessed_info = f"{patient_info}\n\næ–‡ä»¶æå–çš„æ‚£è€…ä¿¡æ¯:\n" + "\n\n".join(files_content)
                    else:
                        logger.info("No valid file content found, using only patient_info and messages")
                        preprocessed_info = patient_info
                else:
                    logger.info(f"Preprocessing {len(filtered_files)} files (total tokens: {total_file_tokens})")
                    
                    # æ–‡ä»¶é¢„å¤„ç†æ­¥éª¤
                    preprocessed_info = patient_info
                    
                    # å‡†å¤‡æ–‡ä»¶é¢„å¤„ç†ä»»åŠ¡çš„è¾“å…¥
                    max_tokens_per_batch = 88000 # æ¯æ‰¹æ¬¡å¤„ç†çš„æœ€å¤§tokenæ•° ï¼Œç°åœ¨ç”¨qwen 128k çš„æ¨¡å‹ï¼Œæ‰€ä»¥è®¾ç½®ä¸º75000
                    max_tokens_per_chunk = 88000  # å•ä¸ªæ–‡ä»¶å—çš„æœ€å¤§tokenæ•°ï¼Œç°åœ¨ç”¨qwen 128k çš„æ¨¡å‹ï¼Œæ‰€ä»¥è®¾ç½®ä¸º75000

                    # éå†å¤„ç†æ‰€æœ‰æ–‡ä»¶ï¼Œæ”¶é›†æ‰€æœ‰æ‰¹æ¬¡
                    all_batches = []
                    current_batch = []
                    current_batch_tokens = 0

                    # ä½¿ç”¨è¿‡æ»¤åçš„æ–‡ä»¶åˆ—è¡¨
                    for file in filtered_files:
                        file_name = file.get('file_name', 'æœªå‘½åæ–‡ä»¶')
                        # ä¼˜å…ˆä½¿ç”¨file_contentï¼Œå…¼å®¹extracted_text
                        file_content = file.get('file_content') or file.get('extracted_text', '')
                        file_uuid = file.get('file_uuid', '')

                        # è·³è¿‡ç©ºå†…å®¹æ–‡ä»¶
                        if not file_content or not file_content.strip():
                            logger.warning(f"Skipping file with empty content during preprocessing: {file_name} (UUID: {file_uuid})")
                            continue
                            
                        file_tokens = self.estimate_tokens(file_content)
                        
                        logger.info(f"File '{file_name}' (UUID: {file_uuid}): {len(file_content)} chars, estimated {file_tokens} tokens")
                        
                        # å¤„ç†å¤§æ–‡ä»¶ - åˆ‡åˆ†ä¸ºå¤šä¸ªå—
                        if file_tokens > max_tokens_per_chunk:
                            logger.info(f"Splitting large file '{file_name}' ({file_tokens} tokens) into chunks")
                            
                            # ä¼°è®¡æ¯ä¸ªå­—ç¬¦çš„å¹³å‡tokenæ•°
                            avg_tokens_per_char = file_tokens / len(file_content) if len(file_content) > 0 else 0.5
                            # ä¼°ç®—æ¯ä¸ªå—çš„æœ€å¤§å­—ç¬¦æ•°
                            chars_per_chunk = int(max_tokens_per_chunk / avg_tokens_per_char) if avg_tokens_per_char > 0 else 20000
                            
                            # è®¡ç®—éœ€è¦çš„å—æ•°
                            num_chunks = (len(file_content) + chars_per_chunk - 1) // chars_per_chunk
                            
                            for chunk_idx in range(num_chunks):
                                start_pos = chunk_idx * chars_per_chunk
                                end_pos = min((chunk_idx + 1) * chars_per_chunk, len(file_content))
                                chunk_content = file_content[start_pos:end_pos]
                                chunk_tokens = self.estimate_tokens(chunk_content)
                                
                                logger.info(f"  Chunk {chunk_idx+1}/{num_chunks}: {len(chunk_content)} chars, estimated {chunk_tokens} tokens")
                                
                                # æ£€æŸ¥å½“å‰æ‰¹æ¬¡æ˜¯å¦ä¼šè¶…å‡ºtokené™åˆ¶
                                if current_batch_tokens + chunk_tokens > max_tokens_per_batch:
                                    # æ·»åŠ å½“å‰æ‰¹æ¬¡åˆ°æ‰€æœ‰æ‰¹æ¬¡åˆ—è¡¨
                                    if current_batch:
                                        all_batches.append(list(current_batch))
                                    
                                    # é‡ç½®æ‰¹æ¬¡
                                    current_batch = []
                                    current_batch_tokens = 0
                                
                                # æ·»åŠ æ–‡ä»¶å—åˆ°å½“å‰æ‰¹æ¬¡
                                current_batch.append({
                                    "file_name": f"{file_name} (Part {chunk_idx+1}/{num_chunks})",
                                    "file_content": chunk_content,
                                    "file_uuid": file_uuid
                                })
                                current_batch_tokens += chunk_tokens
                        else:
                            # å¤„ç†æ ‡å‡†å¤§å°æ–‡ä»¶
                            # æ£€æŸ¥å½“å‰æ‰¹æ¬¡æ˜¯å¦ä¼šè¶…å‡ºtokené™åˆ¶
                            if current_batch_tokens + file_tokens > max_tokens_per_batch:
                                # æ·»åŠ å½“å‰æ‰¹æ¬¡åˆ°æ‰€æœ‰æ‰¹æ¬¡åˆ—è¡¨
                                if current_batch:
                                    all_batches.append(list(current_batch))
                                
                                # é‡ç½®æ‰¹æ¬¡
                                current_batch = []
                                current_batch_tokens = 0
                            
                            # æ·»åŠ åˆ°å½“å‰æ‰¹æ¬¡
                            current_batch.append({
                                "file_name": file_name,
                                "file_content": file_content,
                                "file_uuid": file_uuid
                            })
                            current_batch_tokens += file_tokens
                    
                    # æ·»åŠ æœ€åä¸€ä¸ªæ‰¹æ¬¡
                    if current_batch:
                        all_batches.append(list(current_batch))
                    
                    logger.info(f"Prepared {len(all_batches)} batches for processing")
                    
                    # ä½¿ç”¨å¹¶å‘å¤„ç†æ‰¹æ¬¡
                    all_preprocessed_content = []
                    all_batch_inputs = []
                    
                    # å®šä¹‰æ‰¹æ¬¡å¤„ç†å‡½æ•°
                    def process_batch(batch):
                        batch_input = {
                            "files_batch": batch,
                            "patient_info": patient_info,
                            "current_date": current_date
                        }
                        all_batch_inputs.append(batch_input)  # ä¿å­˜batch_input
                        self.preprocess_files_task().interpolate_inputs_and_add_conversation_history(batch_input)
                        return self.file_preprocessor().execute_task(self.preprocess_files_task())
                    
                    # è·å–æœ€å¤§å¹¶å‘æ•°
                    max_concurrent = min(self.max_concurrency, len(all_batches))
                    logger.info(f"Processing {len(all_batches)} batches with maximum {max_concurrent} concurrent workers")
                    
                    # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¹¶å‘å¤„ç†
                    completed_batches = 0
                    with concurrent.futures.ThreadPoolExecutor(max_workers=max_concurrent) as executor:
                        # æäº¤æ‰€æœ‰æ‰¹æ¬¡å¤„ç†ä»»åŠ¡
                        future_to_batch = {executor.submit(process_batch, batch): i for i, batch in enumerate(all_batches)}

                        # æ”¶é›†æ‰€æœ‰ç»“æœ
                        for future in concurrent.futures.as_completed(future_to_batch):
                            batch_idx = future_to_batch[future]
                            try:
                                result = future.result()
                                all_preprocessed_content.append(result)
                                completed_batches += 1
                                logger.info(f"Completed processing batch {batch_idx+1}/{len(all_batches)}")

                                # å‘é€æ–‡ä»¶æ‰¹æ¬¡å¤„ç†è¿›åº¦ï¼ˆ10-30%ä¹‹é—´ï¼‰
                                batch_progress = 10 + int(20 * completed_batches / len(all_batches))
                                yield {"type": "progress", "stage": "file_preprocessing", "message": f"æ­£åœ¨å¤„ç†æ–‡ä»¶æ‰¹æ¬¡ {completed_batches}/{len(all_batches)}", "progress": batch_progress}
                            except Exception as e:
                                logger.error(f"Error processing batch {batch_idx+1}: {str(e)}")
                    
                    # åˆå¹¶æ‰€æœ‰é¢„å¤„ç†ç»“æœ
                    if all_preprocessed_content:
                        # ä¿å­˜é¢„å¤„ç†ç»“æœåˆ°æœ¬åœ°æ–‡ä»¶
                        try:
                            # ç¡®ä¿ç›®å½•å­˜åœ¨
                            log_dir = "logs/patient_data_preprocessed"
                            os.makedirs(log_dir, exist_ok=True)
                            
                            # åˆ›å»ºå¸¦æœ‰æ—¶é—´æˆ³çš„æ–‡ä»¶å
                            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                            
                            # ä¿å­˜é¢„å¤„ç†è¾“å‡ºç»“æœ
                            output_filename = f"{log_dir}/patient_data_preprocessed_{timestamp}.json"
                            # å‡†å¤‡JSONæ•°æ®
                            output_json_data = {
                                "timestamp": datetime.now().isoformat(),
                                "content_count": len(all_preprocessed_content),
                                "preprocessed_content": all_preprocessed_content
                            }
                            # å†™å…¥JSONæ–‡ä»¶
                            JsonUtils.safe_json_dump(output_json_data, output_filename)
                            
                            # ä¿å­˜é¢„å¤„ç†è¾“å…¥æ•°æ®
                            input_filename = f"{log_dir}/patient_data_input_{timestamp}.json"
                            # å‡†å¤‡JSONæ•°æ®
                            input_json_data = {
                                "timestamp": datetime.now().isoformat(),
                                "batch_count": len(all_batch_inputs),
                                "batch_inputs": all_batch_inputs
                            }
                            # å†™å…¥JSONæ–‡ä»¶
                            JsonUtils.safe_json_dump(input_json_data, input_filename)
                            
                            logger.info(f"Preprocessed content saved to {output_filename}")
                            logger.info(f"Batch input data saved to {input_filename}")
                        except Exception as e:
                            logger.error(f"Failed to save preprocessed content: {e}")
                        
                        preprocessed_info = f"{patient_info}\n\næ–‡ä»¶æå–çš„æ‚£è€…ä¿¡æ¯:\n" + "\n\n".join(all_preprocessed_content)
                        preprocessed_tokens = self.estimate_tokens(preprocessed_info)
                        logger.info(f"Files preprocessing completed, combined result: {len(preprocessed_info)} chars, estimated {preprocessed_tokens} tokens")

            # è®°å½•æ–‡ä»¶é¢„å¤„ç†è€—æ—¶
            file_preprocessing_duration = time.time() - file_preprocessing_start_time
            logger.info("-" * 80)
            logger.info(f"ã€é˜¶æ®µ1ã€‘æ–‡ä»¶é¢„å¤„ç†å®Œæˆï¼Œè€—æ—¶: {file_preprocessing_duration:.2f} ç§’ ({file_preprocessing_duration/60:.2f} åˆ†é’Ÿ)")
            logger.info("-" * 80)

            # å‘é€æ–‡ä»¶é¢„å¤„ç†å®Œæˆè¿›åº¦
            yield {"type": "progress", "stage": "file_preprocessing_completed", "message": "æ–‡ä»¶é¢„å¤„ç†å®Œæˆ", "progress": 30}

            # ä½¿ç”¨é¢„å¤„ç†åçš„æ‚£è€…ä¿¡æ¯æ‰§è¡ŒåŸå§‹ä»»åŠ¡
            # ========== é˜¶æ®µ2: ç–¾ç—…é…ç½®è¯†åˆ« ==========
            disease_config_start_time = time.time()
            logger.info("-" * 80)
            logger.info("ã€é˜¶æ®µ2ã€‘å¼€å§‹ç–¾ç—…é…ç½®è¯†åˆ«")
            logger.info("-" * 80)

            # å‘é€è¿›åº¦æ›´æ–°
            yield {"type": "progress", "stage": "disease_config", "message": "æ­£åœ¨è¯†åˆ«ç–¾ç—…é…ç½®", "progress": 35}

            disease_config_inputs = {
                "patient_info": preprocessed_info
            }
            self.get_disease_config_task().interpolate_inputs_and_add_conversation_history(disease_config_inputs)
            disease_config_result = self.disease_config_agent().execute_task(self.get_disease_config_task())

            # è®°å½•ç–¾ç—…é…ç½®è¯†åˆ«è€—æ—¶
            disease_config_duration = time.time() - disease_config_start_time
            logger.info("-" * 80)
            logger.info(f"ã€é˜¶æ®µ2ã€‘ç–¾ç—…é…ç½®è¯†åˆ«å®Œæˆï¼Œè€—æ—¶: {disease_config_duration:.2f} ç§’ ({disease_config_duration/60:.2f} åˆ†é’Ÿ)")
            logger.info("-" * 80)

            # è§£æç–¾ç—…é…ç½®ç»“æœ
            disease_config_data = JsonUtils.safe_parse_json(disease_config_result, debug_prefix="Disease config identification")
            if disease_config_data:
                logger.info(f"Identified diseases config: {disease_config_data.get('status', 'unknown')}")
                if disease_config_data.get('configs'):
                    logger.info(f"Found {len(disease_config_data['configs'])} disease configurations")
            else:
                logger.warning("Failed to parse disease config result, will proceed without specific disease config")
                disease_config_data = {"status": "error", "configs": []}

            # å‘é€ç–¾ç—…é…ç½®è¯†åˆ«å®Œæˆè¿›åº¦
            yield {"type": "progress", "stage": "disease_config_completed", "message": "ç–¾ç—…é…ç½®è¯†åˆ«å®Œæˆ", "progress": 45}

            # ========== é˜¶æ®µ3: æ‚£è€…æ•°æ®å¤„ç†ï¼ˆæ—¶é—´è½´ç”Ÿæˆï¼‰ ==========
            patient_data_processing_start_time = time.time()
            logger.info("-" * 80)
            logger.info("ã€é˜¶æ®µ3ã€‘å¼€å§‹æ‚£è€…æ•°æ®å¤„ç†ï¼ˆæ—¶é—´è½´ç”Ÿæˆï¼‰")
            logger.info("-" * 80)

            # å‘é€è¿›åº¦æ›´æ–°
            yield {"type": "progress", "stage": "timeline_generation", "message": "æ­£åœ¨ç”Ÿæˆæ‚£è€…æ—¶é—´è½´", "progress": 50}

            # æ­¥éª¤2: æ‰§è¡Œæ‚£è€…æ•°æ®å¤„ç†ä»»åŠ¡ï¼Œå°†ç–¾ç—…é…ç½®ä½œä¸ºä¸Šä¸‹æ–‡ä¼ é€’
            inputs = {
                "patient_info": preprocessed_info,
                "patient_timeline": patient_timeline,
                "current_date": current_date,
                "existing_timeline": existing_timeline if existing_timeline else [],
                "disease_config": disease_config_data  # ä¼ é€’ç–¾ç—…é…ç½®
            }
            self.process_patient_data_task().interpolate_inputs_and_add_conversation_history(inputs)

            # æ‰§è¡Œä»»åŠ¡
            patient_data_result = self.patient_data_processor().execute_task(self.process_patient_data_task())

            # è®°å½•æ‚£è€…æ•°æ®å¤„ç†è€—æ—¶
            patient_data_processing_duration = time.time() - patient_data_processing_start_time
            logger.info("-" * 80)
            logger.info(f"ã€é˜¶æ®µ3ã€‘æ‚£è€…æ•°æ®å¤„ç†å®Œæˆï¼Œè€—æ—¶: {patient_data_processing_duration:.2f} ç§’ ({patient_data_processing_duration/60:.2f} åˆ†é’Ÿ)")
            logger.info("-" * 80)

            # è§£æç»“æœï¼Œç¡®ä¿å…¶ä¸ºæœ‰æ•ˆçš„JSON
            parsed_result = JsonUtils.safe_parse_json(patient_data_result, debug_prefix="Patient data processing")

            # é¢å¤–çš„Unicodeæ¸…ç†æ­¥éª¤ï¼Œç¡®ä¿æ²¡æœ‰é—æ¼çš„Unicodeç¼–ç 
            if parsed_result:
                parsed_result = JsonUtils._decode_unicode_in_dict(parsed_result)

            # å‘é€æ—¶é—´è½´ç”Ÿæˆå®Œæˆè¿›åº¦
            yield {"type": "progress", "stage": "timeline_generation_completed", "message": "æ‚£è€…æ—¶é—´è½´ç”Ÿæˆå®Œæˆ", "progress": 65}

            # ========== é˜¶æ®µ4: æ‚£è€…æ—…ç¨‹æå– ==========
            patient_journey_start_time = time.time()
            logger.info("-" * 80)
            logger.info("ã€é˜¶æ®µ4ã€‘å¼€å§‹æ‚£è€…æ—…ç¨‹æå–")
            logger.info("-" * 80)

            # å‘é€è¿›åº¦æ›´æ–°
            yield {"type": "progress", "stage": "patient_journey", "message": "æ­£åœ¨æå–æ‚£è€…æ—…ç¨‹æ•°æ®", "progress": 70}

            # æ‰§è¡Œ"æ‚£è€…æ—¶é—´æ—…ç¨‹"ä»»åŠ¡
            special_parsed_result = None
            try:
                core_inputs = {
                    "current_date": current_date,
                    "patient_content": preprocessed_info,
                    "full_structure_data": parsed_result if parsed_result else {},
                    "existing_patient_journey": existing_patient_journey if existing_patient_journey else {},
                    "disease_config": disease_config_data  # ä¼ é€’ç–¾ç—…é…ç½®
                }
                self.extract_core_points_task().interpolate_inputs_and_add_conversation_history(core_inputs)
                special_result = self.core_points_extractor().execute_task(self.extract_core_points_task())
                special_parsed_result = JsonUtils.safe_parse_json(special_result, debug_prefix="Patient journey extraction")
                
                # é¢å¤–çš„Unicodeæ¸…ç†æ­¥éª¤å’Œç»“æ„éªŒè¯
                if special_parsed_result:
                    special_parsed_result = JsonUtils._decode_unicode_in_dict(special_parsed_result)
                    # éªŒè¯æ˜¯å¦åŒ…å«é¢„æœŸçš„å­—æ®µ
                    if isinstance(special_parsed_result, dict):
                        expected_fields = ["timeline_journey", "indicator_series"]
                        missing_fields = [field for field in expected_fields if field not in special_parsed_result]
                        if missing_fields:
                            logger.warning(f"æ‚£è€…æ—¶é—´æ—…ç¨‹JSONç¼ºå°‘å­—æ®µ: {missing_fields}")
                        else:
                            logger.info(f"æˆåŠŸæå–æ‚£è€…æ—¶é—´æ—…ç¨‹ï¼ŒåŒ…å«{len(special_parsed_result.get('timeline_journey', []))}ä¸ªæ—¶é—´èŠ‚ç‚¹å’Œ{len(special_parsed_result.get('indicator_series', []))}ä¸ªæŒ‡æ ‡åºåˆ—")
                    else:
                        logger.warning("æ‚£è€…æ—¶é—´æ—…ç¨‹è§£æç»“æœä¸æ˜¯å­—å…¸æ ¼å¼")
                else:
                    logger.warning("æ‚£è€…æ—¶é—´æ—…ç¨‹è§£æç»“æœä¸ºç©º")
            except Exception as e:
                logger.error(f"Error in patient journey extraction: {e}")

            # è®°å½•æ‚£è€…æ—…ç¨‹æå–è€—æ—¶
            patient_journey_duration = time.time() - patient_journey_start_time
            logger.info("-" * 80)
            logger.info(f"ã€é˜¶æ®µ4ã€‘æ‚£è€…æ—…ç¨‹æå–å®Œæˆï¼Œè€—æ—¶: {patient_journey_duration:.2f} ç§’ ({patient_journey_duration/60:.2f} åˆ†é’Ÿ)")
            logger.info("-" * 80)

            # å‘é€æ‚£è€…æ—…ç¨‹æå–å®Œæˆè¿›åº¦
            yield {"type": "progress", "stage": "patient_journey_completed", "message": "æ‚£è€…æ—…ç¨‹æ•°æ®æå–å®Œæˆ", "progress": 80}

            # ========== é˜¶æ®µ5: MDTæŠ¥å‘Šç”Ÿæˆ ==========
            mdt_report_start_time = time.time()
            logger.info("-" * 80)
            logger.info("ã€é˜¶æ®µ5ã€‘å¼€å§‹MDTæŠ¥å‘Šç”Ÿæˆ")
            logger.info("-" * 80)

            # å‘é€è¿›åº¦æ›´æ–°
            yield {"type": "progress", "stage": "mdt_report", "message": "æ­£åœ¨ç”ŸæˆMDTæŠ¥å‘Š", "progress": 85}

            # æ‰§è¡ŒMDTæŠ¥å‘Šç”Ÿæˆä»»åŠ¡
            mdt_report_result = None
            try:
                mdt_inputs = {
                    "current_date": current_date,
                    "patient_content": preprocessed_info,
                    "patient_structured_data": parsed_result if parsed_result else {},
                    "existing_mdt_report": existing_mdt_report if existing_mdt_report else {},
                    "disease_config": disease_config_data  # ä¼ é€’ç–¾ç—…é…ç½®
                }
                self.generate_mdt_report_task().interpolate_inputs_and_add_conversation_history(mdt_inputs)
                mdt_result = self.mdt_report_generator().execute_task(self.generate_mdt_report_task())
                mdt_parsed_result = JsonUtils.safe_parse_json(mdt_result, debug_prefix="MDT report generation")
                
                # ğŸš¨ ä¿®å¤ï¼šæ­£ç¡®æå–mdt_simple_reportå­—æ®µ
                if mdt_parsed_result:
                    mdt_parsed_result = JsonUtils._decode_unicode_in_dict(mdt_parsed_result)
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«mdt_simple_reportå­—æ®µ
                    if isinstance(mdt_parsed_result, dict) and "mdt_simple_report" in mdt_parsed_result:
                        mdt_report_result = mdt_parsed_result["mdt_simple_report"]
                        logger.info(f"æˆåŠŸæå–MDTæŠ¥å‘Šï¼ŒåŒ…å«{len(mdt_report_result)}ä¸ªæ¡ç›®")
                    else:
                        # å¦‚æœæ²¡æœ‰mdt_simple_reportå­—æ®µï¼Œä½¿ç”¨æ•´ä¸ªè§£æç»“æœ
                        logger.warning("MDTæŠ¥å‘ŠJSONä¸­æœªæ‰¾åˆ°mdt_simple_reportå­—æ®µï¼Œä½¿ç”¨æ•´ä¸ªè§£æç»“æœ")
                        mdt_report_result = mdt_parsed_result
                else:
                    logger.warning("MDTæŠ¥å‘Šè§£æç»“æœä¸ºç©º")
            except Exception as e:
                logger.error(f"Error in MDT report generation: {e}")

            # è®°å½•MDTæŠ¥å‘Šç”Ÿæˆè€—æ—¶
            mdt_report_duration = time.time() - mdt_report_start_time
            logger.info("-" * 80)
            logger.info(f"ã€é˜¶æ®µ5ã€‘MDTæŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {mdt_report_duration:.2f} ç§’ ({mdt_report_duration/60:.2f} åˆ†é’Ÿ)")
            logger.info("-" * 80)

            # å‘é€MDTæŠ¥å‘Šç”Ÿæˆå®Œæˆè¿›åº¦
            yield {"type": "progress", "stage": "mdt_report_completed", "message": "MDTæŠ¥å‘Šç”Ÿæˆå®Œæˆ", "progress": 90}

            if parsed_result:
                timeline_count = len(parsed_result.get('timeline', []))
                logger.info(f"Successfully processed patient data with {timeline_count} timeline entries")
            else:
                logger.warning("Failed to parse patient data processing result")
            
            # å‡†å¤‡è¿”å›çš„ç»“æœ
            result_data = {
                "patient_content": preprocessed_info,
                "full_structure_data": parsed_result if parsed_result else {"error": "Failed to parse patient data", "raw": patient_data_result},
                "patient_journey": special_parsed_result if special_parsed_result else {},
                "mdt_simple_report": mdt_report_result if mdt_report_result else {}
            }

            # ğŸš¨ ç®€åŒ–ï¼šç§»é™¤å¤æ‚çš„éªŒè¯é€»è¾‘ï¼Œç›´æ¥ä¿å­˜å¤„ç†ç»“æœ

            # ========== ç”Ÿæˆæ‚£è€…æ—¶é—´æ—…ç¨‹å›¾ç‰‡å¹¶ä¸Šä¼ åˆ°ä¸ƒç‰›äº‘ ==========
            # ğŸš¨ ä¸´æ—¶ç¦ç”¨ï¼šå› ä¸ºPlaywrightåŠ è½½è¶…æ—¶é—®é¢˜
            if False and special_parsed_result and agent_session_id:
                try:
                    logger.info("å¼€å§‹ç”Ÿæˆæ‚£è€…æ—¶é—´æ—…ç¨‹å›¾ç‰‡...")

                    # ä»patient_journeyä¸­æå–æ•°æ®
                    journey_list = special_parsed_result if isinstance(special_parsed_result, list) else []

                    if journey_list:
                        # ç”Ÿæˆå›¾ç‰‡æ–‡ä»¶åå’Œè·¯å¾„
                        image_uuid = str(uuid_lib.uuid4())
                        output_dir = Path("output/files_extract") / agent_session_id / "patient_journey_images"
                        output_dir.mkdir(parents=True, exist_ok=True)

                        image_filename = f"patient_journey_{image_uuid}.png"
                        image_path = output_dir / image_filename

                        # æå–æ‚£è€…å§“åï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                        patient_name = "æ‚£è€…"
                        if parsed_result and isinstance(parsed_result, dict):
                            patient_name = parsed_result.get("patient_name", "æ‚£è€…")

                        # ç”Ÿæˆå›¾ç‰‡
                        success = generate_patient_journey_image_sync(
                            patient_journey_data=journey_list,
                            output_path=str(image_path),
                            patient_name=patient_name
                        )

                        if success and image_path.exists():
                            logger.info(f"æ‚£è€…æ—¶é—´æ—…ç¨‹å›¾ç‰‡ç”ŸæˆæˆåŠŸ: {image_path}")

                            # ä¸Šä¼ åˆ°ä¸ƒç‰›äº‘
                            try:
                                qiniu_service = QiniuUploadService()
                                qiniu_key = f"patient_journey/{image_uuid}.png"

                                upload_success, cloud_url, error = qiniu_service.upload_file(
                                    str(image_path),
                                    qiniu_key
                                )

                                if upload_success:
                                    logger.info(f"æ‚£è€…æ—¶é—´æ—…ç¨‹å›¾ç‰‡å·²ä¸Šä¼ åˆ°ä¸ƒç‰›äº‘: {cloud_url}")

                                    # å°†å›¾ç‰‡URLæ·»åŠ åˆ°patient_journey JSONä¸­
                                    if isinstance(result_data["patient_journey"], dict):
                                        result_data["patient_journey"]["image_url"] = cloud_url
                                    elif isinstance(result_data["patient_journey"], list):
                                        # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå­—å…¸ç»“æ„
                                        result_data["patient_journey"] = {
                                            "timeline_journey": result_data["patient_journey"],
                                            "image_url": cloud_url
                                        }
                                else:
                                    logger.error(f"ä¸Šä¼ æ‚£è€…æ—¶é—´æ—…ç¨‹å›¾ç‰‡åˆ°ä¸ƒç‰›äº‘å¤±è´¥: {error}")
                            except Exception as upload_error:
                                logger.error(f"ä¸Šä¼ æ‚£è€…æ—¶é—´æ—…ç¨‹å›¾ç‰‡åˆ°ä¸ƒç‰›äº‘æ—¶å‡ºé”™: {upload_error}")
                        else:
                            logger.warning("æ‚£è€…æ—¶é—´æ—…ç¨‹å›¾ç‰‡ç”Ÿæˆå¤±è´¥")
                    else:
                        logger.info("æ‚£è€…æ—…ç¨‹æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡å›¾ç‰‡ç”Ÿæˆ")

                except Exception as e:
                    logger.error(f"ç”Ÿæˆæˆ–ä¸Šä¼ æ‚£è€…æ—¶é—´æ—…ç¨‹å›¾ç‰‡æ—¶å‡ºé”™: {e}")
                    import traceback
                    logger.error(traceback.format_exc())

            # ========== ç”Ÿæˆæ ¸å¿ƒæŒ‡æ ‡è¶‹åŠ¿å›¾ç‰‡å¹¶ä¸Šä¼ åˆ°ä¸ƒç‰›äº‘ ==========
            # ğŸš¨ ä¸´æ—¶ç¦ç”¨ï¼šå› ä¸ºPlaywrightåŠ è½½è¶…æ—¶é—®é¢˜
            indicator_chart_image_url = None
            if False and special_parsed_result and agent_session_id:
                try:
                    # ä»patient_journeyä¸­æå–indicator_seriesæ•°æ®
                    indicator_series = None
                    if isinstance(special_parsed_result, dict) and 'indicator_series' in special_parsed_result:
                        indicator_series = special_parsed_result.get('indicator_series')

                    if indicator_series and isinstance(indicator_series, list) and indicator_series:
                        logger.info(f"å¼€å§‹ç”Ÿæˆæ ¸å¿ƒæŒ‡æ ‡è¶‹åŠ¿å›¾ç‰‡ï¼ŒåŒ…å« {len(indicator_series)} ä¸ªæŒ‡æ ‡...")

                        # ç”Ÿæˆå›¾ç‰‡æ–‡ä»¶åå’Œè·¯å¾„
                        image_uuid = str(uuid_lib.uuid4())
                        output_dir = Path("output/files_extract") / agent_session_id / "indicator_chart_images"
                        output_dir.mkdir(parents=True, exist_ok=True)

                        image_filename = f"indicator_chart_{image_uuid}.png"
                        image_path = output_dir / image_filename

                        # æå–æ‚£è€…å§“å
                        patient_name = "æ‚£è€…"
                        if parsed_result and isinstance(parsed_result, dict):
                            patient_name = parsed_result.get("patient_name", "æ‚£è€…")

                        # ç”Ÿæˆå›¾ç‰‡
                        success = generate_indicator_chart_image_sync(
                            indicator_series_data=indicator_series,
                            output_path=str(image_path),
                            patient_name=patient_name
                        )

                        if success and image_path.exists():
                            logger.info(f"æ ¸å¿ƒæŒ‡æ ‡è¶‹åŠ¿å›¾ç‰‡ç”ŸæˆæˆåŠŸ: {image_path}")

                            # ä¸Šä¼ åˆ°ä¸ƒç‰›äº‘
                            try:
                                qiniu_service = QiniuUploadService()
                                qiniu_key = f"indicator_chart/{image_uuid}.png"

                                upload_success, cloud_url, error = qiniu_service.upload_file(
                                    str(image_path),
                                    qiniu_key
                                )

                                if upload_success:
                                    indicator_chart_image_url = cloud_url
                                    # å°†URLæ·»åŠ åˆ°patient_journey JSONä¸­
                                    if isinstance(special_parsed_result, dict):
                                        special_parsed_result["indicator_chart_image_url"] = cloud_url
                                        result_data["patient_journey"] = special_parsed_result
                                    logger.info(f"æ ¸å¿ƒæŒ‡æ ‡è¶‹åŠ¿å›¾ç‰‡å·²ä¸Šä¼ åˆ°ä¸ƒç‰›äº‘: {cloud_url}")
                                else:
                                    logger.error(f"ä¸Šä¼ æ ¸å¿ƒæŒ‡æ ‡è¶‹åŠ¿å›¾ç‰‡åˆ°ä¸ƒç‰›äº‘å¤±è´¥: {error}")
                            except Exception as upload_error:
                                logger.error(f"ä¸Šä¼ æ ¸å¿ƒæŒ‡æ ‡è¶‹åŠ¿å›¾ç‰‡åˆ°ä¸ƒç‰›äº‘æ—¶å‡ºé”™: {upload_error}")
                        else:
                            logger.warning("æ ¸å¿ƒæŒ‡æ ‡è¶‹åŠ¿å›¾ç‰‡ç”Ÿæˆå¤±è´¥")
                    else:
                        logger.info("æŒ‡æ ‡åºåˆ—æ•°æ®ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡å›¾ç‰‡ç”Ÿæˆ")

                except Exception as e:
                    logger.error(f"ç”Ÿæˆæˆ–ä¸Šä¼ æ ¸å¿ƒæŒ‡æ ‡è¶‹åŠ¿å›¾ç‰‡æ—¶å‡ºé”™: {e}")
                    import traceback
                    logger.error(traceback.format_exc())

            # ä¿å­˜æ‚£è€…æ•°æ®åˆ°è¾“å‡ºç›®å½•ï¼ˆä¸intent_determine_crewç›¸åŒçš„sessionç›®å½•ï¼‰
            if agent_session_id:
                output_file_path = self._save_patient_data_to_output(
                    agent_session_id,
                    preprocessed_info,
                    result_data["full_structure_data"],
                    result_data.get("patient_journey"),
                    result_data.get("mdt_simple_report")
                )
                if output_file_path:
                    logger.info(f"æ‚£è€…æ•°æ®å·²ä¿å­˜åˆ°è¾“å‡ºç›®å½•: {output_file_path}")
                else:
                    logger.warning("ä¿å­˜æ‚£è€…æ•°æ®åˆ°è¾“å‡ºç›®å½•å¤±è´¥")
            else:
                logger.warning("No agent_session_id provided, skipping patient data save")

            # ========== æ€»ä½“è€—æ—¶ç»Ÿè®¡ ==========
            overall_duration = time.time() - overall_start_time
            logger.info("=" * 80)
            logger.info("æ‚£è€…æ•°æ®å¤„ç†æµç¨‹å®Œæˆ - è€—æ—¶ç»Ÿè®¡")
            logger.info("=" * 80)
            logger.info(f"ã€é˜¶æ®µ1ã€‘æ–‡ä»¶é¢„å¤„ç†:        {file_preprocessing_duration:.2f} ç§’ ({file_preprocessing_duration/60:.2f} åˆ†é’Ÿ) - {(file_preprocessing_duration/overall_duration*100):.1f}%")
            logger.info(f"ã€é˜¶æ®µ2ã€‘ç–¾ç—…é…ç½®è¯†åˆ«:      {disease_config_duration:.2f} ç§’ ({disease_config_duration/60:.2f} åˆ†é’Ÿ) - {(disease_config_duration/overall_duration*100):.1f}%")
            logger.info(f"ã€é˜¶æ®µ3ã€‘æ‚£è€…æ•°æ®å¤„ç†:      {patient_data_processing_duration:.2f} ç§’ ({patient_data_processing_duration/60:.2f} åˆ†é’Ÿ) - {(patient_data_processing_duration/overall_duration*100):.1f}%")
            logger.info(f"ã€é˜¶æ®µ4ã€‘æ‚£è€…æ—…ç¨‹æå–:      {patient_journey_duration:.2f} ç§’ ({patient_journey_duration/60:.2f} åˆ†é’Ÿ) - {(patient_journey_duration/overall_duration*100):.1f}%")
            logger.info(f"ã€é˜¶æ®µ5ã€‘MDTæŠ¥å‘Šç”Ÿæˆ:       {mdt_report_duration:.2f} ç§’ ({mdt_report_duration/60:.2f} åˆ†é’Ÿ) - {(mdt_report_duration/overall_duration*100):.1f}%")
            logger.info("-" * 80)
            logger.info(f"ã€æ€»è®¡ã€‘æ•´ä½“å¤„ç†æ—¶é—´:       {overall_duration:.2f} ç§’ ({overall_duration/60:.2f} åˆ†é’Ÿ)")
            logger.info("=" * 80)

            # å‘é€æœ€ç»ˆå¤„ç†å®Œæˆè¿›åº¦
            yield {"type": "progress", "stage": "finalizing", "message": "å¤„ç†å®Œæˆï¼Œæ­£åœ¨æ•´ç†ç»“æœ", "progress": 95}

            # yield æœ€ç»ˆç»“æœ
            yield {"type": "result", "data": result_data}

        except Exception as e:
            logger.error(f"Error in patient data processing: {e}")
            yield {"type": "result", "data": {"error": str(e)}} 

