# src/custom_tools/extract_content_from_path.py

from typing import Any, Optional, Type
from pydantic import BaseModel, Field
from crewai.tools import BaseTool
import os
import re
import zipfile
import requests
import docx
import html2text
from goose3 import Goose
from goose3.text import StopWordsChinese
from bs4 import BeautifulSoup
import trafilatura
import chardet
import random
# import pypdf
import pypdf
import pdfplumber
import fitz  # PyMuPDF
import io
import logging
from dotenv import load_dotenv
import json
import base64  # æ–°å¢å¯¼å…¥
from openai import OpenAI  # æ–°å¢å¯¼å…¥
import shutil  # æ–°å¢å¯¼å…¥ï¼Œç”¨äºæ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤¹
import time  # æ–°å¢å¯¼å…¥ï¼Œç”¨äºæ—¶é—´ç»Ÿè®¡
import concurrent.futures  # ğŸš¨ æ–°å¢ï¼šç”¨äºå¹¶å‘å¤„ç†
from functools import partial  # ğŸš¨ æ–°å¢ï¼šç”¨äºåˆ›å»ºåå‡½æ•°
import uuid  # æ–°å¢å¯¼å…¥ï¼Œç”¨äºç”ŸæˆUUID
from pathlib import Path  # æ–°å¢å¯¼å…¥ï¼Œç”¨äºå¤„ç†æ–‡ä»¶è·¯å¾„



load_dotenv()

# Set up logger
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)

# å°è¯•å¯¼å…¥HEICè½¬æ¢ç›¸å…³åº“
try:
    from PIL import Image
    from pillow_heif import register_heif_opener
    register_heif_opener()
    HEIC_CONVERSION_AVAILABLE = True
    logger.info("HEICè½¬æ¢åŠŸèƒ½å¯ç”¨")
except ImportError:
    HEIC_CONVERSION_AVAILABLE = False
    logger.warning("HEICè½¬æ¢åŠŸèƒ½ä¸å¯ç”¨ï¼Œéœ€è¦å®‰è£… pillow-heif åº“")
    
try:
    from pptx import Presentation  # æ–°å¢å¯¼å…¥ï¼Œç”¨äºPPTæ–‡ä»¶å¤„ç†
    PPTX_AVAILABLE = True
except ImportError:
    PPTX_AVAILABLE = False
    logger.warning("python-pptx not available. PPT file processing will be skipped.")
    
# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ç”¨äºå›¾ç‰‡å¤„ç†
try:
    # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
    api_keys_str = os.getenv("MULTIMODAL_API_KEY")
    base_url = os.getenv("MULTIMODAL_BASE_URL")
    multimodal_model_name_str = os.getenv("MULTIMODAL_MODEL_NAME")

    # è§£æå¤šä¸ª API keysï¼ˆé€—å·åˆ†éš”ï¼‰
    if api_keys_str:
        multimodal_api_keys = [key.strip() for key in api_keys_str.split(',') if key.strip()]
    else:
        multimodal_api_keys = []

    # è§£æå¤šä¸ªæ¨¡å‹åç§°ï¼ˆé€—å·åˆ†éš”ï¼‰
    if multimodal_model_name_str:
        multimodal_model_names = [name.strip() for name in multimodal_model_name_str.split(',') if name.strip()]
    else:
        multimodal_model_names = []

    if not multimodal_api_keys or not base_url:
        logger.warning("MULTIMODAL_API_KEY or MULTIMODAL_BASE_URL not found in environment variables")
        openai_client = None
    else:
        # ä½¿ç”¨ç¬¬ä¸€ä¸ª API key åˆå§‹åŒ–é»˜è®¤å®¢æˆ·ç«¯ï¼ˆå®é™…ä½¿ç”¨æ—¶ä¼šåŠ¨æ€é€‰æ‹©ï¼‰
        openai_client = OpenAI(
            api_key=multimodal_api_keys[0],
            base_url=base_url
        )
        logger.info(f"OpenAI client initialized with {len(multimodal_api_keys)} API key(s) and {len(multimodal_model_names)} model(s): {multimodal_model_names}")
except Exception as e:
    logger.warning(f"Failed to initialize OpenAI client for image processing: {e}")
    openai_client = None
    multimodal_api_keys = []
    multimodal_model_names = []

class ExtractContentFromPathsSchema(BaseModel):
    """Input for ExtractContentFromPathsTool."""
    paths: list[str] = Field(..., description="List of absolute paths or URLs to extract content from")

class ExtractContentFromPathsTool(BaseTool):
    name: str = "Extract content from multiple files path or URLs"
    description: str = (
        "A tool that can extract content from multiple files (PDF, DOCX, TXT, images) or URLs at once."
    )
    args_schema: Type[BaseModel] = ExtractContentFromPathsSchema

    def _run(
        self,
        paths: list[str],
    ) -> Any:
        single_extractor = ExtractContentFromPathTool()
        results = []
        
        for path in paths:
            try:
                result = single_extractor._run(path=path)
                if result:
                    if isinstance(result, list):
                        results.extend(result)
                    else:
                        results.append(result)
            except Exception as e:
                logger.info(f"Error processing path {path}: {e}")
                continue
                
        return json.dumps(results)
    
class ExtractContentFromPathSchema(BaseModel):
    """Input for ExtractContentFromPathTool."""
    path: str = Field(..., description="Absolute Path to the file or URL to extract content from")

class ExtractContentFromPathTool(BaseTool):
    name: str = "Extract content from file or URL"
    description: str = (
        "A tool that can extract content from various file types (PDF, DOCX, TXT, images) or URLs."
    )
    args_schema: Type[BaseModel] = ExtractContentFromPathSchema

    def _run(
        self,
        path: str,
    ) -> Any:
        # è¿‡æ»¤ç³»ç»Ÿéšè—æ–‡ä»¶
        if os.path.isfile(path):
            filename = os.path.basename(path)
            if filename.startswith('._') or filename.startswith('.DS_Store'):
                logger.warning(f"è·³è¿‡ç³»ç»Ÿéšè—æ–‡ä»¶: {filename}")
                return {
                    'file_extension': 'hidden',
                    'file_name': filename,
                    'file_content': f"ç³»ç»Ÿéšè—æ–‡ä»¶: {filename} (å·²è·³è¿‡å¤„ç†)",
                    'extraction_success': False,
                    'extraction_error': 'ç³»ç»Ÿéšè—æ–‡ä»¶ï¼Œå·²è·³è¿‡å¤„ç†'
                    # æ³¨æ„ï¼šä¸ç”ŸæˆUUIDï¼Œç”±ä¸Šå±‚ç»Ÿä¸€ç®¡ç†
                }

        try:
            if path.startswith(('http://', 'https://')):
                result = self.read_url(path)
            elif os.path.isfile(path):
                file_extension = os.path.splitext(path)[1].lower()
                if file_extension == '.pdf':
                    # æ ¹æ®ç¯å¢ƒå˜é‡é€‰æ‹©PDFå¤„ç†æ–¹å¼
                    pdf_extraction_mode = os.getenv("PDF_EXTRACTION_MODE", "default").lower()

                    if pdf_extraction_mode == "with_images":
                        logger.info(f"ä½¿ç”¨å›¾ç‰‡æå–æ¨¡å¼å¤„ç†PDF: {os.path.basename(path)}")
                        result = self.read_pdf_with_images(path)
                    else:
                        logger.info(f"ä½¿ç”¨é»˜è®¤æ¨¡å¼å¤„ç†PDF: {os.path.basename(path)}")
                        result = self.read_pdf(path)
                elif file_extension == '.docx':
                    result = self.read_docx(path)
                elif file_extension in ['.pptx', '.ppt']:  # æ–°å¢PPTæ–‡ä»¶æ”¯æŒ
                    result = self.read_ppt(path)
                elif file_extension == '.txt':
                    result = self.read_txt(path)
                elif file_extension == '.md':
                    result = self.read_md(path)
                elif file_extension == '.json':
                    result = self.read_json(path)
                elif file_extension == '.zip':
                    # è§£å‹zipæ–‡ä»¶å¹¶é€’å½’å¤„ç†æ‰€æœ‰æ–‡ä»¶
                    result = self.process_zip_file(path)
                elif file_extension in ['.png', '.jpg', '.jpeg', '.webp', '.heic', '.heif']:
                    result = self.read_image(path)
                else:
                    result = self.read_file(path)
            else:
                raise ValueError(f"Invalid path or unsupported file type: {path}")

            # ğŸš¨ æ ‡è®°æå–çŠ¶æ€ï¼ˆä¸ç”ŸæˆUUIDï¼ŒUUIDç”±ä¸Šå±‚file_processing_managerç»Ÿä¸€ç®¡ç†ï¼‰
            if isinstance(result, dict):
                # æ ‡è®°æå–æˆåŠŸï¼ˆå¦‚æœresultæ˜¯å­—å…¸ä¸”æ²¡æœ‰errorå­—æ®µï¼‰
                if 'extraction_success' not in result:
                    # åˆ¤æ–­æ˜¯å¦æå–æˆåŠŸï¼šæœ‰file_contentä¸”å†…å®¹ä¸ä¸ºç©º
                    has_content = result.get('file_content') and len(str(result.get('file_content', '')).strip()) > 0
                    result['extraction_success'] = has_content
                    if not has_content:
                        result['extraction_error'] = 'æå–å†…å®¹ä¸ºç©º'

                # æ³¨æ„ï¼šä¸åœ¨è¿™é‡Œç”Ÿæˆfile_uuidï¼Œç”±ä¸Šå±‚ç»Ÿä¸€ç®¡ç†
            elif isinstance(result, list):
                # å¯¹äºè¿”å›åˆ—è¡¨çš„æƒ…å†µï¼ˆå¦‚zipæ–‡ä»¶ã€PDF with imagesï¼‰ï¼Œæ ‡è®°æå–çŠ¶æ€
                for item in result:
                    if isinstance(item, dict):
                        # æ ‡è®°æå–æˆåŠŸ
                        if 'extraction_success' not in item:
                            has_content = item.get('file_content') and len(str(item.get('file_content', '')).strip()) > 0
                            item['extraction_success'] = has_content
                            if not has_content:
                                item['extraction_error'] = 'æå–å†…å®¹ä¸ºç©º'

                        # æ³¨æ„ï¼šè¿™é‡Œä¿ç•™UUIDç”Ÿæˆï¼Œå› ä¸ºzip/PDFä¸­çš„å­æ–‡ä»¶éœ€è¦æ–°çš„UUID
                        # ä½†æ˜¯ä¸»æ–‡ä»¶çš„UUIDåº”è¯¥ä¿ç•™åŸå§‹å€¼
                        if 'file_uuid' not in item:
                            item['file_uuid'] = str(uuid.uuid4())

            return result
        except Exception as e:
            filename = os.path.basename(path) if os.path.isfile(path) else path
            logger.error(f"æ–‡ä»¶æå–å¤±è´¥: {filename}, é”™è¯¯: {str(e)}")
            return {
                'file_extension': os.path.splitext(filename)[1].lower()[1:] if os.path.isfile(path) else 'unknown',
                'file_name': filename,
                'file_content': f"æ–‡ä»¶æå–å¤±è´¥: {str(e)}",
                'extraction_success': False,
                'extraction_error': f"{type(e).__name__}: {str(e)}"
                # æ³¨æ„ï¼šä¸ç”ŸæˆUUIDï¼Œç”±ä¸Šå±‚ç»Ÿä¸€ç®¡ç†
            }

    def detect_image_format(self, path):
        """æ£€æµ‹å›¾ç‰‡çš„å®é™…æ ¼å¼ï¼Œä¸ä¾èµ–æ–‡ä»¶æ‰©å±•å"""
        try:
            # è¯»å–æ–‡ä»¶å¤´æ¥æ£€æµ‹æ ¼å¼
            with open(path, 'rb') as f:
                header = f.read(32)  # è¯»å–æ›´å¤šå­—èŠ‚ä»¥æ”¯æŒæ›´å¤šæ ¼å¼
                
            # æ£€æŸ¥å¸¸è§çš„å›¾ç‰‡æ ¼å¼é­”æ•°
            if header.startswith(b'\x89PNG\r\n\x1a\n'):
                return 'png'
            elif header.startswith(b'\xff\xd8\xff'):
                return 'jpeg'
            elif header.startswith(b'RIFF') and b'WEBP' in header:
                return 'webp'
            elif header.startswith(b'GIF87a') or header.startswith(b'GIF89a'):
                return 'gif'
            elif header.startswith(b'BM'):
                return 'bmp'
            elif b'ftypheic' in header or b'ftypheix' in header:
                return 'heic'
            elif b'ftypmif1' in header or b'ftypmsf1' in header:
                return 'heif'
            elif header.startswith(b'\x00\x00\x01\x00') or header.startswith(b'\x00\x00\x02\x00'):
                return 'ico'
                
        except Exception as e:
            logger.debug(f"æ ¼å¼æ£€æµ‹å¤±è´¥: {e}")
            
        return None

    def convert_heic_to_jpg(self, heic_path, output_path=None):
        """å°†HEICæ ¼å¼è½¬æ¢ä¸ºJPGæ ¼å¼"""
        if not HEIC_CONVERSION_AVAILABLE:
            logger.error("HEICè½¬æ¢åŠŸèƒ½ä¸å¯ç”¨ï¼Œéœ€è¦å®‰è£… pillow-heif åº“")
            return None
            
        try:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºè·¯å¾„ï¼Œåˆ™åœ¨åŒç›®å½•ä¸‹ç”Ÿæˆ
            if output_path is None:
                base_name = os.path.splitext(heic_path)[0]
                output_path = f"{base_name}_converted.jpg"
            
            # ä½¿ç”¨PILæ‰“å¼€HEICæ–‡ä»¶å¹¶è½¬æ¢ä¸ºJPG
            with Image.open(heic_path) as img:
                # å¦‚æœå›¾ç‰‡æœ‰é€æ˜é€šé“ï¼Œè½¬æ¢ä¸ºRGBæ¨¡å¼
                if img.mode in ('RGBA', 'LA', 'P'):
                    # åˆ›å»ºç™½è‰²èƒŒæ™¯
                    background = Image.new('RGB', img.size, (255, 255, 255))
                    if img.mode == 'P':
                        img = img.convert('RGBA')
                    background.paste(img, mask=img.split()[-1] if img.mode == 'RGBA' else None)
                    img = background
                elif img.mode != 'RGB':
                    img = img.convert('RGB')
                
                # ä¿å­˜ä¸ºJPGæ ¼å¼ï¼Œè´¨é‡è®¾ç½®ä¸º90
                img.save(output_path, 'JPEG', quality=90, optimize=True)
                
            logger.info(f"HEICè½¬æ¢æˆåŠŸ: {os.path.basename(heic_path)} -> {os.path.basename(output_path)}")
            return output_path
            
        except Exception as e:
            logger.error(f"HEICè½¬æ¢å¤±è´¥: {str(e)}")
            return None

    def read_image(self, path):
        """ä½¿ç”¨å¤šæ¨¡æ€æ¨¡å‹æå–å›¾ç‰‡å†…å®¹æè¿°"""
        try:
            filename = os.path.basename(path)
            file_extension = os.path.splitext(path)[1].lower()
            result = {
                'file_extension': file_extension[1:],
                'file_name': filename,
                'extraction_success': False,  # é»˜è®¤å¤±è´¥ï¼ŒæˆåŠŸåä¼šæ›´æ–°
                'extraction_error': None
            }
            
            # è¿‡æ»¤éšè—æ–‡ä»¶å’Œç³»ç»Ÿæ–‡ä»¶
            if filename.startswith('._') or filename.startswith('.DS_Store'):
                logger.warning(f"è·³è¿‡ç³»ç»Ÿéšè—æ–‡ä»¶: {filename}")
                result['file_content'] = f"ç³»ç»Ÿéšè—æ–‡ä»¶: {filename} (å·²è·³è¿‡å¤„ç†)"
                result['extraction_error'] = 'ç³»ç»Ÿéšè—æ–‡ä»¶ï¼Œå·²è·³è¿‡å¤„ç†'
                return result
            
            # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§å’Œå¤§å°
            if not os.path.exists(path):
                result['file_content'] = f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {filename}"
                return result
                
            file_size = os.path.getsize(path)
            if file_size == 0:
                logger.warning(f"æ–‡ä»¶å¤§å°ä¸º0: {filename}")
                result['file_content'] = f"å›¾ç‰‡æ–‡ä»¶: {filename} (æ–‡ä»¶å¤§å°ä¸º0)"
                return result
            elif file_size > 20 * 1024 * 1024:  # å¤§äº20MB
                logger.warning(f"æ–‡ä»¶è¿‡å¤§: {filename} ({file_size / 1024 / 1024:.1f}MB)")
                result['file_content'] = f"å›¾ç‰‡æ–‡ä»¶: {filename} (æ–‡ä»¶è¿‡å¤§: {file_size / 1024 / 1024:.1f}MB)"
                return result
            
            # æ£€æµ‹å®é™…å›¾ç‰‡æ ¼å¼
            actual_format = self.detect_image_format(path)
            original_path = path  # ä¿å­˜åŸå§‹è·¯å¾„
            converted_file = None  # è½¬æ¢åçš„æ–‡ä»¶è·¯å¾„
            
            if actual_format:
                # å¦‚æœå®é™…æ ¼å¼ä¸æ‰©å±•åä¸ç¬¦ï¼Œè®°å½•è­¦å‘Š
                expected_format = file_extension[1:] if file_extension else 'unknown'
                if actual_format != expected_format and expected_format != 'jpg':  # jpgå’Œjpegè§†ä¸ºç›¸åŒ
                    if not (actual_format == 'jpeg' and expected_format in ['jpg', 'jpeg']):
                        logger.warning(f"æ ¼å¼ä¸åŒ¹é…: {filename} æ‰©å±•å={expected_format}, å®é™…={actual_format}")
                
                # å¦‚æœæ˜¯HEICæ ¼å¼ï¼Œå…ˆè½¬æ¢ä¸ºJPG
                if actual_format in ['heic', 'heif']:
                    logger.info(f"æ£€æµ‹åˆ°HEIC/HEIFæ ¼å¼ï¼Œå¼€å§‹è½¬æ¢: {filename}")
                    import tempfile
                    # åœ¨ä¸´æ—¶ç›®å½•åˆ›å»ºè½¬æ¢åçš„æ–‡ä»¶
                    temp_dir = tempfile.mkdtemp()
                    converted_filename = f"{os.path.splitext(filename)[0]}_converted.jpg"
                    converted_path = os.path.join(temp_dir, converted_filename)
                    
                    converted_file = self.convert_heic_to_jpg(path, converted_path)
                    if converted_file:
                        path = converted_file  # ä½¿ç”¨è½¬æ¢åçš„æ–‡ä»¶è·¯å¾„
                        actual_format = 'jpeg'  # æ›´æ–°æ ¼å¼
                        logger.info(f"HEICè½¬æ¢æˆåŠŸï¼Œä½¿ç”¨è½¬æ¢åçš„æ–‡ä»¶è¿›è¡Œå¤„ç†")
                    else:
                        logger.error(f"HEICè½¬æ¢å¤±è´¥ï¼Œå°è¯•ç›´æ¥å¤„ç†åŸæ–‡ä»¶")
                
                # ä½¿ç”¨å®é™…æ ¼å¼ç¡®å®šMIMEç±»å‹
                format_to_mime = {
                    'png': 'image/png',
                    'jpeg': 'image/jpeg',
                    'jpg': 'image/jpeg',
                    'webp': 'image/webp',
                    'heic': 'image/jpeg',  # HEICä½¿ç”¨JPEG MIMEç±»å‹
                    'heif': 'image/jpeg'   # HEIFä½¿ç”¨JPEG MIMEç±»å‹
                }
                mime_type = format_to_mime.get(actual_format, 'image/png')
            else:
                # å›é€€åˆ°åŸºäºæ‰©å±•åçš„MIMEç±»å‹
                mime_type_map = {
                    '.png': 'image/png',
                    '.jpg': 'image/jpeg',
                    '.jpeg': 'image/jpeg',
                    '.webp': 'image/webp',
                    '.heic': 'image/jpeg',
                    '.heif': 'image/jpeg'
                }
                mime_type = mime_type_map.get(file_extension, 'image/png')
            
            # æ£€æŸ¥å¤šæ¨¡æ€å®¢æˆ·ç«¯
            if not openai_client:
                logger.warning(f"å¤šæ¨¡æ€å®¢æˆ·ç«¯ä¸å¯ç”¨: {filename}")
                result['file_content'] = f"å›¾ç‰‡æ–‡ä»¶: {filename} (æ— æ³•å¤„ç†å›¾ç‰‡å†…å®¹ï¼Œå¤šæ¨¡æ€æ¨¡å‹ä¸å¯ç”¨)"
                return result
            
            logger.info(f"å¼€å§‹å¤„ç†å›¾ç‰‡: {filename} (æ ¼å¼: {actual_format or 'unknown'})")
            
            # è¯»å–å›¾ç‰‡å¹¶è½¬æ¢ä¸ºbase64
            with open(path, 'rb') as image_file:
                image_data = image_file.read()
                image_base64 = base64.b64encode(image_data).decode('utf-8')
            
            # è°ƒç”¨å¤šæ¨¡æ€æ¨¡å‹ï¼ˆæ”¯æŒå¤šAPI keyå’Œå¤šæ¨¡å‹å®¹é”™ï¼‰
            try:
                response_content = None
                last_error = None
                successful_model = None
                successful_api_key_index = None

                # éšæœºæ‰“ä¹± API keys é¡ºåº
                shuffled_api_keys = multimodal_api_keys.copy()
                random.shuffle(shuffled_api_keys)

                for api_key_index, api_key in enumerate(shuffled_api_keys):
                    # ä¸ºæ¯ä¸ª API key åˆ›å»ºæ–°çš„å®¢æˆ·ç«¯
                    try:
                        current_client = OpenAI(
                            api_key=api_key,
                            base_url=base_url
                        )
                    except Exception as client_error:
                        logger.warning(f"åˆ›å»ºå®¢æˆ·ç«¯å¤±è´¥ (API key {api_key_index + 1}/{len(shuffled_api_keys)}): {str(client_error)[:100]}")
                        continue

                    # å°è¯•è¯¥ API key çš„æ‰€æœ‰æ¨¡å‹
                    for model_index, model_name in enumerate(multimodal_model_names):
                        try:
                            logger.info(f"å°è¯• API key {api_key_index + 1}/{len(shuffled_api_keys)}, æ¨¡å‹ {model_index + 1}/{len(multimodal_model_names)}: {model_name}")

                            # å°è¯•ä½¿ç”¨JSONæ¨¡å¼ï¼ˆéƒ¨åˆ†æ¨¡å‹ä¸æ”¯æŒï¼Œéœ€è¦å®¹é”™å¤„ç†ï¼‰
                            request_params = {
                                "model": model_name,
                                "messages": [
                                    {
                                        "role": "user",
                                        "content": [
                                            {"type": "text", "text": """è¯·åˆ†æè¿™å¼ å›¾ç‰‡å¹¶è¿”å›JSONæ ¼å¼çš„ç»“æœ:
{
  "has_medical_image": true/false,
  "content": "å›¾ç‰‡å†…å®¹æè¿°",
  "image_bbox": {
    "x": 0.0-1.0,
    "y": 0.0-1.0,
    "width": 0.0-1.0,
    "height": 0.0-1.0
  }
}

ã€åˆ¤æ–­è§„åˆ™ã€‘:
1. has_medical_image: åˆ¤æ–­å›¾ç‰‡æ˜¯å¦åŒ…å«åŒ»å­¦å½±åƒ(å¦‚CTã€MRIã€Xå…‰ç‰‡ã€è¶…å£°ã€ç—…ç†åˆ‡ç‰‡ã€å†…é•œå›¾åƒç­‰)
   - å¦‚æœå›¾ç‰‡ä¸­åŒ…å«å®é™…çš„åŒ»å­¦å½±åƒå›¾ç‰‡(ä¸æ˜¯çº¯æ–‡å­—æŠ¥å‘Šæˆªå›¾),è¿”å›true
   - å¦‚æœæ˜¯çº¯æ–‡å­—æŠ¥å‘Šå•ã€æ£€éªŒå•æˆªå›¾,è¿”å›false
   - å¦‚æœä¸æ˜¯åŒ»å­¦ç›¸å…³å›¾ç‰‡,è¿”å›false

2. content: å›¾ç‰‡å†…å®¹æè¿°
   - åªè¿”å›åŒ»å­¦èµ„æ–™çš„å†…å®¹,ä¸è¿”å›å…¶ä»–æ— å…³çš„å†…å®¹
   - å¦‚æœå’ŒåŒ»å­¦èµ„æ–™æ— å…³,åˆ™ä¸è¿”å›å†…å®¹
   - å¦‚æœå’ŒåŒ»å­¦èµ„æ–™æœ‰å…³,è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹,åŒ…æ‹¬å›¾ç‰‡ä¸­çš„æ–‡å­—ã€æ—¥æœŸã€æˆ–è€…å¯¹è±¡,æ¯”å¦‚æ£€æŸ¥æ£€éªŒå•,CTå½±åƒç­‰æ‰€æœ‰å¯è§ä¿¡æ¯
   - å¦‚æœå›¾ç‰‡åŒ…å«æ–‡æ¡£æˆ–è¡¨æ ¼,è¯·å°½å¯èƒ½å‡†ç¡®åœ°è½¬å½•å…¶ä¸­çš„æ–‡å­—å†…å®¹
   - å¯¹äºåŒ»å­¦è¡¨æ ¼æ£€éªŒæ£€æŸ¥æŠ¥å‘Šå•,ä¸éœ€è¦ä¸€äº›åŒ»ç”Ÿå§“åç›¸å…³çš„ä¿¡æ¯
   - ç»“æœè¿”å›è§£æåçš„åŒ»å­¦æŠ¥å‘Šçš„markdownæ ¼å¼å†…å®¹

3. image_bbox: åŒ»å­¦å½±åƒå›¾ç‰‡çš„è¾¹ç•Œæ¡†(ä»…å½“has_medical_image=trueæ—¶å¡«å†™)
   - ç”¨äºä»æ··åˆå›¾ç‰‡(æ–‡å­—+å›¾åƒ)ä¸­è£å‰ªå‡ºåŒ»å­¦å½±åƒéƒ¨åˆ†
   - bboxåæ ‡ä½¿ç”¨å½’ä¸€åŒ–å€¼(0.0-1.0),å…¶ä¸­:
     * x: å›¾åƒä¸­å¿ƒç‚¹çš„æ°´å¹³ä½ç½®(0=æœ€å·¦,1=æœ€å³)
     * y: å›¾åƒä¸­å¿ƒç‚¹çš„å‚ç›´ä½ç½®(0=æœ€ä¸Š,1=æœ€ä¸‹)
     * width: å›¾åƒå®½åº¦å æ•´å¼ å›¾ç‰‡å®½åº¦çš„æ¯”ä¾‹
     * height: å›¾åƒé«˜åº¦å æ•´å¼ å›¾ç‰‡é«˜åº¦çš„æ¯”ä¾‹
   - å¦‚æœæ•´å¼ å›¾ç‰‡å°±æ˜¯åŒ»å­¦å½±åƒ(æ²¡æœ‰æ–‡å­—æŠ¥å‘Šéƒ¨åˆ†),bboxå¯ä»¥æ˜¯ {"x": 0.5, "y": 0.5, "width": 1.0, "height": 1.0}
   - å¦‚æœå›¾ç‰‡æ˜¯æ–‡å­—æŠ¥å‘Šä¸­åµŒå…¥çš„åŒ»å­¦å½±åƒ,è¿”å›å½±åƒéƒ¨åˆ†çš„å‡†ç¡®bbox

ç›´æ¥è¿”å›JSON,ä¸è¦è¾“å‡ºå…¶ä»–å›å¤è¯­æ°”è¯"""},
                                        {
                                            "type": "image_url",
                                            "image_url": {
                                                "url": f"data:{mime_type};base64,{image_base64}"
                                            }
                                        }
                                    ]
                                }
                            ],
                            "max_tokens": 8190
                        }

                            # ä»…å¯¹æ”¯æŒJSONæ¨¡å¼çš„æ¨¡å‹æ·»åŠ response_formatå‚æ•°
                            # GLM-4.5Vç­‰éƒ¨åˆ†æ¨¡å‹ä¸æ”¯æŒè¯¥å‚æ•°ï¼Œä¼šè¿”å›400é”™è¯¯
                            if "Qwen" in model_name or "gpt-" in model_name.lower():
                                request_params["response_format"] = {"type": "json_object"}

                            response = current_client.chat.completions.create(**request_params)

                            response_content = response.choices[0].message.content
                            successful_model = model_name
                            successful_api_key_index = api_key_index
                            logger.info(f"è°ƒç”¨æˆåŠŸ - API key {api_key_index + 1}, æ¨¡å‹: {model_name}")
                            break  # æˆåŠŸåˆ™è·³å‡ºæ¨¡å‹å¾ªç¯

                        except Exception as api_error:
                            last_error = api_error
                            logger.warning(f"è°ƒç”¨å¤±è´¥ - API key {api_key_index + 1}, æ¨¡å‹ {model_name}: {str(api_error)[:200]}")

                            # å¦‚æœä¸æ˜¯æœ€åä¸€ä¸ªæ¨¡å‹ï¼Œç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹
                            if model_index < len(multimodal_model_names) - 1:
                                logger.info(f"ç»§ç»­å°è¯•è¯¥ API key çš„ä¸‹ä¸€ä¸ªæ¨¡å‹...")
                                continue

                    # å¦‚æœå½“å‰ API key çš„æŸä¸ªæ¨¡å‹æˆåŠŸï¼Œè·³å‡º API key å¾ªç¯
                    if response_content:
                        break

                    # å¦‚æœå½“å‰ API key çš„æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ª API key
                    if api_key_index < len(shuffled_api_keys) - 1:
                        logger.info(f"è¯¥ API key æ‰€æœ‰æ¨¡å‹å‡å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ª API key...")
                    else:
                        logger.error(f"æ‰€æœ‰ API key å’Œæ¨¡å‹ç»„åˆå‡å¤±è´¥")
                        if last_error:
                            raise last_error

                # å¤„ç†å“åº”å†…å®¹
                if response_content:
                    # è§£æJSONå“åº”
                    try:
                        import json as json_lib
                        response_data = json_lib.loads(response_content.strip())

                        has_medical_image = response_data.get('has_medical_image', False)
                        image_description = response_data.get('content', '')
                        image_bbox = response_data.get('image_bbox')  # ç®€åŒ–ï¼šåªä¿ç•™ä¸€ä¸ªbbox

                        # æ¸…ç†ç‰¹æ®Šæ ‡è®°
                        if image_description:
                            image_description = image_description.strip()
                            if image_description.startswith("\n<|begin_of_box|>"):
                                image_description = image_description[len("\n<|begin_of_box|>"):].strip()
                            elif image_description.startswith("<|begin_of_box|>"):
                                image_description = image_description[len("<|begin_of_box|>"):].strip()
                            if image_description.endswith("<|end_of_box|>"):
                                image_description = image_description[:-len("<|end_of_box|>")].strip()

                        if image_description.strip():
                            log_msg = f"æˆåŠŸæå–å›¾ç‰‡å†…å®¹: {filename} ({len(image_description)} å­—ç¬¦), æ¨¡å‹: {successful_model}, æ˜¯å¦åŒ…å«åŒ»å­¦å½±åƒ: {has_medical_image}"
                            if image_bbox:
                                log_msg += f", è¾¹ç•Œæ¡†: x={image_bbox.get('x', 0):.2f}, y={image_bbox.get('y', 0):.2f}"
                            logger.info(log_msg)

                            result['file_content'] = image_description
                            result['has_medical_image'] = has_medical_image
                            result['model_used'] = successful_model
                            result['extraction_success'] = True  # æ ‡è®°æå–æˆåŠŸ

                            # å­˜å‚¨è¾¹ç•Œæ¡†ç”¨äºè£å‰ª
                            if has_medical_image and image_bbox:
                                result['image_bbox'] = image_bbox
                                logger.info(f"åŒ»å­¦å½±åƒè¾¹ç•Œæ¡†: {image_bbox}")

                                # è£å‰ªåŒ»å­¦å½±åƒå¹¶ä¿å­˜
                                try:
                                    from PIL import Image as PILImage
                                    img = PILImage.open(path)
                                    img_width, img_height = img.size

                                    # è½¬æ¢å½’ä¸€åŒ–åæ ‡ä¸ºåƒç´ åæ ‡
                                    center_x = image_bbox.get('x', 0.5) * img_width
                                    center_y = image_bbox.get('y', 0.5) * img_height
                                    bbox_width = image_bbox.get('width', 1.0) * img_width
                                    bbox_height = image_bbox.get('height', 1.0) * img_height

                                    # è®¡ç®—è£å‰ªåŒºåŸŸ
                                    left = max(0, int(center_x - bbox_width / 2))
                                    top = max(0, int(center_y - bbox_height / 2))
                                    right = min(img_width, int(center_x + bbox_width / 2))
                                    bottom = min(img_height, int(center_y + bbox_height / 2))

                                    # è£å‰ªå›¾ç‰‡
                                    cropped_img = img.crop((left, top, right, bottom))

                                    # ä¿å­˜è£å‰ªåçš„å›¾ç‰‡åˆ°ä¸´æ—¶æ–‡ä»¶
                                    import tempfile
                                    temp_dir = tempfile.mkdtemp(prefix=f"cropped_images_{uuid.uuid4().hex[:8]}_")
                                    cropped_filename = f"cropped_{os.path.splitext(filename)[0]}.{image_ext if 'image_ext' in locals() else 'jpg'}"
                                    cropped_path = os.path.join(temp_dir, cropped_filename)

                                    # ä¿å­˜ä¸ºJPEGæ ¼å¼
                                    if cropped_img.mode in ('RGBA', 'LA', 'P'):
                                        # è½¬æ¢ä¸ºRGB
                                        background = PILImage.new('RGB', cropped_img.size, (255, 255, 255))
                                        if cropped_img.mode == 'P':
                                            cropped_img = cropped_img.convert('RGBA')
                                        background.paste(cropped_img, mask=cropped_img.split()[-1] if cropped_img.mode == 'RGBA' else None)
                                        cropped_img = background
                                    elif cropped_img.mode != 'RGB':
                                        cropped_img = cropped_img.convert('RGB')

                                    cropped_img.save(cropped_path, 'JPEG', quality=95)

                                    # ğŸš¨ ä¸ºè£å‰ªå›¾ç‰‡ç”Ÿæˆç‹¬ç«‹çš„UUID
                                    cropped_uuid = str(uuid.uuid4())

                                    # æ·»åŠ è£å‰ªå›¾ç‰‡ä¿¡æ¯åˆ°ç»“æœ
                                    result['cropped_image_uuid'] = cropped_uuid
                                    result['cropped_image_path'] = cropped_path
                                    result['cropped_image_available'] = True
                                    result['cropped_temp_dir'] = temp_dir
                                    result['cropped_image_filename'] = cropped_filename

                                    logger.info(f"æˆåŠŸè£å‰ªåŒ»å­¦å½±åƒ: {cropped_path}, UUID: {cropped_uuid}, å°ºå¯¸: {right-left}x{bottom-top}")

                                except Exception as crop_error:
                                    logger.error(f"è£å‰ªåŒ»å­¦å½±åƒå¤±è´¥: {str(crop_error)}")
                                    result['cropped_image_available'] = False
                        else:
                            logger.warning(f"å›¾ç‰‡å†…å®¹æå–ä¸ºç©º: {filename}")
                            result['file_content'] = f"å›¾ç‰‡æ–‡ä»¶: {filename} (APIè°ƒç”¨æˆåŠŸä½†è¿”å›å†…å®¹ä¸ºç©º)"
                            result['has_medical_image'] = False
                            result['model_used'] = successful_model
                            result['extraction_error'] = 'APIè°ƒç”¨æˆåŠŸä½†è¿”å›å†…å®¹ä¸ºç©º'
                    except Exception as json_error:
                        logger.warning(f"JSONè§£æå¤±è´¥: {filename}, é”™è¯¯: {str(json_error)}, åŸå§‹å†…å®¹: {response_content[:200]}")
                        # é™çº§å¤„ç†ï¼šç›´æ¥ä½¿ç”¨åŸå§‹å†…å®¹
                        image_description = response_content.strip()
                        if image_description.startswith("\n<|begin_of_box|>"):
                            image_description = image_description[len("\n<|begin_of_box|>"):].strip()
                        elif image_description.startswith("<|begin_of_box|>"):
                            image_description = image_description[len("<|begin_of_box|>"):].strip()
                        if image_description.endswith("<|end_of_box|>"):
                            image_description = image_description[:-len("<|end_of_box|>")].strip()

                        result['file_content'] = image_description
                        result['has_medical_image'] = False
                        result['model_used'] = successful_model
                        result['extraction_success'] = True if image_description else False  # åªæœ‰éç©ºå†…å®¹æ‰ç®—æˆåŠŸ
                        if not result['extraction_success']:
                            result['extraction_error'] = 'JSONè§£æå¤±è´¥ä¸”å†…å®¹ä¸ºç©º'
                else:
                    logger.warning(f"å›¾ç‰‡å†…å®¹æå–ä¸ºç©º: {filename}")
                    result['file_content'] = f"å›¾ç‰‡æ–‡ä»¶: {filename} (APIè°ƒç”¨æˆåŠŸä½†è¿”å›å†…å®¹ä¸ºç©º)"
                    result['has_medical_image'] = False
                    result['model_used'] = successful_model

                return result

            except Exception as outer_error:
                # æ„å»ºè¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                error_details = str(outer_error)
                error_type = type(outer_error).__name__
                status_code = None
                response_text = None

                # å°è¯•æå–HTTPå“åº”è¯¦æƒ…
                if hasattr(outer_error, 'response') and outer_error.response is not None:
                    try:
                        status_code = getattr(outer_error.response, 'status_code', None)
                        if hasattr(outer_error.response, 'json'):
                            error_json = outer_error.response.json()
                            response_text = f"APIé”™è¯¯å“åº”: {error_json}"
                        elif hasattr(outer_error.response, 'text'):
                            response_text = outer_error.response.text[:500]  # é™åˆ¶é•¿åº¦
                    except Exception as parse_error:
                        response_text = f"æ— æ³•è§£æå“åº”: {str(parse_error)}"

                # è®°å½•è¯¦ç»†çš„é”™è¯¯æ—¥å¿—
                logger.error(f"""å¤šæ¨¡æ€APIè°ƒç”¨å¤±è´¥è¯¦æƒ…:
æ–‡ä»¶å: {filename}
é”™è¯¯ç±»å‹: {error_type}
é”™è¯¯ä¿¡æ¯: {error_details}
HTTPçŠ¶æ€ç : {status_code}
å“åº”å†…å®¹: {response_text}
æ–‡ä»¶æ ¼å¼: {actual_format or file_extension}
MIMEç±»å‹: {mime_type}
å°è¯•çš„æ¨¡å‹: {multimodal_model_names}
æ–‡ä»¶å¤§å°: {file_size / 1024:.1f} KB""")

                # æ„å»ºè¿”å›çš„é”™è¯¯ä¿¡æ¯
                result['file_content'] = f"""å›¾ç‰‡æ–‡ä»¶: {filename}
å¤„ç†çŠ¶æ€: å¤šæ¨¡æ€APIè°ƒç”¨å¤±è´¥
é”™è¯¯ç±»å‹: {error_type}
é”™è¯¯ä¿¡æ¯: {error_details}
HTTPçŠ¶æ€ç : {status_code if status_code else 'N/A'}
æ–‡ä»¶æ ¼å¼: {actual_format or file_extension}
MIMEç±»å‹: {mime_type}
å°è¯•çš„æ¨¡å‹: {', '.join(multimodal_model_names)}"""
                result['extraction_error'] = f"{error_type}: {error_details[:200]}"  # æˆªå–å‰200å­—ç¬¦ä½œä¸ºé”™è¯¯ä¿¡æ¯

                return result

            finally:
                # æ¸…ç†ä¸´æ—¶è½¬æ¢æ–‡ä»¶
                if converted_file and os.path.exists(converted_file):
                    try:
                        # åˆ é™¤è½¬æ¢åçš„æ–‡ä»¶
                        os.unlink(converted_file)
                        # åˆ é™¤ä¸´æ—¶ç›®å½•
                        temp_dir = os.path.dirname(converted_file)
                        if os.path.exists(temp_dir):
                            os.rmdir(temp_dir)
                        logger.debug(f"æ¸…ç†HEICè½¬æ¢ä¸´æ—¶æ–‡ä»¶: {converted_file}")
                    except Exception as cleanup_error:
                        logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {cleanup_error}")

        except Exception as e:
            logger.error(f"""å›¾ç‰‡å¤„ç†æ•´ä½“å¤±è´¥:
æ–‡ä»¶å: {filename if 'filename' in locals() else (os.path.basename(original_path) if 'original_path' in locals() else os.path.basename(path))}
æ–‡ä»¶è·¯å¾„: {original_path if 'original_path' in locals() else path}
é”™è¯¯ç±»å‹: {type(e).__name__}
é”™è¯¯ä¿¡æ¯: {str(e)}
å †æ ˆè·Ÿè¸ª: {str(e.__traceback__) if hasattr(e, '__traceback__') else 'N/A'}""")
            filename = os.path.basename(original_path) if 'original_path' in locals() else (os.path.basename(path) if 'filename' not in locals() else filename)
            result = {
                'file_extension': 'image',
                'file_name': filename,
                'extraction_success': False,
                'extraction_error': f"{type(e).__name__}: {str(e)}"
            }
            result['file_content'] = f"å›¾ç‰‡æ–‡ä»¶: {filename} (å¤„ç†å¤±è´¥: {type(e).__name__}: {str(e)})"

            # ç¡®ä¿æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if 'converted_file' in locals() and converted_file and os.path.exists(converted_file):
                try:
                    os.unlink(converted_file)
                    temp_dir = os.path.dirname(converted_file)
                    if os.path.exists(temp_dir):
                        os.rmdir(temp_dir)
                except:
                    pass

            return result

    def read_md(self, path):
        try:
            filename = os.path.basename(path)
            result = {'file_extension': 'md', 'file_name': filename}
            with open(path, 'r', encoding='utf-8') as file:
                result['file_content'] = file.read()
            return result
        except Exception as e:
            logger.error(f"""MDæ–‡ä»¶è¯»å–å¤±è´¥:
æ–‡ä»¶å: {filename if 'filename' in locals() else os.path.basename(path)}
æ–‡ä»¶è·¯å¾„: {path}
é”™è¯¯ç±»å‹: {type(e).__name__}
é”™è¯¯ä¿¡æ¯: {str(e)}""")
            return {'file_extension': 'md', 'file_name': filename, 'file_content': ''}

    def read_json(self, path):
        try:
            filename = os.path.basename(path)
            result = {'file_extension': 'json', 'file_name': filename}
            with open(path, 'r', encoding='utf-8') as file:
                content = json.load(file)
                # Convert JSON object to string to maintain consistency
                result['file_content'] = content
            return result
        except Exception as e:
            logger.error(f"""JSONæ–‡ä»¶è¯»å–å¤±è´¥:
æ–‡ä»¶å: {filename if 'filename' in locals() else os.path.basename(path)}
æ–‡ä»¶è·¯å¾„: {path}
é”™è¯¯ç±»å‹: {type(e).__name__}
é”™è¯¯ä¿¡æ¯: {str(e)}""")
            return {'file_extension': 'json', 'file_name': filename, 'file_content': ''}

    def extract_text_html2text(self, url, ignore_links=False, ignore_images=False, ignore_videos=False):
        try:
            response = requests.get(url)
            response.raise_for_status()
            html_content = response.text

            h = html2text.HTML2Text()
            h.ignore_links = ignore_links
            h.ignore_images = ignore_images
            h.ignore_videos = ignore_videos
            text_content = h.handle(html_content)

            return text_content
        except Exception as e:
            logger.info(f"html2text extraction failed: {e}")
            return ""

    def extract_content_goose(self, url):
        try:
            g = Goose({'stopwords_class': StopWordsChinese})
            article = g.extract(url=url)
            html = article.raw_html
            soup = BeautifulSoup(html, 'html.parser')

            for img in soup.find_all('img'):
                img_src = img.get('src')
                if img_src:
                    img.replace_with(f'[Image: {img_src}]')

            for video in soup.find_all('video'):
                video_src = video.get('src')
                if video_src:
                    video.replace_with(f'[Video: {video_src}]')

            for a in soup.find_all('a'):
                link_href = a.get('href')
                if link_href:
                    a.replace_with(f'[Link: {link_href}]')

            for code in soup.find_all('pre'):
                code_text = code.get_text()
                if code_text:
                    code.replace_with(f'[Code Block: {code_text}]')

            text = soup.get_text()
            lines = text.splitlines()
            cleaned_lines = [line.strip() for line in lines if line.strip()]
            cleaned_text = "\n".join(cleaned_lines)

            return cleaned_text
        except Exception as e:
            logger.info(f"Goose extraction failed: {e}")
            return ""

    def extract_content_trafilatura(self, url):
        try:
            downloaded = trafilatura.fetch_url(url)
            content = trafilatura.extract(downloaded, include_images=True, include_links=True, include_formatting=True)
            return content
        except Exception as e:
            logger.info(f"Trafilatura extraction failed: {e}")
            return ""

    def read_url(self, url, priority=None):
        methods = {
            'Trafilatura': self.extract_content_trafilatura,
            'html2text': self.extract_text_html2text,
            'Goose': self.extract_content_goose
        }
        
        if not priority:
            priority = list(methods.keys())
            random.shuffle(priority)
        
        for method_name in priority:
            method = methods[method_name]
            text = method(url)
            if text and len(text) > 50 and "å½“å‰ç¯å¢ƒå¼‚å¸¸ï¼Œå®ŒæˆéªŒè¯åå³å¯ç»§ç»­è®¿é—®" not in text:
                return {'file_extension': 'url', 'file_name': url, 'file_content': text}

        return None

    def unzip_file(self, zip_file_path):
        extract_to_path = os.path.dirname(zip_file_path)
        zip_filename_without_ext = os.path.splitext(os.path.basename(zip_file_path))[0]
        extract_path = os.path.join(extract_to_path, zip_filename_without_ext)
        
        os.makedirs(extract_path, exist_ok=True)
        
        try:
            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
                zip_info_list = zip_ref.infolist()
                total_files = len(zip_info_list)
                logger.info(f"å¼€å§‹è§£å‹ZIPæ–‡ä»¶ï¼ŒåŒ…å« {total_files} ä¸ªæ¡ç›®")
                
                extracted_count = 0
                skipped_count = 0
                
                for zip_info in zip_info_list:
                    try:
                        original_filename = zip_info.filename
                        
                        # å°è¯•ä¸åŒçš„ç¼–ç æ–¹å¼è§£ç æ–‡ä»¶å
                        filename = None
                        encoding_attempts = [
                            ('cp437', 'utf-8'),
                            ('cp437', 'gbk'),
                            ('utf-8', None),
                        ]
                        
                        for src_encoding, target_encoding in encoding_attempts:
                            try:
                                if target_encoding:
                                    filename = original_filename.encode(src_encoding).decode(target_encoding)
                                else:
                                    detected_encoding = chardet.detect(original_filename.encode('utf-8'))
                                    detected_enc = detected_encoding.get('encoding', 'utf-8')
                                    filename = original_filename.encode('utf-8').decode(detected_enc)
                                break
                            except (UnicodeDecodeError, TypeError):
                                continue
                        
                        if filename is None:
                            logger.warning(f"æ–‡ä»¶åç¼–ç å¤±è´¥ï¼Œè·³è¿‡: {original_filename}")
                            skipped_count += 1
                            continue

                        zip_info.filename = filename
                        zip_ref.extract(zip_info, extract_path)
                        extracted_count += 1
                        
                    except Exception as e:
                        logger.warning(f"è§£å‹æ–‡ä»¶å¤±è´¥: {zip_info.filename}")
                        skipped_count += 1
                        continue
                
                logger.info(f"ZIPè§£å‹å®Œæˆ: æˆåŠŸ {extracted_count} ä¸ª, è·³è¿‡ {skipped_count} ä¸ª")
                
        except Exception as e:
            logger.error(f"è§£å‹ZIPæ–‡ä»¶å¤±è´¥: {str(e)}")
            raise
        
        return extract_path

    def process_zip_file(self, zip_file_path, processed_zips=None, cleanup=True):
        """
        å¤„ç†zipæ–‡ä»¶ï¼šè§£å‹å¹¶é€’å½’æå–æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶ç±»å‹å†…å®¹ï¼ŒåŒ…æ‹¬åµŒå¥—çš„zipæ–‡ä»¶
        è¿”å›æ ¼å¼ä¸å…¶ä»–æ–‡ä»¶ç±»å‹ä¿æŒä¸€è‡´
        """
        start_time = time.time()
        zip_filename = os.path.basename(zip_file_path)
        logger.info(f"å¼€å§‹å¤„ç†ZIPæ–‡ä»¶: {zip_filename}")
        
        # é˜²æ­¢å¾ªç¯å¼•ç”¨çš„zipæ–‡ä»¶
        if processed_zips is None:
            processed_zips = set()
        
        # è·å–zipæ–‡ä»¶çš„ç»å¯¹è·¯å¾„ï¼Œé¿å…é‡å¤å¤„ç†
        abs_zip_path = os.path.abspath(zip_file_path)
        if abs_zip_path in processed_zips:
            logger.warning(f"æ£€æµ‹åˆ°å¾ªç¯å¼•ç”¨çš„zipæ–‡ä»¶ï¼Œè·³è¿‡: {zip_filename}")
            return []
        
        processed_zips.add(abs_zip_path)
        
        extract_path = None
        persistent_temp_dir = None  # ğŸš¨ æ–°å¢ï¼šæŒä¹…ä¸´æ—¶ç›®å½•ï¼Œç”¨äºä¿å­˜éœ€è¦ä¸Šä¼ çš„æ–‡ä»¶
        
        try:
            file_size_mb = os.path.getsize(zip_file_path) / 1024 / 1024
            logger.info(f"ZIPæ–‡ä»¶å¤§å°: {file_size_mb:.1f} MB")
            
            # åˆ›å»ºæŒä¹…ä¸´æ—¶ç›®å½•ï¼Œç”¨äºä¿å­˜éœ€è¦ä¸Šä¼ çš„äºŒè¿›åˆ¶æ–‡ä»¶
            import tempfile
            persistent_temp_dir = tempfile.mkdtemp(prefix=f"zip_upload_{uuid.uuid4().hex[:8]}_")
            
            # è§£å‹zipæ–‡ä»¶
            extract_path = self.unzip_file(zip_file_path)
            
            if not os.path.exists(extract_path):
                logger.error(f"è§£å‹å¤±è´¥ï¼Œè·¯å¾„ä¸å­˜åœ¨: {extract_path}")
                return [{
                    'file_extension': 'zip',
                    'file_name': zip_filename,
                    'file_content': f'è§£å‹å¤±è´¥ï¼šè·¯å¾„ä¸å­˜åœ¨'
                }]
            
            # è·å–è§£å‹åçš„æ‰€æœ‰æ–‡ä»¶
            extracted_files = self.get_all_files_recursive(extract_path)
            logger.info(f"ZIPè§£å‹å®Œæˆï¼ŒåŒ…å« {len(extracted_files)} ä¸ªæ–‡ä»¶")
            
            if not extracted_files:
                logger.warning(f"ZIPæ–‡ä»¶è§£å‹åæœªæ‰¾åˆ°ä»»ä½•æ–‡ä»¶")
                return [{
                    'file_extension': 'zip',
                    'file_name': zip_filename,
                    'file_content': 'è§£å‹åæœªæ‰¾åˆ°ä»»ä½•æ–‡ä»¶'
                }]
            
            # åˆ†ç±»å¤„ç†æ–‡ä»¶
            non_zip_files = []
            zip_files = []
            other_files = []
            
            for file_path in extracted_files:
                file_ext = os.path.splitext(file_path)[1].lower()
                
                if file_ext == '.zip':
                    zip_files.append(file_path)
                elif file_ext in ['.pdf', '.docx', '.pptx', '.ppt', '.txt', '.md', '.json', '.png', '.jpg', '.jpeg', '.webp', '.heic', '.heif']:
                    non_zip_files.append(file_path)
                else:
                    other_files.append(file_path)
            
            logger.info(f"æ–‡ä»¶åˆ†ç±»: {len(non_zip_files)} ä¸ªæ”¯æŒæ–‡ä»¶, {len(zip_files)} ä¸ªåµŒå¥—ZIP, {len(other_files)} ä¸ªå…¶ä»–æ–‡ä»¶")
            
            # å¤„ç†æ‰€æœ‰æ–‡ä»¶
            processed_files = []
            
            # å¤„ç†ézipæ–‡ä»¶
            if non_zip_files:
                # ä»ç¯å¢ƒå˜é‡è·å–å¹¶å‘æ•°é…ç½®
                max_workers = int(os.getenv("PDF_IMAGE_CONCURRENT_WORKERS", "10"))
                logger.info(f"å¼€å§‹å¹¶å‘å¤„ç† {len(non_zip_files)} ä¸ªæ–‡ä»¶ï¼ˆå¹¶å‘æ•°ï¼š{max_workers}ï¼‰...")
                success_count = 0
                fail_count = 0

                # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
                with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                    futures = {executor.submit(self._run_single_file, file_path): file_path for file_path in non_zip_files}
                    
                    for future in concurrent.futures.as_completed(futures):
                        file_path = futures[future]
                        filename = os.path.basename(file_path)
                        
                        try:
                            result = future.result()
                            if result:
                                if isinstance(result, list):
                                    for sub_result in result:
                                        if isinstance(sub_result, dict):
                                            original_path = file_path
                                            file_ext = os.path.splitext(filename)[1].lower()
                                            
                                            if file_ext in ['.png', '.jpg', '.jpeg', '.webp', '.heic', '.heif', '.pdf', '.docx', '.pptx', '.ppt']:
                                                # ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶åˆ›å»ºæŒä¹…å‰¯æœ¬
                                                persistent_filename = f"{sub_result.get('file_uuid', str(uuid.uuid4()))}{file_ext}"
                                                persistent_path = os.path.join(persistent_temp_dir, persistent_filename)
                                                
                                                try:
                                                    shutil.copy2(original_path, persistent_path)
                                                    sub_result['original_file_path'] = persistent_path
                                                    sub_result['temp_file_available'] = True
                                                    sub_result['persistent_temp_file'] = True
                                                except Exception as e:
                                                    logger.warning(f"å¤åˆ¶æ–‡ä»¶å¤±è´¥: {filename}")
                                                    sub_result['original_file_path'] = original_path
                                                    sub_result['temp_file_available'] = False
                                            else:
                                                sub_result['original_file_path'] = original_path
                                                sub_result['temp_file_available'] = True
                                    
                                    processed_files.extend(result)
                                    success_count += len(result)
                                else:
                                    if isinstance(result, dict):
                                        original_path = file_path
                                        file_ext = os.path.splitext(filename)[1].lower()
                                        
                                        if file_ext in ['.png', '.jpg', '.jpeg', '.webp', '.heic', '.heif', '.pdf', '.docx', '.pptx', '.ppt']:
                                            persistent_filename = f"{result.get('file_uuid', str(uuid.uuid4()))}{file_ext}"
                                            persistent_path = os.path.join(persistent_temp_dir, persistent_filename)
                                            
                                            try:
                                                shutil.copy2(original_path, persistent_path)
                                                result['original_file_path'] = persistent_path
                                                result['temp_file_available'] = True
                                                result['persistent_temp_file'] = True
                                            except Exception as e:
                                                logger.warning(f"å¤åˆ¶æ–‡ä»¶å¤±è´¥: {filename}")
                                                result['original_file_path'] = original_path
                                                result['temp_file_available'] = False
                                        else:
                                            result['original_file_path'] = original_path
                                            result['temp_file_available'] = True
                                    
                                    processed_files.append(result)
                                    success_count += 1
                            else:
                                fail_count += 1
                                logger.warning(f"æ–‡ä»¶å¤„ç†è¿”å›ç©ºç»“æœ: {filename}")
                        except Exception as e:
                            fail_count += 1
                            logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {filename}: {str(e)}")
                            continue
                
                logger.info(f"æ–‡ä»¶å¤„ç†å®Œæˆ: æˆåŠŸ {success_count} ä¸ª, å¤±è´¥ {fail_count} ä¸ª")
            
            # å¤„ç†åµŒå¥—zipæ–‡ä»¶
            if zip_files:
                logger.info(f"å¼€å§‹å¤„ç† {len(zip_files)} ä¸ªåµŒå¥—ZIPæ–‡ä»¶...")
                nested_success_count = 0
                nested_fail_count = 0
                
                # ä½¿ç”¨è¾ƒå°çš„çº¿ç¨‹æ± å¤„ç†åµŒå¥—zipæ–‡ä»¶
                with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
                    process_func = partial(self.process_zip_file, processed_zips=processed_zips.copy(), cleanup=False)
                    futures = {executor.submit(process_func, zip_file_path): zip_file_path for zip_file_path in zip_files}
                    
                    for future in concurrent.futures.as_completed(futures):
                        zip_file_path = futures[future]
                        zip_filename = os.path.basename(zip_file_path)
                        
                        try:
                            nested_results = future.result()
                            
                            if nested_results:
                                if isinstance(nested_results, list):
                                    processed_files.extend(nested_results)
                                    nested_success_count += len(nested_results)
                                else:
                                    processed_files.append(nested_results)
                                    nested_success_count += 1
                                logger.info(f"åµŒå¥—ZIPå¤„ç†å®Œæˆ: {zip_filename}")
                            else:
                                nested_fail_count += 1
                                logger.warning(f"åµŒå¥—ZIPå¤„ç†è¿”å›ç©ºç»“æœ: {zip_filename}")
                        except Exception as e:
                            nested_fail_count += 1
                            logger.error(f"å¤„ç†åµŒå¥—ZIPå¤±è´¥ {zip_filename}: {str(e)}")
                            continue
                
                logger.info(f"åµŒå¥—ZIPå¤„ç†å®Œæˆ: æˆåŠŸ {nested_success_count} ä¸ªæ–‡ä»¶, å¤±è´¥ {nested_fail_count} ä¸ªZIP")
            
            total_elapsed = time.time() - start_time
            logger.info(f"ZIPæ–‡ä»¶å¤„ç†å®Œæˆ: {zip_filename} (ç”¨æ—¶ {total_elapsed:.1f}s, å…± {len(processed_files)} ä¸ªæ–‡ä»¶)")
            
            # ä¸ºç»“æœæ·»åŠ æ¸…ç†ä¿¡æ¯
            if persistent_temp_dir and processed_files:
                for pf in processed_files:
                    if isinstance(pf, dict) and pf.get('persistent_temp_file'):
                        pf['cleanup_temp_dir'] = persistent_temp_dir
            
            return processed_files
            
        except Exception as e:
            total_elapsed = time.time() - start_time
            logger.error(f"ZIPæ–‡ä»¶å¤„ç†å¤±è´¥: {zip_filename} - {str(e)} (ç”¨æ—¶ {total_elapsed:.1f}s)")
            return [{
                'file_extension': 'zip',
                'file_name': zip_filename,
                'file_content': f'ZIPæ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}'
            }]
        finally:
            # æ¸…ç†è§£å‹çš„ä¸´æ—¶æ–‡ä»¶å¤¹ï¼ˆåªæœ‰é¡¶çº§zipæ‰æ¸…ç†ï¼‰
            if cleanup and extract_path and os.path.exists(extract_path):
                try:
                    shutil.rmtree(extract_path)
                    logger.debug(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤¹: {extract_path}")
                except Exception as e:
                    logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤¹å¤±è´¥: {str(e)}")
            
            # æ³¨æ„ï¼šä¸æ¸…ç†persistent_temp_dirï¼Œè®©conversations.pyåœ¨ä¸Šä¼ å®Œæˆåæ¸…ç†

    def get_all_files_recursive(self, folder_path):
        """é€’å½’è·å–æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶è·¯å¾„"""
        file_paths = []
        
        try:
            for root, dirs, files in os.walk(folder_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    file_paths.append(file_path)
            
            logger.debug(f"é€’å½’æ‰«æå®Œæˆ: {len(file_paths)} ä¸ªæ–‡ä»¶")
            
        except Exception as e:
            logger.error(f"é€’å½’éå†æ–‡ä»¶å¤¹å¤±è´¥: {str(e)}")
        
        return file_paths

    def _run_single_file(self, path: str):
        """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼Œä¸é€’å½’å¤„ç†zipæ–‡ä»¶"""
        filename = os.path.basename(path)
        
        # è¿‡æ»¤ç³»ç»Ÿéšè—æ–‡ä»¶
        if filename.startswith('._') or filename.startswith('.DS_Store'):
            logger.warning(f"è·³è¿‡ç³»ç»Ÿéšè—æ–‡ä»¶: {filename}")
            return {
                'file_extension': 'hidden',
                'file_name': filename,
                'file_content': f"ç³»ç»Ÿéšè—æ–‡ä»¶: {filename} (å·²è·³è¿‡å¤„ç†)"
            }
        
        if not os.path.isfile(path):
            logger.warning(f"è·¯å¾„ä¸æ˜¯æœ‰æ•ˆæ–‡ä»¶: {path}")
            return None
            
        file_extension = os.path.splitext(path)[1].lower()
        
        try:
            # å¤„ç†å„ç§æ–‡ä»¶ç±»å‹ï¼Œä½†ä¸å¤„ç†zipæ–‡ä»¶
            result = None
            if file_extension == '.pdf':
                result = self.read_pdf(path)
            elif file_extension == '.docx':
                result = self.read_docx(path)
            elif file_extension in ['.pptx', '.ppt']:
                result = self.read_ppt(path)
            elif file_extension == '.txt':
                result = self.read_txt(path)
            elif file_extension == '.md':
                result = self.read_md(path)
            elif file_extension == '.json':
                result = self.read_json(path)
            elif file_extension in ['.png', '.jpg', '.jpeg', '.webp', '.heic', '.heif']:
                result = self.read_image(path)
            elif file_extension == '.zip':
                # åœ¨å•æ–‡ä»¶å¤„ç†ä¸­ä¸å¤„ç†zipï¼Œé¿å…é‡å¤
                return None
            else:
                result = self.read_file(path)
            
            if result and isinstance(result, dict):
                content = result.get('file_content', '')
                content_length = len(str(content)) if content else 0
                if content_length > 0:
                    logger.debug(f"æ–‡ä»¶å¤„ç†æˆåŠŸ: {filename} ({content_length} å­—ç¬¦)")
                else:
                    logger.warning(f"æ–‡ä»¶å¤„ç†å®Œæˆä½†å†…å®¹ä¸ºç©º: {filename}")
            
            return result
            
        except Exception as e:
            logger.error(f"""å•æ–‡ä»¶å¤„ç†å¤±è´¥:
æ–‡ä»¶å: {filename}
æ–‡ä»¶è·¯å¾„: {path}
æ–‡ä»¶ç±»å‹: {file_extension}
é”™è¯¯ç±»å‹: {type(e).__name__}
é”™è¯¯ä¿¡æ¯: {str(e)}""")
            return None

    def list_files_in_folder(self, folder_path):
        file_paths = []
        for filename in os.listdir(folder_path):
            filepath = os.path.join(folder_path, filename)
            if os.path.isfile(filepath):
                file_paths.append(filepath)
        return file_paths

    def read_file(self, file_path):
        result = {}
        file_name = os.path.basename(file_path)
        file_name_without_extension, file_extension = os.path.splitext(file_name)

        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                file_content = file.read()
        except UnicodeDecodeError:
            try:
                with open(file_path, 'r', encoding='gbk') as file:
                    file_content = file.read()
            except Exception:
                return []
        except Exception:
            return []

        result['file_extension'] = file_extension
        result['file_name'] = file_name
        result['file_content'] = file_content

        return result


    def is_text_garbled(self, text):
        chinese_characters = re.findall(r'[\u4e00-\u9fff]', text)
        symbol_characters = re.findall(r'[\u0000-\u0020\u3000\uFFFD]', text)

        if len(chinese_characters) > 0:
            chinese_ratio = len(chinese_characters) / max(len(text), 1)
            symbol_ratio = len(symbol_characters) / max(len(text), 1)
            return chinese_ratio < 0.2 or symbol_ratio > 0.3

        non_ascii_ratio = sum(1 for char in text if ord(char) > 127) / max(len(text), 1)
        return non_ascii_ratio > 0.3

    def read_pdf_with_images(self, filepath):
        """
        æ–°çš„PDFå¤„ç†æ–¹å¼ï¼šæå–æ–‡æœ¬å’Œå›¾ç‰‡ï¼Œå›¾ç‰‡ç”¨å¤šæ¨¡æ€æ¨¡å‹å¤„ç†
        è¿”å›åˆ—è¡¨ï¼š[PDFæœ¬èº«, å›¾ç‰‡1, å›¾ç‰‡2, ...]
        """
        filename = os.path.basename(filepath)
        pdf_uuid = str(uuid.uuid4())

        logger.info(f"å¼€å§‹æå–PDFå†…å®¹ï¼ˆå¸¦å›¾ç‰‡æ¨¡å¼ï¼‰: {filename}")

        def clean_text(text):
            try:
                return text.encode('utf-8', 'ignore').decode('utf-8')
            except UnicodeDecodeError:
                try:
                    return text.encode('utf-8', 'ignore').decode('gbk')
                except UnicodeDecodeError:
                    return text

        try:
            import tempfile

            # æ‰“å¼€PDFæ–‡æ¡£
            doc = fitz.open(filepath)

            full_content_parts = []  # å®Œæ•´å†…å®¹ï¼ˆæ–‡æœ¬+å›¾ç‰‡æè¿°æŒ‰é¡µé¢é¡ºåºï¼‰
            extracted_images = []  # æå–çš„å›¾ç‰‡åˆ—è¡¨
            total_images = 0

            # åˆ›å»ºä¸´æ—¶ç›®å½•ä¿å­˜æå–çš„å›¾ç‰‡
            temp_dir = tempfile.mkdtemp(prefix=f"pdf_images_{uuid.uuid4().hex[:8]}_")
            logger.info(f"åˆ›å»ºä¸´æ—¶ç›®å½•: {temp_dir}")

            try:
                # ä¿å­˜é¡µé¢æ€»æ•°ï¼ˆåœ¨å…³é—­æ–‡æ¡£å‰ï¼‰
                total_pages = len(doc)

                # æ­¥éª¤1ï¼šå…ˆæ¸²æŸ“æ‰€æœ‰é¡µé¢ä¸ºå›¾ç‰‡ï¼ˆä¸è°ƒç”¨å¤šæ¨¡æ€æ¨¡å‹ï¼‰
                page_image_paths = []
                for page_num in range(total_pages):
                    page = doc[page_num]
                    page_number = page_num + 1

                    # 1. æå–é¡µé¢æ–‡æœ¬
                    page_text = clean_text(page.get_text())
                    if page_text and page_text.strip():
                        full_content_parts.append(f"[ç¬¬{page_number}é¡µ]\n{page_text.strip()}")
                        logger.info(f"æå–ç¬¬{page_number}é¡µæ–‡æœ¬: {len(page_text)} å­—ç¬¦")

                    # 2. æ¸²æŸ“é¡µé¢ä¸ºå›¾ç‰‡
                    try:
                        pix = page.get_pixmap(matrix=fitz.Matrix(2.0, 2.0))
                        img_filename = f"page_{page_number}.png"
                        img_path = os.path.join(temp_dir, img_filename)
                        pix.save(img_path)

                        width = pix.width
                        height = pix.height
                        file_size = os.path.getsize(img_path)

                        logger.info(f"æ¸²æŸ“ç¬¬{page_number}é¡µä¸ºå®Œæ•´å›¾ç‰‡: {img_filename} ({file_size} bytes, {width}x{height}px)")
                        page_image_paths.append((page_number, img_path))

                    except Exception as img_error:
                        logger.error(f"æ¸²æŸ“ç¬¬{page_number}é¡µä¸ºå›¾ç‰‡æ—¶å‡ºé”™: {str(img_error)}")
                        continue

                # å…³é—­PDFæ–‡æ¡£
                doc.close()

                # æ­¥éª¤2ï¼šå¹¶å‘è°ƒç”¨å¤šæ¨¡æ€æ¨¡å‹å¤„ç†æ‰€æœ‰å›¾ç‰‡
                if page_image_paths:
                    max_workers = int(os.getenv("PDF_IMAGE_CONCURRENT_WORKERS", "10"))
                    logger.info(f"å¼€å§‹å¹¶å‘å¤„ç† {len(page_image_paths)} å¼ PDFé¡µé¢å›¾ç‰‡ï¼ˆå¹¶å‘æ•°ï¼š{max_workers}ï¼‰...")

                    def process_page_image(page_info):
                        """å¤„ç†å•ä¸ªé¡µé¢å›¾ç‰‡"""
                        page_number, img_path = page_info
                        try:
                            image_result = self.read_image(img_path)
                            return (page_number, img_path, image_result)
                        except Exception as e:
                            logger.error(f"å¤„ç†ç¬¬{page_number}é¡µå›¾ç‰‡å¤±è´¥: {str(e)}")
                            return (page_number, img_path, None)

                    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
                    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                        futures = {executor.submit(process_page_image, page_info): page_info for page_info in page_image_paths}

                        for future in concurrent.futures.as_completed(futures):
                            page_info = futures[future]
                            page_number = page_info[0]

                            try:
                                page_number, img_path, image_result = future.result()

                                if image_result:
                                    img_uuid = str(uuid.uuid4())
                                    image_description = image_result.get('file_content', '')
                                    has_medical_image = image_result.get('has_medical_image', False)

                                    # æ’å…¥å›¾ç‰‡æè¿°åˆ°å®Œæ•´å†…å®¹ä¸­ï¼ˆä¿æŒé¡ºåºï¼‰
                                    if image_description and image_description.strip():
                                        full_content_parts.append(
                                            f"\n[ç¬¬{page_number}é¡µ - å®Œæ•´é¡µé¢å›¾ç‰‡]\n{image_description.strip()}\n"
                                        )
                                        logger.info(f"æå–ç¬¬{page_number}é¡µå®Œæ•´å›¾ç‰‡å†…å®¹: {len(image_description)} å­—ç¬¦, åŒ»å­¦å½±åƒ: {has_medical_image}")

                                    # æ„å»ºå›¾ç‰‡é¡¹
                                    image_item = {
                                        'file_uuid': img_uuid,
                                        'file_name': f"{os.path.splitext(filename)[0]}_ç¬¬{page_number}é¡µ.png",
                                        'file_extension': 'png',
                                        'file_content': image_description,
                                        'has_medical_image': has_medical_image,
                                        'model_used': image_result.get('model_used'),

                                        # æ¥æºä¿¡æ¯
                                        'source_type': 'rendered_pdf_page',
                                        'parent_pdf_uuid': pdf_uuid,
                                        'parent_pdf_filename': filename,

                                        # ä½ç½®ä¿¡æ¯
                                        'page_number': page_number,
                                        'image_index_in_page': 0,
                                        'position_in_parent': f"page_{page_number}_full",

                                        # ä¸´æ—¶æ–‡ä»¶è·¯å¾„
                                        'temp_file_path': img_path,
                                        'temp_file_available': True,
                                        'persistent_temp_file': True,
                                        'cleanup_temp_dir': temp_dir,

                                        # åŒ»å­¦å½±åƒè£å‰ªä¿¡æ¯
                                        'image_bbox': image_result.get('image_bbox'),
                                        'cropped_image_uuid': image_result.get('cropped_image_uuid'),
                                        'cropped_image_path': image_result.get('cropped_image_path'),
                                        'cropped_image_filename': image_result.get('cropped_image_filename'),
                                        'cropped_image_available': image_result.get('cropped_image_available', False),
                                        'cropped_temp_dir': image_result.get('cropped_temp_dir')
                                    }

                                    extracted_images.append(image_item)
                                    total_images += 1
                                else:
                                    logger.warning(f"ç¬¬{page_number}é¡µå®Œæ•´å›¾ç‰‡æå–å¤±è´¥")

                            except Exception as e:
                                logger.error(f"å¤„ç†ç¬¬{page_number}é¡µç»“æœæ—¶å‡ºé”™: {str(e)}")
                                continue

                # æ„å»ºPDFæœ¬èº«çš„é¡¹
                pdf_item = {
                    'file_uuid': pdf_uuid,
                    'file_name': filename,
                    'file_extension': 'pdf',
                    'file_content': '\n\n'.join(full_content_parts) if full_content_parts else '',
                    'extraction_mode': 'with_images',
                    'extracted_image_count': total_images,
                    'extraction_success': True
                }

                logger.info(f"PDFæå–å®Œæˆ: {filename}, æ€»é¡µæ•°: {total_pages}, æå–å›¾ç‰‡: {total_images} å¼ ")

                # è¿”å›åˆ—è¡¨ï¼š[PDFæœ¬èº«, å›¾ç‰‡1, å›¾ç‰‡2, ...]
                return [pdf_item] + extracted_images

            except Exception as e:
                logger.error(f"PDFå›¾ç‰‡æå–è¿‡ç¨‹å‡ºé”™: {filename}, é”™è¯¯: {str(e)}")
                # æ¸…ç†ä¸´æ—¶ç›®å½•
                if os.path.exists(temp_dir):
                    try:
                        shutil.rmtree(temp_dir)
                    except:
                        pass
                raise

        except Exception as e:
            logger.error(f"PDFå›¾ç‰‡æå–æ•´ä½“å¤±è´¥: {filename}, é”™è¯¯: {str(e)}")
            return {
                'file_uuid': pdf_uuid,
                'file_name': filename,
                'file_extension': 'pdf',
                'file_content': f'PDFå›¾ç‰‡æå–å¤±è´¥: {str(e)}',
                'extraction_success': False,
                'extraction_error': f"{type(e).__name__}: {str(e)}"
            }

    def read_pdf(self, filepath):
        """åŸæœ‰çš„PDFå¤„ç†æ–¹å¼ï¼šåªæå–æ–‡æœ¬"""
        filename = os.path.basename(filepath)
        result = {'file_extension': 'pdf', 'file_name': filename}

        def clean_text(text):
            # å°è¯•å…ˆç”¨ UTF-8 è§£ç ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨ GBK è§£ç 
            try:
                return text.encode('utf-8', 'ignore').decode('utf-8')
            except UnicodeDecodeError:
                try:
                    return text.encode('utf-8', 'ignore').decode('gbk')
                except UnicodeDecodeError:
                    return text  # å¦‚æœä¸¤ç§è§£ç éƒ½å¤±è´¥ï¼Œä¿æŒåŸå§‹æ–‡æœ¬

        # 1. å°è¯•ä½¿ç”¨ OCR API
        # å®šä¹‰æ–°çš„OCR API URL
        ocr_api_url = 'https://pdf-parsing.dev.6ccloud.com/pdf_parsing_api'
        try:
            with open(filepath, 'rb') as file:
                pdf = pypdf.PdfReader(file)
                num_pages = len(pdf.pages)
                all_markdown_content = []
                
                for i in range(0, num_pages, 20):
                    end_page = min(i + 20, num_pages)
                    
                    # åˆ›å»ºä¸€ä¸ªæ–°çš„ PDF å†™å…¥å™¨
                    pdf_writer = pypdf.PdfWriter()
                    for page_num in range(i, end_page):
                        pdf_writer.add_page(pdf.pages[page_num])
                    
                    # å°†åˆ‡åˆ†åçš„ PDF ä¿å­˜åˆ°å†…å­˜ä¸­
                    temp_pdf = io.BytesIO()
                    pdf_writer.write(temp_pdf)
                    temp_pdf.seek(0)
                    
                    # å‘é€è¯·æ±‚åˆ°æ–°çš„ OCR API
                    files = {'pdf_file': (f'{filename}_part_{i//20+1}.pdf', temp_pdf, 'application/pdf')}
                    response = requests.post(ocr_api_url, files=files, headers={'accept': 'application/json'}, timeout=600)
                    
                    # å¤„ç†å“åº”
                    if response.status_code == 200:
                        response_json = response.json()
                        markdown_content = response_json.get('markdown', '')
                        if markdown_content:
                            all_markdown_content.append(markdown_content)
                        logger.info(f"{filename} part {i//20+1}, OCR API returned markdown content: {markdown_content[:50]}...")
                    elif response.status_code in [400, 404, 420, 500]:
                        # è®°å½•è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                        try:
                            error_body = response.json() if response.content else "æ— å“åº”å†…å®¹"
                        except:
                            error_body = response.text[:500] if response.text else "æ— æ³•è§£æå“åº”"
                        logger.error(f"""OCR APIè°ƒç”¨å¤±è´¥è¯¦æƒ…:
æ–‡ä»¶å: {filename} part {i//20+1}
HTTPçŠ¶æ€ç : {response.status_code}
å“åº”å†…å®¹: {error_body}
è¯·æ±‚URL: {ocr_api_url}""")
                        # ç»§ç»­å°è¯•å…¶ä»–é¡µ
                    else:
                        # è®°å½•è¯¦ç»†çš„éé¢„æœŸçŠ¶æ€ç é”™è¯¯
                        try:
                            error_body = response.json() if response.content else "æ— å“åº”å†…å®¹"
                        except:
                            error_body = response.text[:500] if response.text else "æ— æ³•è§£æå“åº”"
                        logger.error(f"""OCR APIè¿”å›éé¢„æœŸçŠ¶æ€ç :
æ–‡ä»¶å: {filename} part {i//20+1}
HTTPçŠ¶æ€ç : {response.status_code}
å“åº”å†…å®¹: {error_body}
è¯·æ±‚URL: {ocr_api_url}""")
                
                if all_markdown_content:
                    combined_ocr_text = clean_text("".join(all_markdown_content))
                    logger.info(f"OCR API extracted: {combined_ocr_text[:50]}...")  # æ‰“å°å‰ 50 ä¸ªå­—ç¬¦
                    if combined_ocr_text and not self.is_text_garbled(combined_ocr_text):
                        result['file_content'] = combined_ocr_text
                        return result
                logger.info(f"OCR result is empty or garbled for {filename}")
            
        except Exception as e:
            logger.error(f"""OCR APIæ•´ä½“å¤„ç†å¤±è´¥:
æ–‡ä»¶å: {filename}
é”™è¯¯ç±»å‹: {type(e).__name__}
é”™è¯¯ä¿¡æ¯: {str(e)}
è¯·æ±‚URL: {ocr_api_url}""")
            if 'response' in locals():
                try:
                    logger.error(f"""OCRå“åº”è¯¦æƒ…:
çŠ¶æ€ç : {response.status_code}
å“åº”å†…å®¹: {response.text[:500]}""")
                except:
                    pass

        # 2. å°è¯•ä½¿ç”¨ MinerU
        mineru_token = os.getenv("MINERU_TOKEN")
        if mineru_token:
            try:
                logger.info(f"Attempting to extract {filename} using MinerU...")

                # MinerU APIé…ç½®
                mineru_base_url = "https://mineru.net/api/v4"

                # Step 1: ç”³è¯·ä¸Šä¼ URL
                upload_url_endpoint = f"{mineru_base_url}/file-urls/batch"
                headers = {
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {mineru_token}"
                }
                upload_data = {
                    "enable_formula": True,
                    "language": "ch",
                    "enable_table": True,
                    "files": [
                        {
                            "name": filename,
                            "is_ocr": True,
                            "data_id": f"pdf_{uuid.uuid4().hex[:8]}"
                        }
                    ]
                }

                upload_response = requests.post(upload_url_endpoint, headers=headers, json=upload_data, timeout=30)

                if upload_response.status_code == 200:
                    upload_result = upload_response.json()
                    if upload_result.get("code") == 0:
                        batch_id = upload_result["data"]["batch_id"]
                        upload_url = upload_result["data"]["file_urls"][0]
                        logger.info(f"MinerU: Upload URL obtained, batch_id={batch_id}")

                        # Step 2: ä¸Šä¼ æ–‡ä»¶
                        with open(filepath, 'rb') as f:
                            upload_file_response = requests.put(upload_url, data=f, timeout=300)

                        if upload_file_response.status_code == 200:
                            logger.info(f"MinerU: File uploaded successfully")

                            # Step 3: ç­‰å¾…å¤„ç†å¹¶è½®è¯¢ç»“æœ
                            time.sleep(5)  # ç­‰å¾…ç³»ç»Ÿå¤„ç†
                            result_url = f"{mineru_base_url}/extract-results/batch/{batch_id}"
                            max_retries = 60
                            retry_interval = 5

                            for attempt in range(max_retries):
                                result_response = requests.get(result_url, headers=headers, timeout=30)

                                if result_response.status_code == 200:
                                    result_data = result_response.json()

                                    if result_data.get("code") == 0:
                                        extract_results = result_data["data"]["extract_result"]

                                        for file_result in extract_results:
                                            state = file_result["state"]

                                            if state == "done":
                                                # Step 4: ä¸‹è½½ç»“æœZIP
                                                zip_url = file_result.get("full_zip_url")
                                                if zip_url:
                                                    logger.info(f"MinerU: Extraction completed, downloading result...")

                                                    # åˆ›å»ºä¸´æ—¶ç›®å½•ä¿å­˜ç»“æœ
                                                    output_dir = Path("/home/ubuntu/github/mediwise/output/mineru")
                                                    output_dir.mkdir(parents=True, exist_ok=True)

                                                    zip_filename = f"{uuid.uuid4().hex[:8]}_{filename.replace('.pdf', '')}_result.zip"
                                                    zip_path = output_dir / zip_filename

                                                    # ä¸‹è½½ZIPæ–‡ä»¶
                                                    zip_response = requests.get(zip_url, stream=True, timeout=300)
                                                    if zip_response.status_code == 200:
                                                        with open(zip_path, 'wb') as f:
                                                            for chunk in zip_response.iter_content(chunk_size=8192):
                                                                f.write(chunk)

                                                        logger.info(f"MinerU: Result downloaded to {zip_path}")

                                                        # Step 5: è§£å‹å¹¶æå–MDå†…å®¹
                                                        extract_folder = output_dir / f"{uuid.uuid4().hex[:8]}_{filename.replace('.pdf', '')}_extracted"
                                                        extract_folder.mkdir(exist_ok=True)

                                                        try:
                                                            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                                                                zip_ref.extractall(extract_folder)

                                                            # æŸ¥æ‰¾MDæ–‡ä»¶
                                                            md_files = list(extract_folder.rglob("*.md"))
                                                            if md_files:
                                                                # è¯»å–ç¬¬ä¸€ä¸ªMDæ–‡ä»¶çš„å†…å®¹
                                                                with open(md_files[0], 'r', encoding='utf-8') as md_file:
                                                                    md_content = md_file.read()

                                                                if md_content:
                                                                    md_content = clean_text(md_content)
                                                                    logger.info(f"MinerU extracted: {md_content[:50]}...")
                                                                    result['file_content'] = md_content
                                                                    logger.info(f"MinerU extraction successful for {filename}")
                                                                    return result
                                                                else:
                                                                    logger.info(f"MinerU result is empty for {filename}")
                                                                    break  # ç©ºå†…å®¹ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                                                            else:
                                                                logger.warning(f"MinerU: No MD files found in extracted result")
                                                                break  # æ²¡æœ‰æ‰¾åˆ°MDæ–‡ä»¶ï¼Œè·³å‡ºé‡è¯•å¾ªç¯

                                                        except Exception as extract_error:
                                                            logger.error(f"MinerU: Error extracting ZIP: {extract_error}")
                                                            break  # è§£å‹å‡ºé”™ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                                                    else:
                                                        logger.error(f"MinerU: Failed to download result ZIP, status={zip_response.status_code}")
                                                        break  # ä¸‹è½½å¤±è´¥ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                                                break

                                            elif state == "failed":
                                                logger.error(f"MinerU: Extraction failed - {file_result.get('err_msg')}")
                                                break

                                            elif state in ["running", "waiting-file", "pending", "converting"]:
                                                if attempt < max_retries - 1:
                                                    logger.info(f"MinerU: Processing... state={state} [Attempt {attempt + 1}/{max_retries}]")
                                                    time.sleep(retry_interval)
                                                else:
                                                    logger.warning(f"MinerU: Timeout after {max_retries * retry_interval} seconds")
                                                    break
                                    else:
                                        logger.error(f"MinerU: API error - {result_data.get('msg')}")
                                        break
                                else:
                                    logger.error(f"MinerU: Failed to get result, status={result_response.status_code}")
                                    break
                        else:
                            logger.error(f"MinerU: File upload failed, status={upload_file_response.status_code}")
                    else:
                        logger.error(f"MinerU: Failed to get upload URL - {upload_result.get('msg')}")
                else:
                    logger.error(f"MinerU: Failed to apply upload URL, status={upload_response.status_code}")

            except Exception as mineru_error:
                logger.error(f"MinerU extraction failed for {filename}: {mineru_error}")
        else:
            logger.info(f"MINERU_TOKEN not found in environment, skipping MinerU extraction")

        # 3. å°è¯•ä½¿ç”¨ pymupdf4llm
        try:
            import pymupdf4llm
            logger.info(f"Attempting to extract {filename} using pymupdf4llm...")
            
            # ä½¿ç”¨pymupdf4llmæå–PDFå†…å®¹ä¸ºMarkdownæ ¼å¼
            md_text = pymupdf4llm.to_markdown(
                filepath,
                force_text=True,
                show_progress=False,  # ä¸æ˜¾ç¤ºè¿›åº¦æ¡
                write_images=False,   # ä¸å†™å‡ºå›¾ç‰‡
                embed_images=False    # ä¸åµŒå…¥å›¾ç‰‡
            )
            
            if md_text:
                # æ¸…ç†æ–‡æœ¬
                md_text = clean_text(md_text)
                logger.info(f"pymupdf4llm extracted: {md_text[:50]}...")  # æ‰“å°å‰ 50 ä¸ªå­—ç¬¦
                
                if md_text and not self.is_text_garbled(md_text):
                    result['file_content'] = md_text
                    return result
                logger.info(f"pymupdf4llm result is empty or garbled for {filename}")
        except ImportError:
            logger.info("pymupdf4llm not installed. Skipping this extraction method.")
        except Exception as e:
            logger.info(f"pymupdf4llm failed for {filename}: {e}")

        # 4. å°è¯•ä½¿ç”¨ PyMuPDF (fitz)
        try:
            document = fitz.open(filepath)
            content = []
            for page in document:
                content.append(clean_text(page.get_text()))
            combined_text = clean_text("".join(content))
            logger.info(f"PyMuPDF extracted: {combined_text[:50]}...")  # æ‰“å°å‰ 50 ä¸ªå­—ç¬¦
            if combined_text and not self.is_text_garbled(combined_text):
                result['file_content'] = combined_text
                return result
        except Exception as e:
            logger.info(f"PyMuPDF (fitz) failed for {filename}: {e}")

        # 5. å°è¯•ä½¿ç”¨ pdfplumber
        try:
            with pdfplumber.open(filepath) as pdf:
                content = []
                for page in pdf.pages:
                    content.append(clean_text(page.extract_text()))
            combined_text = clean_text("".join(content))
            logger.info(f"pdfplumber extracted: {combined_text[:50]}...")  # æ‰“å°å‰ 50 ä¸ªå­—ç¬¦
            if combined_text and not self.is_text_garbled(combined_text):
                result['file_content'] = combined_text
                return result
        except Exception as e:
            logger.info(f"pdfplumber failed for {filename}: {e}")

        # 6. å°è¯•ä½¿ç”¨ pypdf
        try:
            with open(filepath, 'rb') as file:
                reader = pypdf.PdfReader(file)
                content = []
                for page in reader.pages:
                    content.append(clean_text(page.extract_text()))
            combined_text = clean_text("".join(content))
            logger.info(f"pypdf extracted: {combined_text[:50]}...")  # æ‰“å°å‰ 50 ä¸ªå­—ç¬¦
            if combined_text and not self.is_text_garbled(combined_text):
                result['file_content'] = combined_text
                return result
        except Exception as e:
            logger.info(f"pypdf failed for {filename}: {e}")

        # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥æˆ–æå–çš„å†…å®¹ä¸ºä¹±ç ï¼Œè¿”å›ç©º
        logger.info(f"All extraction methods failed or text is garbled for {filename}")
        return {}

    def read_docx(self, filepath):
        try:
            filename = os.path.basename(filepath)
            result = {'file_extension': 'docx', 'file_name': filename}
            doc = docx.Document(filepath)
            full_text = []

            # Iterate through all elements in document body in order
            for element in doc.element.body:
                # Check if it's a paragraph
                if element.tag.endswith('p'):
                    para = docx.text.paragraph.Paragraph(element, doc)
                    if para.text.strip():
                        full_text.append(para.text)

                # Check if it's a table
                elif element.tag.endswith('tbl'):
                    table = docx.table.Table(element, doc)
                    for row in table.rows:
                        row_text = []
                        for cell in row.cells:
                            cell_text = cell.text.strip()
                            if cell_text:
                                row_text.append(cell_text)
                        if row_text:
                            full_text.append(' | '.join(row_text))

            result['file_content'] = '\n'.join(full_text)
            logger.info(f"result: {result}")
            return result
        except Exception as e:
            logger.error(f"""DOCXæ–‡ä»¶è¯»å–å¤±è´¥:
æ–‡ä»¶å: {filename if 'filename' in locals() else os.path.basename(filepath)}
æ–‡ä»¶è·¯å¾„: {filepath}
é”™è¯¯ç±»å‹: {type(e).__name__}
é”™è¯¯ä¿¡æ¯: {str(e)}""")
            return {'file_extension': 'docx', 'file_name': filename, 'file_content': f'Error reading {filepath}: {e}'}


    def read_ppt(self, filepath):
        """è¯»å–PPT/PPTXæ–‡ä»¶å†…å®¹"""
        try:
            filename = os.path.basename(filepath)
            file_extension = os.path.splitext(filepath)[1].lower()
            result = {'file_extension': file_extension[1:], 'file_name': filename}
            
            if not PPTX_AVAILABLE:
                logger.warning(f"python-pptxåº“ä¸å¯ç”¨ï¼Œæ— æ³•å¤„ç†PPTæ–‡ä»¶: {filename}")
                result['file_content'] = f"PPTæ–‡ä»¶: {filename} (æ— æ³•å¤„ç†ï¼Œpython-pptxåº“ä¸å¯ç”¨)"
                return result
            
            # åªæ”¯æŒ.pptxæ ¼å¼ï¼Œ.pptæ ¼å¼éœ€è¦è½¬æ¢
            if file_extension == '.ppt':
                logger.warning(f"ä¸æ”¯æŒæ—§ç‰ˆPPTæ ¼å¼(.ppt)ï¼Œè¯·è½¬æ¢ä¸º.pptxæ ¼å¼: {filename}")
                result['file_content'] = f"PPTæ–‡ä»¶: {filename} (ä¸æ”¯æŒ.pptæ ¼å¼ï¼Œè¯·è½¬æ¢ä¸º.pptxæ ¼å¼)"
                return result
            
            # è¯»å–PPTXæ–‡ä»¶
            prs = Presentation(filepath)
            full_text = []
            
            # æå–æ¯ä¸€é¡µçš„å†…å®¹
            for i, slide in enumerate(prs.slides, 1):
                slide_content = []
                slide_content.append(f"\n=== ç¬¬ {i} é¡µ ===")
                
                # æå–å¹»ç¯ç‰‡ä¸­çš„æ‰€æœ‰æ–‡æœ¬
                for shape in slide.shapes:
                    if hasattr(shape, "text") and shape.text.strip():
                        slide_content.append(shape.text.strip())
                    
                    # å¤„ç†è¡¨æ ¼
                    if shape.shape_type == 19:  # MSO_SHAPE_TYPE.TABLE
                        try:
                            table = shape.table
                            table_content = []
                            for row in table.rows:
                                row_content = []
                                for cell in row.cells:
                                    if cell.text.strip():
                                        row_content.append(cell.text.strip())
                                if row_content:
                                    table_content.append(" | ".join(row_content))
                            if table_content:
                                slide_content.append("è¡¨æ ¼å†…å®¹:")
                                slide_content.extend(table_content)
                        except Exception as e:
                            logger.debug(f"å¤„ç†è¡¨æ ¼æ—¶å‡ºé”™: {e}")
                
                # å¤„ç†å¤‡æ³¨
                if slide.notes_slide and slide.notes_slide.notes_text_frame:
                    notes_text = slide.notes_slide.notes_text_frame.text.strip()
                    if notes_text:
                        slide_content.append(f"å¤‡æ³¨: {notes_text}")
                
                if len(slide_content) > 1:  # é™¤äº†é¡µç æ ‡é¢˜å¤–è¿˜æœ‰å…¶ä»–å†…å®¹
                    full_text.extend(slide_content)
            
            # è¿‡æ»¤ç©ºå†…å®¹
            full_text = [item for item in full_text if item.strip()]
            
            if full_text:
                result['file_content'] = '\n'.join(full_text)
                logger.info(f"æˆåŠŸæå–PPTå†…å®¹: {filename}, å…± {len(prs.slides)} é¡µ")
            else:
                result['file_content'] = f"PPTæ–‡ä»¶: {filename} (æœªæå–åˆ°æ–‡æœ¬å†…å®¹)"
                logger.warning(f"PPTæ–‡ä»¶æœªæå–åˆ°å†…å®¹: {filename}")
            
            return result
            
        except Exception as e:
            logger.error(f"""PPTæ–‡ä»¶è¯»å–å¤±è´¥:
æ–‡ä»¶å: {filename if 'filename' in locals() else os.path.basename(filepath)}
æ–‡ä»¶è·¯å¾„: {filepath}
é”™è¯¯ç±»å‹: {type(e).__name__}
é”™è¯¯ä¿¡æ¯: {str(e)}""")
            filename = os.path.basename(filepath) if 'filename' not in locals() else filename
            file_extension = os.path.splitext(filepath)[1].lower()[1:] if 'file_extension' not in locals() else file_extension[1:]
            return {
                'file_extension': file_extension,
                'file_name': filename,
                'file_content': f'PPTæ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}'
            }

    def read_txt(self, filepath):
        try:
            filename = os.path.basename(filepath)
            file_extension = os.path.splitext(filename)[1].lower()
            result = {'file_extension': file_extension, 'file_name': filename}

            with open(filepath, 'r', encoding='utf-8') as file:
                file_content = file.read()
            result['file_content'] = file_content
            return result
        except Exception as e:
            logger.error(f"""TXTæ–‡ä»¶è¯»å–å¤±è´¥:
æ–‡ä»¶å: {filename if 'filename' in locals() else os.path.basename(filepath)}
æ–‡ä»¶è·¯å¾„: {filepath}
é”™è¯¯ç±»å‹: {type(e).__name__}
é”™è¯¯ä¿¡æ¯: {str(e)}""")
            return {'file_extension': '', 'file_name': '', 'file_content': ''}
