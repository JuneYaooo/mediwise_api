"""
è¾“å‡ºåˆ†å—ç”Ÿæˆå™¨ - å¤„ç†è¾“å‡ºé•¿åº¦é™åˆ¶å°çš„æ¨¡å‹

åŠŸèƒ½ï¼š
1. å°†PPTç”Ÿæˆä»»åŠ¡æ‹†åˆ†æˆå¤šä¸ªå­ä»»åŠ¡
2. æ¯ä¸ªå­ä»»åŠ¡ç”ŸæˆPPTçš„ä¸€éƒ¨åˆ†
3. åˆå¹¶æ‰€æœ‰éƒ¨åˆ†ç”Ÿæˆå®Œæ•´PPT
"""

import os
import json
from typing import Dict, List, Any, Optional

# å°è¯•å¯¼å…¥dotenv
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass


class OutputChunkedGenerator:
    """è¾“å‡ºåˆ†å—ç”Ÿæˆå™¨ - å¤„ç†è¾“å‡ºé•¿åº¦é™åˆ¶"""

    # PPTæ•°æ®ç»“æ„åˆ†å—ç­–ç•¥
    PPT_CHUNKS = {
        'basic_info': {
            'name': 'åŸºæœ¬ä¿¡æ¯',
            'fields': ['title', 'patient', 'diag'],
            'priority': 1,
            'estimated_tokens': 500
        },
        'treatments': {
            'name': 'æ²»ç–—ä¿¡æ¯',
            'fields': ['treatments', 'medications'],
            'priority': 2,
            'estimated_tokens': 2000
        },
        'examinations': {
            'name': 'æ£€æŸ¥ä¿¡æ¯',
            'fields': ['examinations', 'lab_tests'],
            'priority': 3,
            'estimated_tokens': 2000
        },
        'images': {
            'name': 'å½±åƒèµ„æ–™',
            'fields': ['images', 'medical_images'],
            'priority': 4,
            'estimated_tokens': 1500
        },
        'timeline': {
            'name': 'æ—¶é—´è½´',
            'fields': ['timeline', 'events'],
            'priority': 5,
            'estimated_tokens': 1000
        },
        'charts': {
            'name': 'å›¾è¡¨æ•°æ®',
            'fields': ['indicators', 'gantt', 'charts'],
            'priority': 6,
            'estimated_tokens': 1000
        }
    }

    def __init__(self, logger=None, token_manager=None):
        """åˆå§‹åŒ–è¾“å‡ºåˆ†å—ç”Ÿæˆå™¨

        Args:
            logger: æ—¥å¿—è®°å½•å™¨
            token_manager: Tokenç®¡ç†å™¨
        """
        self.logger = logger
        self.token_manager = token_manager

    def should_use_chunked_output(self, model_name: str, expected_output_size: int = None) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ä½¿ç”¨åˆ†å—è¾“å‡º

        Args:
            model_name: æ¨¡å‹åç§°
            expected_output_size: é¢„æœŸè¾“å‡ºå¤§å°ï¼ˆå¯é€‰ï¼‰

        Returns:
            bool: æ˜¯å¦éœ€è¦åˆ†å—è¾“å‡º
        """
        if not self.token_manager:
            return False

        # è·å–æ¨¡å‹é…ç½®
        config = self.token_manager.get_model_config(model_name)
        max_output_tokens = config['max_output_tokens']
        safe_output_limit = int(max_output_tokens * config['safe_output_ratio'])

        # å¦‚æœæ²¡æœ‰æä¾›é¢„æœŸè¾“å‡ºå¤§å°ï¼Œä¼°ç®—ä¸€ä¸ª
        if expected_output_size is None:
            # ä¼°ç®—ï¼šå®Œæ•´PPTæ•°æ®é€šå¸¸éœ€è¦8000-15000 tokens
            expected_output_size = 10000

        # å¦‚æœé¢„æœŸè¾“å‡ºè¶…è¿‡å®‰å…¨é™åˆ¶çš„80%ï¼Œå»ºè®®åˆ†å—
        needs_chunking = expected_output_size > safe_output_limit * 0.8

        if self.logger and needs_chunking:
            self.logger.warning(
                f"âš ï¸ é¢„æœŸè¾“å‡º ({expected_output_size} tokens) æ¥è¿‘æˆ–è¶…è¿‡æ¨¡å‹é™åˆ¶ ({max_output_tokens} tokens)ï¼Œ"
                f"å»ºè®®ä½¿ç”¨åˆ†å—ç”Ÿæˆ"
            )

        return needs_chunking

    def generate_ppt_in_chunks(self, llm, patient_data: Dict[str, Any],
                               template_info: Dict[str, Any],
                               model_name: str = 'gemini-3-flash-preview') -> Dict[str, Any]:
        """åˆ†å—ç”ŸæˆPPTæ•°æ®

        Args:
            llm: LLMå¯¹è±¡
            patient_data: æ‚£è€…æ•°æ®
            template_info: æ¨¡æ¿ä¿¡æ¯
            model_name: æ¨¡å‹åç§°

        Returns:
            dict: å®Œæ•´çš„PPTæ•°æ®
        """
        if self.logger:
            self.logger.info("=" * 100)
            self.logger.info("ğŸ”€ å¯åŠ¨åˆ†å—ç”Ÿæˆæ¨¡å¼")
            self.logger.info("=" * 100)

        # è·å–æ¨¡å‹é…ç½®
        config = self.token_manager.get_model_config(model_name) if self.token_manager else {}
        max_output_tokens = config.get('max_output_tokens', 4096)

        # æŒ‰ä¼˜å…ˆçº§æ’åºåˆ†å—
        sorted_chunks = sorted(
            self.PPT_CHUNKS.items(),
            key=lambda x: x[1]['priority']
        )

        # å­˜å‚¨æ¯ä¸ªåˆ†å—çš„ç»“æœ
        chunk_results = {}

        # é€ä¸ªç”Ÿæˆåˆ†å—
        for chunk_id, chunk_config in sorted_chunks:
            if self.logger:
                self.logger.info(f"\nğŸ“¦ ç”Ÿæˆåˆ†å— {chunk_config['priority']}/{len(sorted_chunks)}: {chunk_config['name']}")
                self.logger.info(f"  â”œâ”€ åŒ…å«å­—æ®µ: {chunk_config['fields']}")
                self.logger.info(f"  â””â”€ é¢„ä¼°tokens: {chunk_config['estimated_tokens']}")

            # ç”Ÿæˆè¯¥åˆ†å—çš„æ•°æ®
            chunk_data = self._generate_chunk(
                llm=llm,
                chunk_id=chunk_id,
                chunk_config=chunk_config,
                patient_data=patient_data,
                template_info=template_info,
                max_output_tokens=max_output_tokens
            )

            if chunk_data:
                chunk_results[chunk_id] = chunk_data
                if self.logger:
                    self.logger.info(f"  âœ… åˆ†å—ç”ŸæˆæˆåŠŸ")
            else:
                if self.logger:
                    self.logger.warning(f"  âš ï¸ åˆ†å—ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡")

        # åˆå¹¶æ‰€æœ‰åˆ†å—
        if self.logger:
            self.logger.info("\nğŸ”— å¼€å§‹åˆå¹¶æ‰€æœ‰åˆ†å—...")

        merged_ppt_data = self._merge_chunks(chunk_results)

        if self.logger:
            self.logger.info("=" * 100)
            self.logger.info(f"âœ… åˆ†å—ç”Ÿæˆå®Œæˆï¼å…±ç”Ÿæˆ {len(chunk_results)} ä¸ªåˆ†å—")
            self.logger.info("=" * 100)

        return merged_ppt_data

    def _generate_chunk(self, llm, chunk_id: str, chunk_config: Dict[str, Any],
                       patient_data: Dict[str, Any], template_info: Dict[str, Any],
                       max_output_tokens: int) -> Optional[Dict[str, Any]]:
        """ç”Ÿæˆå•ä¸ªåˆ†å—

        Args:
            llm: LLMå¯¹è±¡
            chunk_id: åˆ†å—ID
            chunk_config: åˆ†å—é…ç½®
            patient_data: æ‚£è€…æ•°æ®
            template_info: æ¨¡æ¿ä¿¡æ¯
            max_output_tokens: æœ€å¤§è¾“å‡ºtokens

        Returns:
            dict: åˆ†å—æ•°æ®
        """
        # æ„å»ºé’ˆå¯¹è¯¥åˆ†å—çš„æç¤ºè¯
        prompt = self._build_chunk_prompt(
            chunk_id=chunk_id,
            chunk_config=chunk_config,
            patient_data=patient_data,
            template_info=template_info
        )

        try:
            # è°ƒç”¨LLM
            if hasattr(llm, 'call'):
                response = llm.call(prompt)
                response_text = str(response)
            else:
                response = llm.invoke(prompt)
                response_text = response.content if hasattr(response, 'content') else str(response)

            # è§£æJSON
            from src.utils.json_utils import JsonUtils
            chunk_data = JsonUtils.safe_parse_json(response_text, debug_prefix=f"åˆ†å—{chunk_id}")

            return chunk_data

        except Exception as e:
            if self.logger:
                self.logger.error(f"ç”Ÿæˆåˆ†å— {chunk_id} æ—¶å‡ºé”™: {e}")
            return None

    def _build_chunk_prompt(self, chunk_id: str, chunk_config: Dict[str, Any],
                           patient_data: Dict[str, Any], template_info: Dict[str, Any]) -> str:
        """æ„å»ºåˆ†å—ç”Ÿæˆçš„æç¤ºè¯

        Args:
            chunk_id: åˆ†å—ID
            chunk_config: åˆ†å—é…ç½®
            patient_data: æ‚£è€…æ•°æ®
            template_info: æ¨¡æ¿ä¿¡æ¯

        Returns:
            str: æç¤ºè¯
        """
        fields = chunk_config['fields']
        chunk_name = chunk_config['name']

        # ä»æ¨¡æ¿ä¸­æå–è¯¥åˆ†å—ç›¸å…³çš„å­—æ®µè¯´æ˜
        template_json_str = template_info.get('template_json', '{}')

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªåŒ»ç–—æ•°æ®è½¬æ¢ä¸“å®¶ã€‚ç°åœ¨éœ€è¦ç”ŸæˆPPTçš„ã€{chunk_name}ã€‘éƒ¨åˆ†ã€‚

**ä»»åŠ¡**: åªç”Ÿæˆä»¥ä¸‹å­—æ®µçš„æ•°æ®ï¼š{', '.join(fields)}

**æ¨¡æ¿è¯´æ˜**ï¼ˆå®Œæ•´æ¨¡æ¿ï¼Œä½†ä½ åªéœ€è¦ç”Ÿæˆä¸Šè¿°å­—æ®µï¼‰:
{template_json_str}

**æ‚£è€…æ•°æ®**:
{json.dumps(patient_data, ensure_ascii=False, indent=2)}

**é‡è¦è¦æ±‚**:
1. åªç”Ÿæˆ {', '.join(fields)} è¿™äº›å­—æ®µ
2. ä¸¥æ ¼æŒ‰ç…§æ¨¡æ¿ç»“æ„è¾“å‡º
3. åªä½¿ç”¨æ‚£è€…æ•°æ®ä¸­çœŸå®å­˜åœ¨çš„ä¿¡æ¯
4. ç›´æ¥è¾“å‡ºJSONæ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ–‡å­—
5. ä¸è¦åŒ…å«Markdownä»£ç å—æ ‡è®°ï¼ˆå¦‚```jsonï¼‰

è¯·è¾“å‡ºJSONæ•°æ®:"""

        return prompt

    def _merge_chunks(self, chunk_results: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """åˆå¹¶æ‰€æœ‰åˆ†å—

        Args:
            chunk_results: åˆ†å—ç»“æœå­—å…¸

        Returns:
            dict: åˆå¹¶åçš„å®Œæ•´PPTæ•°æ®
        """
        merged = {
            'pptTemplate2Vm': {}
        }

        # æŒ‰ä¼˜å…ˆçº§é¡ºåºåˆå¹¶
        for chunk_id in sorted(chunk_results.keys(),
                              key=lambda x: self.PPT_CHUNKS[x]['priority']):
            chunk_data = chunk_results[chunk_id]

            # å¦‚æœåˆ†å—æ•°æ®æœ‰pptTemplate2VmåŒ…è£…ï¼Œè§£åŒ…
            if 'pptTemplate2Vm' in chunk_data:
                chunk_data = chunk_data['pptTemplate2Vm']

            # åˆå¹¶åˆ°æ€»æ•°æ®ä¸­
            for key, value in chunk_data.items():
                if key not in merged['pptTemplate2Vm']:
                    merged['pptTemplate2Vm'][key] = value
                elif isinstance(value, list) and isinstance(merged['pptTemplate2Vm'][key], list):
                    # åˆ—è¡¨ç±»å‹ï¼šåˆå¹¶
                    merged['pptTemplate2Vm'][key].extend(value)
                elif isinstance(value, dict) and isinstance(merged['pptTemplate2Vm'][key], dict):
                    # å­—å…¸ç±»å‹ï¼šæ›´æ–°
                    merged['pptTemplate2Vm'][key].update(value)
                else:
                    # å…¶ä»–ç±»å‹ï¼šè¦†ç›–
                    merged['pptTemplate2Vm'][key] = value

        return merged

    def estimate_output_size(self, patient_data: Dict[str, Any]) -> int:
        """ä¼°ç®—è¾“å‡ºå¤§å°

        Args:
            patient_data: æ‚£è€…æ•°æ®

        Returns:
            int: ä¼°ç®—çš„è¾“å‡ºtokensæ•°
        """
        # ç®€å•ä¼°ç®—ï¼šåŸºäºè¾“å…¥æ•°æ®é‡
        if not self.token_manager:
            # å¦‚æœæ²¡æœ‰token_managerï¼Œä½¿ç”¨ç®€å•ä¼°ç®—
            input_size = len(json.dumps(patient_data, ensure_ascii=False))
            # å‡è®¾è¾“å‡ºæ˜¯è¾“å…¥çš„1.5å€
            return int(input_size / 2 * 1.5)

        # ä½¿ç”¨token_managerä¼°ç®—
        input_tokens = self.token_manager.estimate_tokens(patient_data)

        # ä¼°ç®—è¾“å‡ºtokensï¼ˆé€šå¸¸æ˜¯è¾“å…¥çš„0.8-1.2å€ï¼‰
        estimated_output = int(input_tokens * 1.0)

        return estimated_output

    def get_chunk_strategy(self, model_name: str) -> str:
        """è·å–åˆ†å—ç­–ç•¥å»ºè®®

        Args:
            model_name: æ¨¡å‹åç§°

        Returns:
            str: ç­–ç•¥å»ºè®®
        """
        if not self.token_manager:
            return "æ— æ³•è·å–ç­–ç•¥å»ºè®®ï¼ˆç¼ºå°‘token_managerï¼‰"

        config = self.token_manager.get_model_config(model_name)
        max_output = config['max_output_tokens']

        if max_output >= 32000:
            return "large_output"  # å¤§è¾“å‡ºæ¨¡å‹ï¼Œé€šå¸¸ä¸éœ€è¦åˆ†å—
        elif max_output >= 8000:
            return "medium_output"  # ä¸­ç­‰è¾“å‡ºï¼Œå¯èƒ½éœ€è¦åˆ†å—
        else:
            return "small_output"  # å°è¾“å‡ºæ¨¡å‹ï¼Œå¼ºçƒˆå»ºè®®åˆ†å—
