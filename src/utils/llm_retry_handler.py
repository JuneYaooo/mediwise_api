"""
LLMé‡è¯•å¤„ç†å™¨ - å¤„ç†tokenè¶…é™é”™è¯¯å’Œè‡ªåŠ¨é‡è¯•

åŠŸèƒ½ï¼š
1. å¸¦è‡ªåŠ¨å‹ç¼©çš„LLMè°ƒç”¨
2. å¤„ç†tokenè¶…é™é”™è¯¯
3. è‡ªåŠ¨é‡è¯•æœºåˆ¶
4. å¤„ç†è¾“å‡ºæˆªæ–­é—®é¢˜
"""

import os
import json
import time
from typing import Dict, Any, Callable, Optional

# å°è¯•å¯¼å…¥dotenvï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è·³è¿‡
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass


class TokenLimitError(Exception):
    """Tokené™åˆ¶é”™è¯¯"""
    pass


class LLMRetryHandler:
    """LLMè°ƒç”¨é‡è¯•å¤„ç†å™¨ - å¤„ç†tokenè¶…é™é”™è¯¯"""

    def __init__(self, logger=None, token_manager=None, data_compressor=None):
        """åˆå§‹åŒ–é‡è¯•å¤„ç†å™¨

        Args:
            logger: æ—¥å¿—è®°å½•å™¨ï¼ˆå¯é€‰ï¼‰
            token_manager: Tokenç®¡ç†å™¨ï¼ˆå¯é€‰ï¼‰
            data_compressor: æ•°æ®å‹ç¼©å™¨ï¼ˆå¯é€‰ï¼‰
        """
        self.logger = logger
        self.token_manager = token_manager
        self.data_compressor = data_compressor

    def call_with_auto_compression(self, llm, prompt: str, data: Dict[str, Any],
                                   model_name: str = 'gemini-3-flash-preview',
                                   max_retries: int = 3) -> Any:
        """å¸¦è‡ªåŠ¨å‹ç¼©çš„LLMè°ƒç”¨

        æµç¨‹ï¼š
        1. é¦–æ¬¡å°è¯•å®Œæ•´æ•°æ®
        2. å¦‚æœtokenè¶…é™ï¼Œè‡ªåŠ¨å‹ç¼©30%åé‡è¯•
        3. å¦‚æœä»è¶…é™ï¼Œå‹ç¼©50%åé‡è¯•
        4. å¦‚æœä»è¶…é™ï¼ŒæŠ›å‡ºå¼‚å¸¸

        Args:
            llm: LLMå¯¹è±¡
            prompt: æç¤ºè¯
            data: æ•°æ®
            model_name: æ¨¡å‹åç§°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

        Returns:
            LLMå“åº”

        Raises:
            TokenLimitError: Tokenè¶…é™é”™è¯¯
        """
        compression_ratios = [1.0, 0.7, 0.5, 0.3]  # å‹ç¼©æ¯”ä¾‹åºåˆ—

        last_error = None

        for retry_count in range(max_retries):
            try:
                # è·å–å½“å‰å‹ç¼©æ¯”ä¾‹
                compression_ratio = compression_ratios[min(retry_count, len(compression_ratios) - 1)]

                # å¦‚æœéœ€è¦å‹ç¼©
                if compression_ratio < 1.0 and self.token_manager and self.data_compressor:
                    if self.logger:
                        self.logger.info(f"ğŸ”„ é‡è¯• {retry_count + 1}/{max_retries}: ä½¿ç”¨å‹ç¼©æ¯”ä¾‹ {compression_ratio:.1%}")

                    # è®¡ç®—ç›®æ ‡tokenæ•°
                    config = self.token_manager.get_model_config(model_name)
                    target_tokens = int(config['max_input_tokens'] * config['safe_input_ratio'] * compression_ratio)

                    # å‹ç¼©æ•°æ®
                    compressed_data = self.data_compressor.compress_data(data, target_tokens)

                    # é‡æ–°æ„å»ºpromptï¼ˆä½¿ç”¨å‹ç¼©åçš„æ•°æ®ï¼‰
                    # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾promptä¸­åŒ…å«dataçš„JSONè¡¨ç¤º
                    # å®é™…ä½¿ç”¨æ—¶å¯èƒ½éœ€è¦æ ¹æ®å…·ä½“æƒ…å†µè°ƒæ•´
                    current_data = compressed_data
                else:
                    current_data = data

                # æ£€æŸ¥tokené™åˆ¶
                if self.token_manager:
                    # ä¼°ç®—prompt + dataçš„æ€»tokenæ•°
                    total_input = prompt + json.dumps(current_data, ensure_ascii=False)
                    check_result = self.token_manager.check_input_limit(total_input, model_name)

                    if not check_result['within_limit']:
                        if self.logger:
                            self.logger.error(f"âŒ Tokenè¶…é™: {check_result['total_tokens']} > {check_result['limit']}")
                        raise TokenLimitError(f"è¾“å…¥è¶…è¿‡æ¨¡å‹é™åˆ¶: {check_result['total_tokens']} tokens")

                # è°ƒç”¨LLM
                if self.logger:
                    self.logger.info(f"ğŸ“¤ è°ƒç”¨LLM (å°è¯• {retry_count + 1}/{max_retries})...")

                response = self._call_llm(llm, prompt)

                if self.logger:
                    self.logger.info(f"âœ… LLMè°ƒç”¨æˆåŠŸ")

                return response

            except TokenLimitError as e:
                last_error = e
                if self.logger:
                    self.logger.warning(f"âš ï¸ Tokenè¶…é™ï¼Œå‡†å¤‡é‡è¯•: {e}")

                # å¦‚æœå·²ç»æ˜¯æœ€åä¸€æ¬¡é‡è¯•ï¼ŒæŠ›å‡ºå¼‚å¸¸
                if retry_count >= max_retries - 1:
                    raise

                # ç­‰å¾…ä¸€å°æ®µæ—¶é—´åé‡è¯•
                time.sleep(1)

            except Exception as e:
                # æ£€æŸ¥æ˜¯å¦æ˜¯tokenç›¸å…³é”™è¯¯
                error_msg = str(e).lower()
                if any(keyword in error_msg for keyword in ['token', 'length', 'limit', 'too long', 'context']):
                    last_error = TokenLimitError(f"LLMè°ƒç”¨å¤±è´¥ï¼ˆå¯èƒ½æ˜¯tokenè¶…é™ï¼‰: {e}")
                    if self.logger:
                        self.logger.warning(f"âš ï¸ æ£€æµ‹åˆ°tokenç›¸å…³é”™è¯¯ï¼Œå‡†å¤‡é‡è¯•: {e}")

                    # å¦‚æœå·²ç»æ˜¯æœ€åä¸€æ¬¡é‡è¯•ï¼ŒæŠ›å‡ºå¼‚å¸¸
                    if retry_count >= max_retries - 1:
                        raise last_error

                    # ç­‰å¾…ä¸€å°æ®µæ—¶é—´åé‡è¯•
                    time.sleep(1)
                else:
                    # étokenç›¸å…³é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                    raise

        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºæœ€åä¸€ä¸ªé”™è¯¯
        if last_error:
            raise last_error
        else:
            raise Exception("LLMè°ƒç”¨å¤±è´¥ï¼ŒåŸå› æœªçŸ¥")

    def _call_llm(self, llm, prompt: str) -> Any:
        """è°ƒç”¨LLM

        Args:
            llm: LLMå¯¹è±¡
            prompt: æç¤ºè¯

        Returns:
            LLMå“åº”
        """
        # å°è¯•ä¸åŒçš„è°ƒç”¨æ–¹å¼
        try:
            # CrewAI LLM å¯¹è±¡ç›´æ¥è°ƒç”¨
            response = llm.call(prompt)
            return str(response)
        except AttributeError:
            # å¦‚æœæ˜¯ LangChain LLMï¼Œä½¿ç”¨ invoke
            try:
                response = llm.invoke(prompt)
                return response.content if hasattr(response, 'content') else str(response)
            except Exception as e:
                if self.logger:
                    self.logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
                raise

    def is_output_complete(self, response: Any) -> bool:
        """æ£€æŸ¥è¾“å‡ºæ˜¯å¦å®Œæ•´

        Args:
            response: LLMå“åº”

        Returns:
            bool: æ˜¯å¦å®Œæ•´
        """
        if not response:
            return False

        response_text = str(response)

        # æ£€æŸ¥JSONæ˜¯å¦å®Œæ•´
        try:
            # å°è¯•æå–JSON
            from src.utils.json_utils import JsonUtils
            json_str = JsonUtils.extract_json_from_text(response_text)
            if json_str:
                parsed = json.loads(json_str)
                # æ£€æŸ¥æ˜¯å¦æœ‰pptTemplate2Vmå­—æ®µ
                if 'pptTemplate2Vm' in parsed or 'template_json' in parsed:
                    return True
        except Exception:
            pass

        # æ£€æŸ¥æ˜¯å¦æœ‰æˆªæ–­æ ‡è®°
        truncation_markers = [
            '...',
            '[truncated]',
            '[çœç•¥]',
            'ï¼ˆçœç•¥ï¼‰',
            'output truncated',
            'response truncated'
        ]

        for marker in truncation_markers:
            if marker in response_text.lower():
                if self.logger:
                    self.logger.warning(f"âš ï¸ æ£€æµ‹åˆ°è¾“å‡ºæˆªæ–­æ ‡è®°: {marker}")
                return False

        return True

    def complete_truncated_output(self, partial_response: Any, llm, context: Dict[str, Any]) -> Any:
        """è¡¥å…¨è¢«æˆªæ–­çš„è¾“å‡º

        Args:
            partial_response: éƒ¨åˆ†å“åº”
            llm: LLMå¯¹è±¡
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯

        Returns:
            å®Œæ•´å“åº”
        """
        if self.logger:
            self.logger.info("ğŸ”§ å°è¯•è¡¥å…¨è¢«æˆªæ–­çš„è¾“å‡º...")

        # æ„å»ºè¡¥å…¨æç¤ºè¯
        prompt = f"""ä¹‹å‰çš„è¾“å‡ºè¢«æˆªæ–­äº†ï¼Œè¯·ç»§ç»­å®Œæˆå‰©ä½™éƒ¨åˆ†ã€‚

å·²æœ‰çš„éƒ¨åˆ†è¾“å‡ºï¼š
{str(partial_response)[-1000:]}  # åªå–æœ€å1000å­—ç¬¦

è¯·ç»§ç»­è¾“å‡ºå‰©ä½™çš„JSONæ•°æ®ï¼Œç¡®ä¿è¾“å‡ºå®Œæ•´çš„JSONç»“æ„ã€‚
åªè¾“å‡ºJSONï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ–‡å­—ã€‚"""

        try:
            # è°ƒç”¨LLMè¡¥å…¨
            completion = self._call_llm(llm, prompt)

            # å°è¯•åˆå¹¶
            merged = self._merge_responses(partial_response, completion)

            if self.logger:
                self.logger.info("âœ… è¾“å‡ºè¡¥å…¨æˆåŠŸ")

            return merged

        except Exception as e:
            if self.logger:
                self.logger.error(f"âŒ è¾“å‡ºè¡¥å…¨å¤±è´¥: {e}")
            # è¿”å›åŸå§‹éƒ¨åˆ†å“åº”
            return partial_response

    def _merge_responses(self, partial: Any, completion: Any) -> Any:
        """åˆå¹¶éƒ¨åˆ†å“åº”å’Œè¡¥å…¨å“åº”

        Args:
            partial: éƒ¨åˆ†å“åº”
            completion: è¡¥å…¨å“åº”

        Returns:
            åˆå¹¶åçš„å“åº”
        """
        # ç®€å•ç­–ç•¥ï¼šæ‹¼æ¥å­—ç¬¦ä¸²
        partial_str = str(partial)
        completion_str = str(completion)

        # å¦‚æœpartialä»¥ä¸å®Œæ•´çš„JSONç»“æŸï¼Œå°è¯•æ™ºèƒ½åˆå¹¶
        if partial_str.rstrip().endswith(',') or partial_str.rstrip().endswith('{') or partial_str.rstrip().endswith('['):
            merged = partial_str + completion_str
        else:
            merged = partial_str + '\n' + completion_str

        return merged
