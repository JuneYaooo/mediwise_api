"""
é€šç”¨JSONåˆ†å—ç”Ÿæˆå™¨ - æ”¯æŒä»»æ„JSONç»“æ„çš„åˆ†å—ç”Ÿæˆ

é€‚ç”¨åœºæ™¯ï¼š
1. PPTç”Ÿæˆï¼ˆpptTemplate2Vmï¼‰
2. æ‚£è€…ä¿¡æ¯ç»“æ„åŒ–ï¼ˆpatient_structured_dataï¼‰
3. ä»»ä½•éœ€è¦å¤§é‡è¾“å‡ºçš„JSONç”Ÿæˆä»»åŠ¡
"""

import os
import json
from typing import Dict, List, Any, Optional, Tuple

try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass


class UniversalChunkedGenerator:
    """é€šç”¨JSONåˆ†å—ç”Ÿæˆå™¨"""

    # é¢„å®šä¹‰çš„åˆ†å—é…ç½®
    CHUNK_CONFIGS = {
        # PPTç”Ÿæˆçš„åˆ†å—é…ç½®
        'ppt_generation': {
            'root_key': 'pptTemplate2Vm',
            'chunks': [
                {
                    'name': 'åŸºæœ¬ä¿¡æ¯',
                    'fields': ['title', 'patient', 'diag'],
                    'priority': 1,
                    'max_tokens': 1000
                },
                {
                    'name': 'æ²»ç–—ä¿¡æ¯',
                    'fields': ['treatments', 'medications', 'surgeries'],
                    'priority': 2,
                    'max_tokens': 3000
                },
                {
                    'name': 'æ£€æŸ¥ä¿¡æ¯',
                    'fields': ['examinations', 'lab_tests', 'vital_signs'],
                    'priority': 3,
                    'max_tokens': 3000
                },
                {
                    'name': 'å½±åƒèµ„æ–™',
                    'fields': ['images', 'medical_images', 'scans'],
                    'priority': 4,
                    'max_tokens': 2000
                },
                {
                    'name': 'æ—¶é—´è½´å’Œå›¾è¡¨',
                    'fields': ['timeline', 'events', 'indicators', 'gantt', 'charts'],
                    'priority': 5,
                    'max_tokens': 2000
                }
            ]
        },

        # æ‚£è€…ä¿¡æ¯ç»“æ„åŒ–çš„åˆ†å—é…ç½®
        'patient_structuring': {
            'root_key': 'patient_structured_data',
            'chunks': [
                {
                    'name': 'åŸºæœ¬ä¿¡æ¯',
                    'fields': ['patient_info', 'demographics', 'contact'],
                    'priority': 1,
                    'max_tokens': 500
                },
                {
                    'name': 'è¯Šæ–­ä¿¡æ¯',
                    'fields': ['diagnoses', 'chief_complaint', 'present_illness'],
                    'priority': 2,
                    'max_tokens': 2000
                },
                {
                    'name': 'ç”¨è¯ä¿¡æ¯',
                    'fields': ['medications', 'allergies', 'adverse_reactions'],
                    'priority': 3,
                    'max_tokens': 2000
                },
                {
                    'name': 'æ£€æŸ¥æ£€éªŒ',
                    'fields': ['lab_tests', 'examinations', 'imaging_studies'],
                    'priority': 4,
                    'max_tokens': 3000
                },
                {
                    'name': 'æ²»ç–—è®°å½•',
                    'fields': ['treatments', 'procedures', 'surgeries'],
                    'priority': 5,
                    'max_tokens': 3000
                },
                {
                    'name': 'ç—…å²å’Œéšè®¿',
                    'fields': ['medical_history', 'family_history', 'follow_ups'],
                    'priority': 6,
                    'max_tokens': 2000
                }
            ]
        }
    }

    def __init__(self, logger=None, token_manager=None):
        """åˆå§‹åŒ–é€šç”¨åˆ†å—ç”Ÿæˆå™¨

        Args:
            logger: æ—¥å¿—è®°å½•å™¨
            token_manager: Tokenç®¡ç†å™¨
        """
        self.logger = logger
        self.token_manager = token_manager

    def should_use_chunking(self, task_type: str, model_name: str,
                           expected_output_size: int = None) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ä½¿ç”¨åˆ†å—ç”Ÿæˆ

        Args:
            task_type: ä»»åŠ¡ç±»å‹ï¼ˆ'ppt_generation' æˆ– 'patient_structuring'ï¼‰
            model_name: æ¨¡å‹åç§°
            expected_output_size: é¢„æœŸè¾“å‡ºå¤§å°ï¼ˆå¯é€‰ï¼‰

        Returns:
            bool: æ˜¯å¦éœ€è¦åˆ†å—
        """
        if not self.token_manager:
            return False

        # è·å–æ¨¡å‹é…ç½®
        config = self.token_manager.get_model_config(model_name)
        max_output_tokens = config['max_output_tokens']
        safe_output_limit = int(max_output_tokens * config['safe_output_ratio'])

        # å¦‚æœæ²¡æœ‰æä¾›é¢„æœŸè¾“å‡ºå¤§å°ï¼Œæ ¹æ®ä»»åŠ¡ç±»å‹ä¼°ç®—
        if expected_output_size is None:
            if task_type == 'ppt_generation':
                expected_output_size = 10000  # PPTé€šå¸¸éœ€è¦10K tokens
            elif task_type == 'patient_structuring':
                expected_output_size = 8000   # æ‚£è€…ç»“æ„åŒ–é€šå¸¸éœ€è¦8K tokens
            else:
                expected_output_size = 5000   # é»˜è®¤5K tokens

        # å¦‚æœé¢„æœŸè¾“å‡ºè¶…è¿‡å®‰å…¨é™åˆ¶çš„80%ï¼Œå»ºè®®åˆ†å—
        needs_chunking = expected_output_size > safe_output_limit * 0.8

        if self.logger and needs_chunking:
            self.logger.warning(
                f"âš ï¸ ä»»åŠ¡ [{task_type}] é¢„æœŸè¾“å‡º ({expected_output_size} tokens) "
                f"æ¥è¿‘æˆ–è¶…è¿‡æ¨¡å‹é™åˆ¶ ({max_output_tokens} tokens)ï¼Œå»ºè®®ä½¿ç”¨åˆ†å—ç”Ÿæˆ"
            )

        return needs_chunking

    def generate_in_chunks(self, llm, task_type: str, input_data: Dict[str, Any],
                          template_or_schema: str, model_name: str = 'gemini-3-flash-preview',
                          custom_chunks: List[Dict] = None) -> Dict[str, Any]:
        """åˆ†å—ç”ŸæˆJSONæ•°æ®ï¼ˆå¸¦ä¸Šä¸‹æ–‡ä¼ é€’ï¼‰

        Args:
            llm: LLMå¯¹è±¡
            task_type: ä»»åŠ¡ç±»å‹ï¼ˆ'ppt_generation' æˆ– 'patient_structuring'ï¼‰
            input_data: è¾“å…¥æ•°æ®ï¼ˆæ‚£è€…æ•°æ®ç­‰ï¼‰
            template_or_schema: æ¨¡æ¿æˆ–Schemaè¯´æ˜
            model_name: æ¨¡å‹åç§°
            custom_chunks: è‡ªå®šä¹‰åˆ†å—é…ç½®ï¼ˆå¯é€‰ï¼‰

        Returns:
            dict: å®Œæ•´çš„JSONæ•°æ®
        """
        if self.logger:
            self.logger.info("=" * 100)
            self.logger.info(f"ğŸ”€ å¯åŠ¨åˆ†å—ç”Ÿæˆæ¨¡å¼ï¼ˆå¸¦ä¸Šä¸‹æ–‡ä¼ é€’ï¼‰- ä»»åŠ¡ç±»å‹: {task_type}")
            self.logger.info("=" * 100)

        # è·å–åˆ†å—é…ç½®
        if custom_chunks:
            chunk_config = {'chunks': custom_chunks, 'root_key': 'data'}
        else:
            chunk_config = self.CHUNK_CONFIGS.get(task_type)
            if not chunk_config:
                if self.logger:
                    self.logger.error(f"âŒ æœªçŸ¥çš„ä»»åŠ¡ç±»å‹: {task_type}")
                return None

        root_key = chunk_config['root_key']
        chunks = sorted(chunk_config['chunks'], key=lambda x: x['priority'])

        # å­˜å‚¨æ¯ä¸ªåˆ†å—çš„ç»“æœ
        chunk_results = {}

        # ğŸ†• ç´¯ç§¯çš„ä¸Šä¸‹æ–‡ï¼ˆå·²ç”Ÿæˆçš„å†…å®¹ï¼‰
        accumulated_context = {}

        # é€ä¸ªç”Ÿæˆåˆ†å—
        for i, chunk in enumerate(chunks, 1):
            if self.logger:
                self.logger.info(f"\nğŸ“¦ ç”Ÿæˆåˆ†å— {i}/{len(chunks)}: {chunk['name']}")
                self.logger.info(f"  â”œâ”€ åŒ…å«å­—æ®µ: {chunk['fields']}")
                self.logger.info(f"  â”œâ”€ æœ€å¤§tokens: {chunk['max_tokens']}")
                if accumulated_context:
                    self.logger.info(f"  â””â”€ ä¸Šä¸‹æ–‡: å·²ç”Ÿæˆ {len(accumulated_context)} ä¸ªå­—æ®µ")

            # ğŸ†• ç”Ÿæˆè¯¥åˆ†å—çš„æ•°æ®ï¼ˆä¼ å…¥å·²ç”Ÿæˆçš„ä¸Šä¸‹æ–‡ï¼‰
            chunk_data = self._generate_single_chunk(
                llm=llm,
                chunk=chunk,
                input_data=input_data,
                template_or_schema=template_or_schema,
                root_key=root_key,
                task_type=task_type,
                previous_context=accumulated_context  # ä¼ å…¥ä¸Šä¸‹æ–‡
            )

            if chunk_data:
                chunk_results[chunk['name']] = chunk_data

                # ğŸ†• æ›´æ–°ç´¯ç§¯ä¸Šä¸‹æ–‡
                if root_key in chunk_data:
                    accumulated_context.update(chunk_data[root_key])
                else:
                    accumulated_context.update(chunk_data)

                if self.logger:
                    self.logger.info(f"  âœ… åˆ†å—ç”ŸæˆæˆåŠŸ")
                    self.logger.info(f"  âœ… ä¸Šä¸‹æ–‡å·²æ›´æ–°: {list(accumulated_context.keys())}")
            else:
                if self.logger:
                    self.logger.warning(f"  âš ï¸ åˆ†å—ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡")

        # åˆå¹¶æ‰€æœ‰åˆ†å—
        if self.logger:
            self.logger.info("\nğŸ”— å¼€å§‹åˆå¹¶æ‰€æœ‰åˆ†å—...")

        merged_data = self._merge_chunks(chunk_results, root_key)

        if self.logger:
            self.logger.info("=" * 100)
            self.logger.info(f"âœ… åˆ†å—ç”Ÿæˆå®Œæˆï¼å…±ç”Ÿæˆ {len(chunk_results)} ä¸ªåˆ†å—")
            self.logger.info(f"ğŸ“¦ {root_key} åŒ…å«å­—æ®µ: {list(merged_data.get(root_key, {}).keys())}")
            self.logger.info("=" * 100)

        return merged_data

    def _generate_single_chunk(self, llm, chunk: Dict, input_data: Dict[str, Any],
                               template_or_schema: str, root_key: str,
                               task_type: str, previous_context: Dict[str, Any] = None) -> Optional[Dict[str, Any]]:
        """ç”Ÿæˆå•ä¸ªåˆ†å—

        Args:
            llm: LLMå¯¹è±¡
            chunk: åˆ†å—é…ç½®
            input_data: è¾“å…¥æ•°æ®
            template_or_schema: æ¨¡æ¿æˆ–Schema
            root_key: æ ¹é”®å
            task_type: ä»»åŠ¡ç±»å‹
            previous_context: ä¹‹å‰å·²ç”Ÿæˆçš„ä¸Šä¸‹æ–‡ï¼ˆç”¨äºä¿æŒä¸€è‡´æ€§ï¼‰

        Returns:
            dict: åˆ†å—æ•°æ®
        """
        # æ„å»ºæç¤ºè¯
        prompt = self._build_chunk_prompt(
            chunk=chunk,
            input_data=input_data,
            template_or_schema=template_or_schema,
            root_key=root_key,
            task_type=task_type,
            previous_context=previous_context
        )

        try:
            # è°ƒç”¨LLM
            if hasattr(llm, 'call'):
                response = llm.call(prompt)
                response_text = str(response)
            else:
                response = llm.invoke(prompt)
                response_text = response.content if hasattr(response, 'content') else str(response)

            # è§£æJSON
            from src.utils.json_utils import JsonUtils
            chunk_data = JsonUtils.safe_parse_json(response_text, debug_prefix=f"åˆ†å—_{chunk['name']}")

            return chunk_data

        except Exception as e:
            if self.logger:
                self.logger.error(f"ç”Ÿæˆåˆ†å— {chunk['name']} æ—¶å‡ºé”™: {e}")
            return None

    def _build_chunk_prompt(self, chunk: Dict, input_data: Dict[str, Any],
                           template_or_schema: str, root_key: str,
                           task_type: str, previous_context: Dict[str, Any] = None) -> str:
        """æ„å»ºåˆ†å—ç”Ÿæˆçš„æç¤ºè¯

        Args:
            chunk: åˆ†å—é…ç½®
            input_data: è¾“å…¥æ•°æ®
            template_or_schema: æ¨¡æ¿æˆ–Schema
            root_key: æ ¹é”®å
            task_type: ä»»åŠ¡ç±»å‹
            previous_context: ä¹‹å‰å·²ç”Ÿæˆçš„ä¸Šä¸‹æ–‡ï¼ˆç”¨äºä¿æŒä¸€è‡´æ€§ï¼‰

        Returns:
            str: æç¤ºè¯
        """
        fields = chunk['fields']
        chunk_name = chunk['name']

        # æ ¹æ®ä»»åŠ¡ç±»å‹å®šåˆ¶æç¤ºè¯
        if task_type == 'ppt_generation':
            task_description = "ç”ŸæˆPPTæ•°æ®"
        elif task_type == 'patient_structuring':
            task_description = "ç»“æ„åŒ–æ‚£è€…ä¿¡æ¯"
        else:
            task_description = "ç”ŸæˆJSONæ•°æ®"

        # ğŸ†• æ„å»ºä¸Šä¸‹æ–‡è¯´æ˜ï¼ˆå¦‚æœæœ‰ä¹‹å‰ç”Ÿæˆçš„å†…å®¹ï¼‰
        context_section = ""
        if previous_context and len(previous_context) > 0:
            context_section = f"""

**å·²ç”Ÿæˆçš„å†…å®¹**ï¼ˆè¯·ä¿æŒä¸€è‡´ï¼Œä¸è¦äº§ç”ŸçŸ›ç›¾ï¼‰:
{json.dumps(previous_context, ensure_ascii=False, indent=2)}

**ä¸Šä¸‹æ–‡ä¸€è‡´æ€§è¦æ±‚**:
- ä½ ç”Ÿæˆçš„å†…å®¹å¿…é¡»ä¸ä¸Šè¿°å·²ç”Ÿæˆçš„å†…å®¹ä¿æŒé€»è¾‘ä¸€è‡´
- ä¾‹å¦‚ï¼šå¦‚æœæ‚£è€…è¯Šæ–­æ˜¯"é«˜è¡€å‹"ï¼Œæ²»ç–—æ–¹æ¡ˆåº”è¯¥æ˜¯é™å‹è¯ï¼Œä¸èƒ½æ˜¯é™ç³–è¯
- å¦‚æœæ‚£è€…å¹´é¾„æ˜¯45å²ï¼Œä¸è¦åœ¨å…¶ä»–åœ°æ–¹è¯´50å²
- ä¿æŒæ‰€æœ‰æ—¥æœŸã€åç§°ã€æ•°å€¼ã€è¯Šæ–­ä¿¡æ¯çš„ä¸€è‡´æ€§
- å¼•ç”¨çš„æ–‡ä»¶åã€æ£€æŸ¥é¡¹ç›®åç§°å¿…é¡»ä¸å·²ç”Ÿæˆå†…å®¹ä¸€è‡´
"""

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªåŒ»ç–—æ•°æ®å¤„ç†ä¸“å®¶ã€‚ç°åœ¨éœ€è¦{task_description}çš„ã€{chunk_name}ã€‘éƒ¨åˆ†ã€‚

**ä»»åŠ¡**: åªç”Ÿæˆä»¥ä¸‹å­—æ®µçš„æ•°æ®ï¼š{', '.join(fields)}

**å®Œæ•´æ¨¡æ¿/Schema**ï¼ˆä½ åªéœ€è¦ç”Ÿæˆä¸Šè¿°å­—æ®µï¼‰:
{template_or_schema}

**è¾“å…¥æ•°æ®**:
{json.dumps(input_data, ensure_ascii=False, indent=2)}
{context_section}
**é‡è¦è¦æ±‚**:
1. åªç”Ÿæˆ {', '.join(fields)} è¿™äº›å­—æ®µ
2. ä¸¥æ ¼æŒ‰ç…§æ¨¡æ¿/Schemaç»“æ„è¾“å‡º
3. åªä½¿ç”¨è¾“å…¥æ•°æ®ä¸­çœŸå®å­˜åœ¨çš„ä¿¡æ¯ï¼Œä¸è¦ç¼–é€ 
4. è¾“å‡ºæ ¼å¼å¿…é¡»æ˜¯ï¼š
   {{
     "{root_key}": {{
       "field1": ...,
       "field2": ...
     }}
   }}
5. ç›´æ¥è¾“å‡ºJSONï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ–‡å­—
6. ä¸è¦åŒ…å«Markdownä»£ç å—æ ‡è®°ï¼ˆå¦‚```jsonï¼‰

è¯·è¾“å‡ºJSONæ•°æ®:"""

        return prompt

    def _merge_chunks(self, chunk_results: Dict[str, Dict[str, Any]],
                     root_key: str) -> Dict[str, Any]:
        """åˆå¹¶æ‰€æœ‰åˆ†å—

        Args:
            chunk_results: åˆ†å—ç»“æœå­—å…¸
            root_key: æ ¹é”®å

        Returns:
            dict: åˆå¹¶åçš„å®Œæ•´æ•°æ®
        """
        merged = {root_key: {}}

        # åˆå¹¶æ‰€æœ‰åˆ†å—
        for chunk_name, chunk_data in chunk_results.items():
            # å¦‚æœåˆ†å—æ•°æ®æœ‰root_keyåŒ…è£…ï¼Œè§£åŒ…
            if root_key in chunk_data:
                chunk_data = chunk_data[root_key]

            # åˆå¹¶åˆ°æ€»æ•°æ®ä¸­
            for key, value in chunk_data.items():
                if key not in merged[root_key]:
                    merged[root_key][key] = value
                elif isinstance(value, list) and isinstance(merged[root_key][key], list):
                    # åˆ—è¡¨ç±»å‹ï¼šåˆå¹¶
                    merged[root_key][key].extend(value)
                elif isinstance(value, dict) and isinstance(merged[root_key][key], dict):
                    # å­—å…¸ç±»å‹ï¼šæ›´æ–°
                    merged[root_key][key].update(value)
                else:
                    # å…¶ä»–ç±»å‹ï¼šè¦†ç›–
                    merged[root_key][key] = value

        return merged

    def estimate_output_size(self, task_type: str, input_data: Dict[str, Any]) -> int:
        """ä¼°ç®—è¾“å‡ºå¤§å°

        Args:
            task_type: ä»»åŠ¡ç±»å‹
            input_data: è¾“å…¥æ•°æ®

        Returns:
            int: ä¼°ç®—çš„è¾“å‡ºtokensæ•°
        """
        if not self.token_manager:
            # ç®€å•ä¼°ç®—
            input_size = len(json.dumps(input_data, ensure_ascii=False))
            return int(input_size / 2 * 1.2)

        # ä½¿ç”¨token_managerä¼°ç®—
        input_tokens = self.token_manager.estimate_tokens(input_data)

        # æ ¹æ®ä»»åŠ¡ç±»å‹è°ƒæ•´ä¼°ç®—æ¯”ä¾‹
        if task_type == 'ppt_generation':
            # PPTè¾“å‡ºé€šå¸¸æ˜¯è¾“å…¥çš„1.0-1.5å€
            estimated_output = int(input_tokens * 1.2)
        elif task_type == 'patient_structuring':
            # æ‚£è€…ç»“æ„åŒ–è¾“å‡ºé€šå¸¸æ˜¯è¾“å…¥çš„0.8-1.2å€
            estimated_output = int(input_tokens * 1.0)
        else:
            estimated_output = int(input_tokens * 1.0)

        return estimated_output

    def create_custom_chunks(self, field_groups: List[Tuple[str, List[str], int]]) -> List[Dict]:
        """åˆ›å»ºè‡ªå®šä¹‰åˆ†å—é…ç½®

        Args:
            field_groups: å­—æ®µåˆ†ç»„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ (åˆ†å—åç§°, å­—æ®µåˆ—è¡¨, æœ€å¤§tokens)

        Returns:
            list: åˆ†å—é…ç½®åˆ—è¡¨

        Example:
            field_groups = [
                ('åŸºæœ¬ä¿¡æ¯', ['name', 'age', 'gender'], 500),
                ('è¯Šæ–­ä¿¡æ¯', ['diagnoses', 'symptoms'], 2000),
            ]
        """
        chunks = []
        for i, (name, fields, max_tokens) in enumerate(field_groups, 1):
            chunks.append({
                'name': name,
                'fields': fields,
                'priority': i,
                'max_tokens': max_tokens
            })
        return chunks
