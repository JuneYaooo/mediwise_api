"""
Tokenç®¡ç†å™¨ - è´Ÿè´£tokenè®¡æ•°ã€é™åˆ¶æ£€æŸ¥ã€æ•°æ®å‹ç¼©åˆ¤æ–­

åŠŸèƒ½ï¼š
1. ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡
2. æ£€æŸ¥è¾“å…¥æ•°æ®æ˜¯å¦è¶…è¿‡æ¨¡å‹é™åˆ¶
3. æä¾›å‹ç¼©å»ºè®®
"""

import os
import json
from typing import Dict, Any, Union

# å°è¯•å¯¼å…¥dotenvï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è·³è¿‡
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass  # dotenvä¸æ˜¯å¿…éœ€çš„ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ç¯å¢ƒå˜é‡


class TokenManager:
    """Tokenç®¡ç†å™¨ - è´Ÿè´£tokenè®¡æ•°ã€é™åˆ¶æ£€æŸ¥ã€æ•°æ®å‹ç¼©"""

    # æ¨¡å‹é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œå¸¦é»˜è®¤å€¼ï¼‰
    MODEL_CONFIGS = {
        'gemini-3-flash-preview': {
            'max_input_tokens': int(os.getenv('MODEL_MAX_INPUT_TOKENS', '1000000')),
            'max_output_tokens': int(os.getenv('MODEL_MAX_OUTPUT_TOKENS', '65535')),
            'safe_input_ratio': float(os.getenv('TOKEN_SAFE_INPUT_RATIO', '0.7')),
            'safe_output_ratio': float(os.getenv('TOKEN_SAFE_OUTPUT_RATIO', '0.9'))
        },
        'deepseek-chat': {
            'max_input_tokens': 64000,  # DeepSeek æ”¯æŒ64Kä¸Šä¸‹æ–‡
            'max_output_tokens': 8192,   # è¾“å‡ºé™åˆ¶8K
            'safe_input_ratio': 0.7,
            'safe_output_ratio': 0.9
        },
        'qwen2.5-72b-instruct': {
            'max_input_tokens': 128000,  # Qwen2.5-72B æ”¯æŒ128Kä¸Šä¸‹æ–‡
            'max_output_tokens': 8192,   # è¾“å‡ºé™åˆ¶8K
            'safe_input_ratio': 0.7,
            'safe_output_ratio': 0.9
        },
        'gpt-4': {
            'max_input_tokens': 128000,
            'max_output_tokens': 4096,
            'safe_input_ratio': 0.7,
            'safe_output_ratio': 0.9
        },
        'gpt-4-turbo': {
            'max_input_tokens': 128000,
            'max_output_tokens': 4096,
            'safe_input_ratio': 0.7,
            'safe_output_ratio': 0.9
        },
        'claude-3-opus': {
            'max_input_tokens': 200000,
            'max_output_tokens': 4096,
            'safe_input_ratio': 0.7,
            'safe_output_ratio': 0.9
        },
        'claude-3-sonnet': {
            'max_input_tokens': 200000,
            'max_output_tokens': 4096,
            'safe_input_ratio': 0.7,
            'safe_output_ratio': 0.9
        }
    }

    def __init__(self, logger=None):
        """åˆå§‹åŒ–Tokenç®¡ç†å™¨

        Args:
            logger: æ—¥å¿—è®°å½•å™¨ï¼ˆå¯é€‰ï¼‰
        """
        self.logger = logger

    def estimate_tokens(self, text: Union[str, dict, list]) -> int:
        """ä¼°ç®—æ–‡æœ¬tokenæ•°

        ä¼°ç®—è§„åˆ™ï¼š
        - ä¸­æ–‡ï¼šçº¦1.5å­—ç¬¦/token
        - è‹±æ–‡ï¼šçº¦4å­—ç¬¦/token
        - æ··åˆæ–‡æœ¬ï¼šä½¿ç”¨2å­—ç¬¦/tokenä½œä¸ºå¹³å‡å€¼

        Args:
            text: æ–‡æœ¬å†…å®¹ï¼ˆå­—ç¬¦ä¸²ã€å­—å…¸æˆ–åˆ—è¡¨ï¼‰

        Returns:
            int: ä¼°ç®—çš„tokenæ•°é‡
        """
        # å¦‚æœæ˜¯å­—å…¸æˆ–åˆ—è¡¨ï¼Œå…ˆè½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
        if isinstance(text, (dict, list)):
            text = json.dumps(text, ensure_ascii=False)
        elif not isinstance(text, str):
            text = str(text)

        # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦æ•°
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        # ç»Ÿè®¡æ€»å­—ç¬¦æ•°
        total_chars = len(text)
        # è‹±æ–‡å­—ç¬¦æ•°
        english_chars = total_chars - chinese_chars

        # ä¼°ç®—tokenæ•°
        # ä¸­æ–‡ï¼š1.5å­—ç¬¦/tokenï¼Œè‹±æ–‡ï¼š4å­—ç¬¦/token
        estimated_tokens = int(chinese_chars / 1.5 + english_chars / 4)

        return estimated_tokens

    def get_model_config(self, model_name: str) -> Dict[str, Any]:
        """è·å–æ¨¡å‹é…ç½®

        Args:
            model_name: æ¨¡å‹åç§°

        Returns:
            dict: æ¨¡å‹é…ç½®ä¿¡æ¯
        """
        # å¦‚æœæ¨¡å‹ä¸åœ¨é…ç½®ä¸­ï¼Œä½¿ç”¨gemini-3-flash-previewçš„é…ç½®ä½œä¸ºé»˜è®¤å€¼
        if model_name not in self.MODEL_CONFIGS:
            if self.logger:
                self.logger.warning(f"æ¨¡å‹ {model_name} æœªåœ¨é…ç½®ä¸­ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self.MODEL_CONFIGS['gemini-3-flash-preview']

        return self.MODEL_CONFIGS[model_name]

    def check_input_limit(self, data: Union[str, dict, list], model_name: str) -> Dict[str, Any]:
        """æ£€æŸ¥è¾“å…¥æ•°æ®æ˜¯å¦è¶…è¿‡é™åˆ¶

        Args:
            data: è¾“å…¥æ•°æ®ï¼ˆå­—ç¬¦ä¸²ã€å­—å…¸æˆ–åˆ—è¡¨ï¼‰
            model_name: æ¨¡å‹åç§°

        Returns:
            dict: æ£€æŸ¥ç»“æœ
                - within_limit: bool, æ˜¯å¦åœ¨é™åˆ¶å†…
                - total_tokens: int, æ€»tokenæ•°
                - limit: int, é™åˆ¶tokenæ•°
                - safe_limit: int, å®‰å…¨é™åˆ¶tokenæ•°
                - compression_needed: bool, æ˜¯å¦éœ€è¦å‹ç¼©
                - compression_ratio: float, å»ºè®®å‹ç¼©æ¯”ä¾‹
        """
        # è·å–æ¨¡å‹é…ç½®
        config = self.get_model_config(model_name)

        # ä¼°ç®—tokenæ•°
        total_tokens = self.estimate_tokens(data)

        # è®¡ç®—é™åˆ¶
        max_input_tokens = config['max_input_tokens']
        safe_input_ratio = config['safe_input_ratio']
        safe_limit = int(max_input_tokens * safe_input_ratio)

        # åˆ¤æ–­æ˜¯å¦éœ€è¦å‹ç¼©
        within_limit = total_tokens <= max_input_tokens
        compression_needed = total_tokens > safe_limit

        # è®¡ç®—å»ºè®®å‹ç¼©æ¯”ä¾‹
        compression_ratio = 1.0
        if compression_needed:
            compression_ratio = safe_limit / total_tokens

        result = {
            'within_limit': within_limit,
            'total_tokens': total_tokens,
            'limit': max_input_tokens,
            'safe_limit': safe_limit,
            'compression_needed': compression_needed,
            'compression_ratio': compression_ratio,
            'usage_ratio': total_tokens / max_input_tokens
        }

        if self.logger:
            if compression_needed:
                self.logger.warning(
                    f"ğŸ“Š Tokenæ£€æŸ¥: å½“å‰={total_tokens}, å®‰å…¨é™åˆ¶={safe_limit}, "
                    f"æœ€å¤§é™åˆ¶={max_input_tokens}, ä½¿ç”¨ç‡={result['usage_ratio']:.1%}, "
                    f"å»ºè®®å‹ç¼©æ¯”ä¾‹={compression_ratio:.1%}"
                )
            else:
                self.logger.info(
                    f"ğŸ“Š Tokenæ£€æŸ¥: å½“å‰={total_tokens}, å®‰å…¨é™åˆ¶={safe_limit}, "
                    f"ä½¿ç”¨ç‡={result['usage_ratio']:.1%} âœ…"
                )

        return result

    def check_output_limit(self, expected_output_size: int, model_name: str) -> Dict[str, Any]:
        """æ£€æŸ¥é¢„æœŸè¾“å‡ºæ˜¯å¦è¶…è¿‡é™åˆ¶

        Args:
            expected_output_size: é¢„æœŸè¾“å‡ºtokenæ•°
            model_name: æ¨¡å‹åç§°

        Returns:
            dict: æ£€æŸ¥ç»“æœ
        """
        config = self.get_model_config(model_name)
        max_output_tokens = config['max_output_tokens']
        safe_output_ratio = config['safe_output_ratio']
        safe_limit = int(max_output_tokens * safe_output_ratio)

        within_limit = expected_output_size <= max_output_tokens
        needs_chunking = expected_output_size > safe_limit

        result = {
            'within_limit': within_limit,
            'expected_tokens': expected_output_size,
            'limit': max_output_tokens,
            'safe_limit': safe_limit,
            'needs_chunking': needs_chunking,
            'usage_ratio': expected_output_size / max_output_tokens
        }

        if self.logger:
            if needs_chunking:
                self.logger.warning(
                    f"ğŸ“¤ è¾“å‡ºTokenæ£€æŸ¥: é¢„æœŸ={expected_output_size}, å®‰å…¨é™åˆ¶={safe_limit}, "
                    f"æœ€å¤§é™åˆ¶={max_output_tokens}, ä½¿ç”¨ç‡={result['usage_ratio']:.1%}, "
                    f"å»ºè®®åˆ†å—è¾“å‡º"
                )
            else:
                self.logger.info(
                    f"ğŸ“¤ è¾“å‡ºTokenæ£€æŸ¥: é¢„æœŸ={expected_output_size}, å®‰å…¨é™åˆ¶={safe_limit}, "
                    f"ä½¿ç”¨ç‡={result['usage_ratio']:.1%} âœ…"
                )

        return result

    def calculate_compression_target(self, current_tokens: int, model_name: str,
                                    target_ratio: float = None) -> int:
        """è®¡ç®—å‹ç¼©ç›®æ ‡tokenæ•°

        Args:
            current_tokens: å½“å‰tokenæ•°
            model_name: æ¨¡å‹åç§°
            target_ratio: ç›®æ ‡æ¯”ä¾‹ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨safe_input_ratioï¼‰

        Returns:
            int: ç›®æ ‡tokenæ•°
        """
        config = self.get_model_config(model_name)

        if target_ratio is None:
            target_ratio = config['safe_input_ratio']

        max_input_tokens = config['max_input_tokens']
        target_tokens = int(max_input_tokens * target_ratio)

        if self.logger:
            compression_ratio = target_tokens / current_tokens if current_tokens > 0 else 1.0
            self.logger.info(
                f"ğŸ¯ å‹ç¼©ç›®æ ‡: å½“å‰={current_tokens}, ç›®æ ‡={target_tokens}, "
                f"å‹ç¼©æ¯”ä¾‹={compression_ratio:.1%}"
            )

        return target_tokens
