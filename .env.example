# PostgreSQL Database Configuration
POSTGRES_HOST=112.124.15.49
POSTGRES_PORT=5432
POSTGRES_USER=mdtadmin
POSTGRES_PASSWORD=mdtadmin@2025
POSTGRES_DATABASE=db_mdt

# Database Pool Settings
DATABASE_POOL_SIZE=5
DATABASE_MAX_OVERFLOW=10
DATABASE_POOL_TIMEOUT=30
DATABASE_POOL_RECYCLE=3600
DATABASE_POOL_PRE_PING=True

# Redis Configuration
REDIS_HOST=localhost
REDIS_PORT=36379
REDIS_DB=0
REDIS_PASSWORD=

# JWT Secret
SECRET_KEY=your_secret_key_here_change_in_production
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30

# External JWT Verification (外部系统 Token 验证)
# 用于验证外部系统传入的 JWT token
JWT_SECRET_KEY=your_jwt_secret_key_here
JWT_ALGORITHM=HS256

# Qiniu Storage
QINIU_ACCESS_KEY=your_qiniu_access_key
QINIU_SECRET_KEY=your_qiniu_secret_key
QINIU_BUCKET_NAME=your_bucket_name
QINIU_DOMAIN=your_qiniu_domain

# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key

# Server Configuration
SERVER_HOST=0.0.0.0
SERVER_PORT=8888

# LLM Configuration - General Chat Model (通用医疗分析模型)
GENERAL_CHAT_MODEL_NAME=gemini-3-flash-preview
GENERAL_CHAT_API_KEY=your_api_key_here
GENERAL_CHAT_BASE_URL=https://api.example.com/v1

# LLM Configuration - Document Generation (文档生成专用模型)
DOCUMENT_GENERATION_MODEL_NAME=gemini-3-flash-preview
DOCUMENT_GENERATION_API_KEY=your_api_key_here
DOCUMENT_GENERATION_BASE_URL=https://api.example.com/v1

# Multimodal Model Configuration (多模态图片处理)
MULTIMODAL_MODEL_NAME=Qwen/Qwen3-VL-32B-Instruct,zai-org/GLM-4.6V
MULTIMODAL_API_KEY=your_multimodal_api_keys_here
MULTIMODAL_BASE_URL=https://api.siliconflow.cn/v1

# PDF并发处理配置
PDF_IMAGE_CONCURRENT_WORKERS=10

# 多模态图片处理并发配置（默认20）
MULTIMODAL_IMAGE_CONCURRENT_WORKERS=20

# PDF处理模式配置
# default: 只提取文本（默认）
# with_images: 提取文本+图片（图片用多模态模型处理）
PDF_EXTRACTION_MODE=with_images

# File Type Classification LLM (文件类型AI判断)
FILE_TYPE_LLM_API_KEY=your_api_key_here
FILE_TYPE_LLM_BASE_URL=https://api.example.com/v1
FILE_TYPE_LLM_MODEL=gemini-3-flash-preview

# Font Configuration (字体配置)
# 支持配置多个中文字体路径，按优先级顺序用逗号分隔
# 系统会按顺序查找，使用第一个存在的字体文件
# 示例: CHINESE_FONT_PATHS=/root/font/SiYuanHeiTi-Regular/SourceHanSansSC-Regular-2.otf,/root/font/QingNiaoHuaGuangJianMeiHei/QingNiaoHuaGuangJianMeiHei-2.ttf
CHINESE_FONT_PATHS=/home/ubuntu/font/SiYuanHeiTi-Regular/SourceHanSansSC-Regular-2.otf,/root/font/SiYuanHeiTi-Regular/SourceHanSansSC-Regular-2.otf

# 兼容旧配置：单个字体路径（如果 CHINESE_FONT_PATHS 未设置，会使用此配置）
CHINESE_FONT_PATH=/path/to/your/chinese/font.otf

# ========== Token管理配置 ==========
# 模型最大输入token数（根据实际使用的模型调整）
# gemini-3-flash-preview: 1000000 (1M)
# gpt-4: 128000
# claude-3-opus: 200000
MODEL_MAX_INPUT_TOKENS=1000000

# 模型最大输出token数
MODEL_MAX_OUTPUT_TOKENS=65535

# Token安全比例（0-1之间的小数）
# safe_input_ratio: 输入使用比例，建议0.7（使用70%，留30%余量）
# safe_output_ratio: 输出使用比例，建议0.9（使用90%，留10%余量）
TOKEN_SAFE_INPUT_RATIO=0.7
TOKEN_SAFE_OUTPUT_RATIO=0.9

# ========== 数据压缩配置 ==========
# 启用自动压缩（当数据超过安全限制时自动压缩）
ENABLE_AUTO_COMPRESSION=true

# 压缩策略: smart(智能), aggressive(激进), minimal(最小)
# - smart: 按优先级保留关键信息，适度压缩（推荐）
# - aggressive: 激进压缩，只保留最核心信息
# - minimal: 最小压缩，尽可能保留原始数据
COMPRESSION_STRATEGY=smart

# 原始文件最大数量（超过此数量会被过滤或压缩）
MAX_RAW_FILES_COUNT=50

# 时间轴记录最大数量（超过此数量会被采样压缩）
MAX_TIMELINE_RECORDS=100

# 提取文本最大长度（字符数）
EXTRACTED_TEXT_MAX_LENGTH=200

# ========== 分块处理配置 ==========
# 启用分块处理（当数据量过大时，分块生成后合并）
ENABLE_CHUNKED_PROCESSING=true

# 每块的token大小（建议不超过模型输入限制的50%）
CHUNK_SIZE_TOKENS=50000

# 分块重叠比例（0-0.3之间，用于保持上下文连贯性）
CHUNK_OVERLAP_RATIO=0.1

# ========== 新功能开关配置 ==========

# 🎯 主开关（推荐使用）- 控制所有新功能
# true: 启用所有新功能（数据压缩 + 分块输出）
# false: 使用原有逻辑（默认）
# 注意: 如果设置了主开关，会覆盖下面的细粒度控制
ENABLE_NEW_FEATURES=false

# ⚙️ 细粒度控制（高级选项）- 仅在未设置主开关时生效
# 如果你需要单独控制某个功能，可以注释掉上面的 ENABLE_NEW_FEATURES，然后使用下面的配置

# 数据压缩功能（可选，默认关闭）
# 启用后会在数据传递给LLM前进行智能压缩，减少token消耗30-50%
# 适用于: patient_data_crew, patient_info_update_crew
# ENABLE_DATA_COMPRESSION=false

# 分块输出功能（可选，默认关闭）
# 启用后会使用带上下文传递的分块生成，提高成功率和逻辑一致性
# 适用于: ppt_generation_crew
# 可选值: true (强制启用), false (强制禁用，默认), auto (自动检测)
# ENABLE_CHUNKED_OUTPUT=false

