"""
Tokenç®¡ç†å’Œæ•°æ®å‹ç¼©ä½¿ç”¨ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨Tokenç®¡ç†å’Œæ•°æ®å‹ç¼©åŠŸèƒ½
"""

import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['MODEL_MAX_INPUT_TOKENS'] = '1000000'
os.environ['MODEL_MAX_OUTPUT_TOKENS'] = '65535'
os.environ['TOKEN_SAFE_INPUT_RATIO'] = '0.7'
os.environ['ENABLE_AUTO_COMPRESSION'] = 'true'
os.environ['COMPRESSION_STRATEGY'] = 'smart'

from src.utils.token_manager import TokenManager
from src.utils.data_compressor import PatientDataCompressor


class SimpleLogger:
    def info(self, msg): print(f"[INFO] {msg}")
    def warning(self, msg): print(f"[WARNING] {msg}")
    def error(self, msg, exc_info=False): print(f"[ERROR] {msg}")


def example_1_basic_token_check():
    """ç¤ºä¾‹1: åŸºç¡€Tokenæ£€æŸ¥"""
    print("\n" + "="*80)
    print("ç¤ºä¾‹1: åŸºç¡€Tokenæ£€æŸ¥")
    print("="*80)

    logger = SimpleLogger()
    token_manager = TokenManager(logger=logger)

    # æ¨¡æ‹Ÿæ‚£è€…æ•°æ®
    patient_data = {
        'patient_name': 'å¼ ä¸‰',
        'patient_timeline': [
            {'date': '2024-01-01', 'event': 'é¦–æ¬¡å°±è¯Š', 'description': 'æ‚£è€…ä¸»è¯‰å¤´ç—›...' * 100}
            for _ in range(50)
        ]
    }

    # æ£€æŸ¥tokené™åˆ¶
    check_result = token_manager.check_input_limit(patient_data, 'gemini-3-flash-preview')

    print(f"\næ£€æŸ¥ç»“æœ:")
    print(f"  æ€»tokens: {check_result['total_tokens']}")
    print(f"  å®‰å…¨é™åˆ¶: {check_result['safe_limit']}")
    print(f"  éœ€è¦å‹ç¼©: {check_result['compression_needed']}")
    print(f"  ä½¿ç”¨ç‡: {check_result['usage_ratio']:.1%}")


def example_2_auto_compression():
    """ç¤ºä¾‹2: è‡ªåŠ¨æ•°æ®å‹ç¼©"""
    print("\n" + "="*80)
    print("ç¤ºä¾‹2: è‡ªåŠ¨æ•°æ®å‹ç¼©")
    print("="*80)

    logger = SimpleLogger()
    token_manager = TokenManager(logger=logger)
    data_compressor = PatientDataCompressor(logger=logger, token_manager=token_manager)

    # åˆ›å»ºå¤§é‡æ•°æ®
    large_patient_data = {
        'patient_name': 'æå››',
        'patient_info': {'age': 45, 'gender': 'ç”·'},
        'patient_timeline': [
            {
                'date': f'2024-{i%12+1:02d}-{i%28+1:02d}',
                'event': f'å°±è¯Šè®°å½•{i}',
                'description': 'è¯¦ç»†çš„å°±è¯Šè®°å½•å†…å®¹...' * 50
            }
            for i in range(200)  # 200æ¡è®°å½•
        ],
        'raw_files_data': [
            {
                'filename': f'æŠ¥å‘Š{i}.pdf',
                'extracted_text': 'æŠ¥å‘Šå†…å®¹...' * 100
            }
            for i in range(100)  # 100ä¸ªæ–‡ä»¶
        ]
    }

    # æ£€æŸ¥åŸå§‹æ•°æ®
    original_tokens = token_manager.estimate_tokens(large_patient_data)
    print(f"\nåŸå§‹æ•°æ®: {original_tokens} tokens")

    # è‡ªåŠ¨å‹ç¼©
    check_result = token_manager.check_input_limit(large_patient_data, 'gemini-3-flash-preview')

    if check_result['compression_needed']:
        print(f"âš ï¸ æ•°æ®è¶…è¿‡å®‰å…¨é™åˆ¶ï¼Œå¼€å§‹å‹ç¼©...")

        # å‹ç¼©åˆ°å®‰å…¨é™åˆ¶
        compressed_data = data_compressor.compress_data(
            large_patient_data,
            target_tokens=check_result['safe_limit']
        )

        # æ£€æŸ¥å‹ç¼©åçš„æ•°æ®
        compressed_tokens = token_manager.estimate_tokens(compressed_data)
        print(f"å‹ç¼©åæ•°æ®: {compressed_tokens} tokens")
        print(f"å‹ç¼©æ¯”ä¾‹: {compressed_tokens/original_tokens:.1%}")

        # éªŒè¯å…³é”®å­—æ®µ
        print(f"\nå…³é”®å­—æ®µéªŒè¯:")
        print(f"  patient_name: {'âœ…' if 'patient_name' in compressed_data else 'âŒ'}")
        print(f"  patient_info: {'âœ…' if 'patient_info' in compressed_data else 'âŒ'}")
        print(f"  patient_timeline: {len(compressed_data.get('patient_timeline', []))} æ¡è®°å½•")
        print(f"  raw_files_data: {len(compressed_data.get('raw_files_data', []))} ä¸ªæ–‡ä»¶")


def example_3_timeline_compression():
    """ç¤ºä¾‹3: æ—¶é—´è½´å‹ç¼©"""
    print("\n" + "="*80)
    print("ç¤ºä¾‹3: æ—¶é—´è½´å‹ç¼©")
    print("="*80)

    logger = SimpleLogger()
    token_manager = TokenManager(logger=logger)
    data_compressor = PatientDataCompressor(logger=logger, token_manager=token_manager)

    # åˆ›å»ºæ—¶é—´è½´æ•°æ®
    timeline = [
        {
            'date': f'2024-{i%12+1:02d}-{i%28+1:02d}',
            'event_type': ['æ£€æŸ¥', 'æ²»ç–—', 'å¤è¯Š'][i % 3],
            'description': f'ç¬¬{i+1}æ¬¡å°±è¯Šï¼Œè¿›è¡Œäº†å¸¸è§„æ£€æŸ¥å’Œæ²»ç–—ã€‚' * 10,
            'doctor': f'åŒ»ç”Ÿ{i%5}'
        }
        for i in range(150)
    ]

    print(f"\nåŸå§‹æ—¶é—´è½´: {len(timeline)} æ¡è®°å½•")

    # å‹ç¼©åˆ°50æ¡
    target_tokens = 10000
    compressed_timeline = data_compressor.compress_timeline(timeline, target_tokens)

    print(f"å‹ç¼©åæ—¶é—´è½´: {len(compressed_timeline)} æ¡è®°å½•")

    # æ˜¾ç¤ºæ—¥æœŸèŒƒå›´
    if compressed_timeline:
        dates = [r['date'] for r in compressed_timeline if 'date' in r]
        if dates:
            print(f"æ—¥æœŸèŒƒå›´: {min(dates)} åˆ° {max(dates)}")


def example_4_file_compression():
    """ç¤ºä¾‹4: æ–‡ä»¶æ•°æ®å‹ç¼©ï¼ˆä¼˜å…ˆä¿ç•™åŒ»å­¦å½±åƒï¼‰"""
    print("\n" + "="*80)
    print("ç¤ºä¾‹4: æ–‡ä»¶æ•°æ®å‹ç¼©")
    print("="*80)

    logger = SimpleLogger()
    token_manager = TokenManager(logger=logger)
    data_compressor = PatientDataCompressor(logger=logger, token_manager=token_manager)

    # åˆ›å»ºæ–‡ä»¶æ•°æ®ï¼ˆåŒ…å«åŒ»å­¦å½±åƒå’Œæ™®é€šæ–‡ä»¶ï¼‰
    raw_files = []
    for i in range(80):
        file_item = {
            'filename': f'æ–‡ä»¶{i+1}.pdf',
            'file_type': ['æ£€éªŒæŠ¥å‘Š', 'å½±åƒæŠ¥å‘Š', 'ç—…å†'][i % 3],
            'exam_date': f'2024-{i%12+1:02d}-{i%28+1:02d}',
            'has_medical_image': i % 4 == 0,  # æ¯4ä¸ªæ–‡ä»¶æœ‰1ä¸ªåŒ»å­¦å½±åƒ
            'extracted_text': f'æ–‡ä»¶å†…å®¹...' * 50
        }
        raw_files.append(file_item)

    medical_count = sum(1 for f in raw_files if f.get('has_medical_image'))
    print(f"\nåŸå§‹æ–‡ä»¶: {len(raw_files)} ä¸ª (åŒ»å­¦å½±åƒ: {medical_count} ä¸ª)")

    # å‹ç¼©åˆ°30ä¸ªæ–‡ä»¶
    target_tokens = 5000
    compressed_files = data_compressor.compress_raw_files(raw_files, target_tokens)

    compressed_medical = sum(1 for f in compressed_files if f.get('has_medical_image'))
    print(f"å‹ç¼©åæ–‡ä»¶: {len(compressed_files)} ä¸ª (åŒ»å­¦å½±åƒ: {compressed_medical} ä¸ª)")
    print(f"åŒ»å­¦å½±åƒä¿ç•™ç‡: {compressed_medical/medical_count:.1%}")


def example_5_integrated_workflow():
    """ç¤ºä¾‹5: å®Œæ•´å·¥ä½œæµç¨‹ï¼ˆæ¨¡æ‹ŸPPTç”Ÿæˆï¼‰"""
    print("\n" + "="*80)
    print("ç¤ºä¾‹5: å®Œæ•´å·¥ä½œæµç¨‹")
    print("="*80)

    logger = SimpleLogger()
    token_manager = TokenManager(logger=logger)
    data_compressor = PatientDataCompressor(logger=logger, token_manager=token_manager)

    # æ¨¡æ‹Ÿå®Œæ•´çš„æ‚£è€…æ•°æ®
    patient_data = {
        'patient_name': 'ç‹äº”',
        'patient_info': {
            'basic': {'name': 'ç‹äº”', 'age': 50, 'gender': 'å¥³'},
            'contact': {'phone': '13800138000'}
        },
        'diagnoses': [
            {'date': '2024-01-15', 'diagnosis': 'é«˜è¡€å‹'},
            {'date': '2024-02-20', 'diagnosis': 'ç³–å°¿ç—…'}
        ],
        'patient_timeline': [
            {
                'date': f'2024-{i%12+1:02d}-{i%28+1:02d}',
                'event': f'å°±è¯Š{i+1}',
                'description': 'è¯¦ç»†è®°å½•...' * 30
            }
            for i in range(100)
        ],
        'raw_files_data': [
            {
                'filename': f'æŠ¥å‘Š{i}.pdf',
                'has_medical_image': i % 3 == 0,
                'extracted_text': 'æŠ¥å‘Šå†…å®¹...' * 50
            }
            for i in range(60)
        ]
    }

    print("\næ­¥éª¤1: æ£€æŸ¥Tokené™åˆ¶")
    check_result = token_manager.check_input_limit(patient_data, 'gemini-3-flash-preview')
    print(f"  æ€»tokens: {check_result['total_tokens']}")
    print(f"  éœ€è¦å‹ç¼©: {check_result['compression_needed']}")

    if check_result['compression_needed']:
        print("\næ­¥éª¤2: æ‰§è¡Œæ•°æ®å‹ç¼©")
        target_tokens = check_result['safe_limit']

        # åˆ†åˆ«å‹ç¼©ä¸åŒéƒ¨åˆ†
        compressed_timeline = data_compressor.compress_timeline(
            patient_data['patient_timeline'],
            target_tokens=int(target_tokens * 0.5)
        )

        compressed_files = data_compressor.compress_raw_files(
            patient_data['raw_files_data'],
            target_tokens=int(target_tokens * 0.3)
        )

        # æ„å»ºå‹ç¼©åçš„æ•°æ®
        compressed_patient_data = {
            'patient_name': patient_data['patient_name'],
            'patient_info': patient_data['patient_info'],
            'diagnoses': patient_data['diagnoses'],
            'patient_timeline': compressed_timeline,
            'raw_files_data': compressed_files
        }

        print("\næ­¥éª¤3: éªŒè¯å‹ç¼©ç»“æœ")
        compressed_tokens = token_manager.estimate_tokens(compressed_patient_data)
        print(f"  å‹ç¼©åtokens: {compressed_tokens}")
        print(f"  å‹ç¼©æ¯”ä¾‹: {compressed_tokens/check_result['total_tokens']:.1%}")
        print(f"  æ—¶é—´è½´è®°å½•: {len(compressed_timeline)} æ¡")
        print(f"  æ–‡ä»¶æ•°é‡: {len(compressed_files)} ä¸ª")

        print("\næ­¥éª¤4: æœ€ç»ˆæ£€æŸ¥")
        final_check = token_manager.check_input_limit(compressed_patient_data, 'gemini-3-flash-preview')
        if final_check['within_limit']:
            print("  âœ… æ•°æ®åœ¨é™åˆ¶å†…ï¼Œå¯ä»¥ç»§ç»­ç”ŸæˆPPT")
        else:
            print("  âŒ æ•°æ®ä»è¶…é™ï¼Œéœ€è¦æ›´æ¿€è¿›çš„å‹ç¼©æˆ–åˆ†å—å¤„ç†")


def main():
    """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("\nğŸš€ Tokenç®¡ç†å’Œæ•°æ®å‹ç¼©ä½¿ç”¨ç¤ºä¾‹")
    print("="*80)

    try:
        example_1_basic_token_check()
        example_2_auto_compression()
        example_3_timeline_compression()
        example_4_file_compression()
        example_5_integrated_workflow()

        print("\n" + "="*80)
        print("âœ… æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
        print("="*80)

    except Exception as e:
        print(f"\nâŒ ç¤ºä¾‹è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
