"""
æ–‡ä»¶å¤„ç†å·¥å…·ç±»
å¤„ç†æ–‡ä»¶å†…å®¹æå–ã€AIç±»å‹åˆ¤æ–­ã€å“ˆå¸Œè®¡ç®—ã€é‡å¤æ£€æµ‹ç­‰
"""

import os
import hashlib
import time
import uuid
from pathlib import Path
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

from src.custom_tools.extract_content_from_path import ExtractContentFromPathTool
from src.utils.logger import BeijingLogger

# åˆå§‹åŒ– logger
logger = BeijingLogger().get_logger()


class FileContentExtractor:
    """æ–‡ä»¶å†…å®¹æå–å™¨"""

    def __init__(self):
        self.content_extractor = ExtractContentFromPathTool()

    def _extract_single_file_content(self, file: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """æå–å•ä¸ªæ–‡ä»¶çš„å†…å®¹"""
        file_url = file.get('file_url', '')
        file_name = file.get('file_name', 'æœªå‘½åæ–‡ä»¶')
        file_uuid = file.get('file_uuid') or file.get('file_id', str(uuid.uuid4()))

        logger.info(f"ğŸ“„ ========== å¼€å§‹å¤„ç†æ–‡ä»¶ ==========")
        logger.info(f"  æ–‡ä»¶å: {file_name}")
        logger.info(f"  æ–‡ä»¶UUID: {file_uuid}")
        logger.info(f"  æ–‡ä»¶URL: {file_url}")

        file_path_from_file = file.get('file_path', '')
        logger.info(f"  æ–‡ä»¶è·¯å¾„: {file_path_from_file}")

        if not file_url and not file_path_from_file:
            logger.warning(f"âŒ [æ–‡ä»¶: {file_name}] æ—¢æ²¡æœ‰file_urlä¹Ÿæ²¡æœ‰file_pathï¼Œè·³è¿‡å†…å®¹æå–")
            return None

        start_time = time.time()
        file_preview = ""
        file_content = None

        try:
            # ç¡®å®šæ–‡ä»¶è·¯å¾„
            logger.info(f"  â”œâ”€ æ­¥éª¤1: ç¡®å®šæ–‡ä»¶è·¯å¾„")
            file_path = None
            if "file_path" in file and file["file_path"] and Path(file["file_path"]).exists():
                file_path = file["file_path"]
                logger.info(f"  â”‚  â””â”€ ä½¿ç”¨file['file_path']: {file_path}")
            elif file_path_from_file and Path(file_path_from_file).exists():
                file_path = file_path_from_file
                logger.info(f"  â”‚  â””â”€ ä½¿ç”¨file_path_from_file: {file_path}")
            else:
                logger.warning(f"âŒ [æ–‡ä»¶: {file_name}] æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ–‡ä»¶è·¯å¾„")

            # å¦‚æœæœ‰æ–‡ä»¶è·¯å¾„ï¼Œä½¿ç”¨æå–å·¥å…·è·å–å†…å®¹
            if file_path:
                logger.info(f"  â”œâ”€ æ­¥éª¤2: ä½¿ç”¨æå–å·¥å…·å¤„ç†æ–‡ä»¶")
                extraction_result = self.content_extractor._run(path=file_path)
                logger.info(f"  â”œâ”€ æ­¥éª¤3: æå–å®Œæˆï¼Œç»“æœç±»å‹: {type(extraction_result).__name__}")

                if extraction_result:
                    if isinstance(extraction_result, list):
                        logger.info(f"  â”œâ”€ æ­¥éª¤4: å¤„ç†åˆ—è¡¨ç»“æœï¼ˆZIP/PDFæå–ï¼‰ï¼ŒåŒ…å« {len(extraction_result)} ä¸ªé¡¹")
                        return self._process_zip_extraction(extraction_result, file, file_name, file_uuid, start_time)
                    elif isinstance(extraction_result, dict) and 'file_content' in extraction_result:
                        logger.info(f"  â”œâ”€ æ­¥éª¤4: å¤„ç†å•ä¸ªæ–‡ä»¶å†…å®¹")
                        content = extraction_result['file_content'] if isinstance(extraction_result['file_content'], str) else str(extraction_result['file_content'])
                        if content and content.strip():
                            logger.info(f"  â”‚  â””â”€ å†…å®¹æå–æˆåŠŸï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
                            file_preview = content[:500] + ("...[å½“å‰æ–‡ä»¶å†…å®¹è¿‡é•¿ï¼Œåªæ˜¾ç¤ºå‰500å­—ç¬¦]" if len(content) >= 500 else "")
                            file_content = content
                        else:
                            logger.warning(f"âŒ [æ–‡ä»¶: {file_name}] å•ä¸ªæ–‡ä»¶å†…å®¹ä¸ºç©º")
                            file_preview = "[æ–‡ä»¶å†…å®¹ä¸ºç©º]"
                            file_content = "[æ–‡ä»¶å†…å®¹ä¸ºç©º]"
                    else:
                        logger.warning(f"âŒ [æ–‡ä»¶: {file_name}] æå–ç»“æœæ ¼å¼å¼‚å¸¸")
                        logger.warning(f"  â””â”€ ç»“æœç±»å‹: {type(extraction_result)}")
                        file_preview = "[æ–‡ä»¶å†…å®¹æå–å¤±è´¥]"
                else:
                    logger.warning(f"âŒ [æ–‡ä»¶: {file_name}] æå–å·¥å…·è¿”å›ç©ºç»“æœ")
                    file_preview = "[æ–‡ä»¶å†…å®¹æå–å¤±è´¥]"
            elif "file_content" in file:
                logger.info(f"  â”œâ”€ æ­¥éª¤2: ä»base64å†…å®¹æå–: {file_name}")
                try:
                    import base64
                    decoded = base64.b64decode(file["file_content"]).decode('utf-8', errors='ignore')
                    logger.info(f"  â”‚  â””â”€ base64è§£ç æˆåŠŸï¼Œé•¿åº¦: {len(decoded)} å­—ç¬¦")
                    file_preview = decoded[:500] + ("...[å½“å‰æ–‡ä»¶å†…å®¹è¿‡é•¿ï¼Œåªæ˜¾ç¤ºå‰500å­—ç¬¦]" if len(decoded) >= 500 else "")
                    file_content = decoded
                except Exception as decode_e:
                    logger.error(f"âŒ [æ–‡ä»¶: {file_name}] base64è§£ç å¤±è´¥: {str(decode_e)}")
                    file_preview = "[æ— æ³•è§£ææ–‡ä»¶å†…å®¹]"
            else:
                logger.warning(f"âŒ [æ–‡ä»¶: {file_name}] æ–‡ä»¶è·¯å¾„ä¸å¯è®¿é—®ä¸”æ— file_content")
                file_preview = "[æ–‡ä»¶è·¯å¾„ä¸å¯è®¿é—®]"
        except Exception as e:
            logger.error(f"âŒ ========== æ–‡ä»¶å¤„ç†å¼‚å¸¸ ==========")
            logger.error(f"  æ–‡ä»¶å: {file_name}")
            logger.error(f"  å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            logger.error(f"  å¼‚å¸¸ä¿¡æ¯: {str(e)}")
            import traceback
            logger.error(f"  é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
            file_preview = f"[æ–‡ä»¶å†…å®¹æå–å‡ºé”™: {str(e)}]"

        extraction_time = time.time() - start_time
        logger.info(f"  â”œâ”€ æ­¥éª¤5: å†…å®¹æå–å®Œæˆï¼Œè€—æ—¶: {extraction_time:.3f}ç§’")

        # ğŸš¨ ä¼˜åŒ–ï¼šç§»é™¤å•ç‹¬çš„AIåˆ¤æ–­ï¼Œæ”¹ä¸ºæ‰¹é‡å¤„ç†ï¼ˆåœ¨process_files_concurrentlyä¸­ç»Ÿä¸€è°ƒç”¨ï¼‰
        # åªæå–æ–‡ä»¶æ‰©å±•åå’ŒåŸºæœ¬ä¿¡æ¯
        file_extension = ""
        if file_name and "." in file_name:
            file_extension = os.path.splitext(file_name)[1].lstrip('.').lower()

        # åˆ¤æ–­æ˜¯å¦åŒ…å«åŒ»å­¦å½±åƒï¼ˆå›¾ç‰‡æ–‡ä»¶ä»æå–ç»“æœè·å–ï¼Œå…¶ä»–æ–‡ä»¶é»˜è®¤Falseï¼‰
        has_medical_image = False
        # ğŸš¨ ä¼˜å…ˆä½¿ç”¨åŸå§‹çš„file_uuidï¼ˆä¿æŒä¸€è‡´æ€§ï¼‰
        extracted_file_uuid = None
        if isinstance(extraction_result, dict):
            has_medical_image = extraction_result.get('has_medical_image', False)
            extracted_file_uuid = extraction_result.get('file_uuid')  # è·å–æå–ç»“æœä¸­çš„UUIDï¼ˆä»…ä½œä¸ºå¤‡ç”¨ï¼‰

        # ğŸ”§ ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨åŸå§‹UUIDè€Œä¸æ˜¯æå–å™¨ç”Ÿæˆçš„UUIDï¼Œç¡®ä¿ä¸æ•°æ®åº“ä¸€è‡´
        final_file_uuid = file_uuid if file_uuid else extracted_file_uuid

        final_result = {
            "file_uuid": final_file_uuid,
            "file_name": file_name,
            "file_url": "",
            "file_path": file.get("file_path", ""),
            "file_extension": file_extension,
            "file_type": None,  # å»¶è¿Ÿåˆ°æ‰¹é‡AIåˆ¤æ–­æ—¶å¡«å……
            "exam_date": None,  # å»¶è¿Ÿåˆ°æ‰¹é‡AIåˆ¤æ–­æ—¶å¡«å……
            "has_medical_image": has_medical_image,
            "file_preview": file_preview,
            "file_content": file_content,
            "extracted_text": file_content,
            "extraction_time": extraction_time,

            # ğŸ”§ ä¿®å¤ï¼šä¼ é€’åŒ»å­¦å½±åƒç›¸å…³å­—æ®µï¼ˆä»extraction_resultè·å–ï¼‰
            "image_bbox": extraction_result.get('image_bbox') if isinstance(extraction_result, dict) else None,
            "cropped_image_uuid": extraction_result.get('cropped_image_uuid') if isinstance(extraction_result, dict) else None,
            "cropped_image_path": extraction_result.get('cropped_image_path') if isinstance(extraction_result, dict) else None,
            "cropped_image_filename": extraction_result.get('cropped_image_filename') if isinstance(extraction_result, dict) else None,
            "cropped_image_available": extraction_result.get('cropped_image_available', False) if isinstance(extraction_result, dict) else False,
            "cropped_temp_dir": extraction_result.get('cropped_temp_dir') if isinstance(extraction_result, dict) else None,

            # ğŸ”§ ä¿®å¤ï¼šä¼ é€’æå–çŠ¶æ€å­—æ®µ
            "extraction_success": extraction_result.get('extraction_success') if isinstance(extraction_result, dict) else None,
            "extraction_error": extraction_result.get('extraction_error') if isinstance(extraction_result, dict) else None,
            "extraction_failed": extraction_result.get('extraction_failed', False) if isinstance(extraction_result, dict) else False
        }

        logger.info(f"âœ… ========== æ–‡ä»¶å†…å®¹æå–å®Œæˆ ==========")
        logger.info(f"  æ–‡ä»¶å: {file_name}")
        logger.info(f"  å†…å®¹é•¿åº¦: {len(file_content) if file_content else 0} å­—ç¬¦")
        logger.info(f"  æå–è€—æ—¶: {extraction_time:.3f}ç§’")
        logger.info(f"  (æ–‡ä»¶ç±»å‹å°†åœ¨æ‰¹é‡AIåˆ¤æ–­é˜¶æ®µç¡®å®š)")

        return final_result

    def _process_zip_extraction(self, extraction_result: List, file: Dict, file_name: str,
                                file_uuid: str, start_time: float) -> Optional[List[Dict]]:
        """å¤„ç†ZIPæ–‡ä»¶æå–ç»“æœ"""
        results = []
        total_content_length = 0
        valid_files_count = 0

        for i, item in enumerate(extraction_result):
            logger.info(f"å¤„ç†å­æ–‡ä»¶ {i+1}/{len(extraction_result)}: {item.get('file_name', f'å­æ–‡ä»¶_{i}')}")
            if isinstance(item, dict) and 'file_content' in item:
                item_content = item['file_content'] if isinstance(item['file_content'], str) else str(item['file_content'])

                # ğŸš¨ ä¼˜å…ˆä½¿ç”¨itemä¸­çš„file_uuidï¼Œå¦‚æœæ²¡æœ‰å†ç”Ÿæˆæ–°çš„
                sub_file_uuid = item.get('file_uuid', str(uuid.uuid4()))
                sub_file_name = item.get('file_name', f'å­æ–‡ä»¶_{i}')
                sub_file_extension = ""
                if sub_file_name and "." in sub_file_name:
                    sub_file_extension = os.path.splitext(sub_file_name)[1].lstrip('.').lower()

                if item_content and item_content.strip():
                    total_content_length += len(item_content)
                    valid_files_count += 1
                    logger.info(f"å­æ–‡ä»¶ {sub_file_name} å†…å®¹æå–æˆåŠŸï¼Œé•¿åº¦: {len(item_content)} å­—ç¬¦")

                    # ğŸš¨ ä¼˜åŒ–ï¼šç§»é™¤å•ç‹¬çš„AIåˆ¤æ–­ï¼Œæ”¹ä¸ºæ‰¹é‡å¤„ç†
                    # ä»æå–ç»“æœè·å–has_medical_image
                    has_medical_image = item.get('has_medical_image', False)

                    sub_result = {
                        "file_uuid": sub_file_uuid,
                        "file_name": sub_file_name,
                        "file_url": "",
                        "file_path": file.get("file_path", ""),
                        "file_extension": sub_file_extension,
                        "file_type": None,  # å»¶è¿Ÿåˆ°æ‰¹é‡AIåˆ¤æ–­
                        "exam_date": None,  # å»¶è¿Ÿåˆ°æ‰¹é‡AIåˆ¤æ–­
                        "has_medical_image": has_medical_image,
                        "file_preview": item_content[:500] + ("...[å½“å‰æ–‡ä»¶å†…å®¹è¿‡é•¿ï¼Œåªæ˜¾ç¤ºå‰500å­—ç¬¦]" if len(item_content) >= 500 else ""),
                        "file_content": item_content,
                        "extracted_text": item_content,
                        "extraction_time": time.time() - start_time,
                        "parent_zip_file": file_name,
                        "parent_zip_uuid": file_uuid,
                        "is_from_zip": True,
                        "original_file_path": item.get('original_file_path'),
                        "temp_file_available": item.get('temp_file_available', True),

                        # ğŸš¨ ä¼ é€’itemä¸­çš„æ‰€æœ‰PDF/å›¾ç‰‡ç›¸å…³å­—æ®µ
                        "source_type": item.get('source_type', 'uploaded'),
                        "parent_pdf_uuid": item.get('parent_pdf_uuid'),
                        "parent_pdf_filename": item.get('parent_pdf_filename'),
                        "page_number": item.get('page_number'),
                        "image_index_in_page": item.get('image_index_in_page'),
                        "position_in_parent": item.get('position_in_parent'),
                        "temp_file_path": item.get('temp_file_path'),
                        "persistent_temp_file": item.get('persistent_temp_file'),
                        "cleanup_temp_dir": item.get('cleanup_temp_dir'),

                        # è£å‰ªå›¾ç‰‡ä¿¡æ¯
                        "image_bbox": item.get('image_bbox'),
                        "cropped_image_uuid": item.get('cropped_image_uuid'),
                        "cropped_image_path": item.get('cropped_image_path'),
                        "cropped_image_filename": item.get('cropped_image_filename'),
                        "cropped_image_available": item.get('cropped_image_available', False),
                        "cropped_temp_dir": item.get('cropped_temp_dir')
                    }
                    results.append(sub_result)
                else:
                    logger.warning(f"å­æ–‡ä»¶ {sub_file_name} å†…å®¹ä¸ºç©ºï¼Œä½†ä»è®°å½•æ–‡ä»¶ä¿¡æ¯")
                    inferred_type = self._infer_file_type_by_extension(sub_file_extension)

                    sub_result = {
                        "file_uuid": sub_file_uuid,
                        "file_name": sub_file_name,
                        "file_url": "",
                        "file_path": file.get("file_path", ""),
                        "file_extension": sub_file_extension,
                        "file_type": inferred_type,
                        "exam_date": None,
                        "has_medical_image": False,
                        "file_preview": f"[æ–‡ä»¶ {sub_file_name} - {inferred_type}ï¼Œå†…å®¹æå–å¤±è´¥æˆ–ä¸ºç©º]",
                        "file_content": f"æ–‡ä»¶å: {sub_file_name}\næ–‡ä»¶ç±»å‹: {inferred_type}\nçŠ¶æ€: å†…å®¹æå–å¤±è´¥æˆ–ä¸ºç©º",
                        "extracted_text": f"æ–‡ä»¶å: {sub_file_name}\næ–‡ä»¶ç±»å‹: {inferred_type}\nçŠ¶æ€: å†…å®¹æå–å¤±è´¥æˆ–ä¸ºç©º",
                        "extraction_time": time.time() - start_time,
                        "parent_zip_file": file_name,
                        "parent_zip_uuid": file_uuid,
                        "is_from_zip": True,
                        "extraction_failed": True
                    }
                    results.append(sub_result)

        # ç”Ÿæˆzipæ–‡ä»¶çš„ç»¼åˆé¢„è§ˆä¿¡æ¯ï¼ˆå»¶è¿Ÿåˆ°AIåˆ¤æ–­åï¼‰
        if results:
            zip_summary = f"ZIPæ–‡ä»¶åŒ…å« {len(results)} ä¸ªå­æ–‡ä»¶ï¼Œå…¶ä¸­ {valid_files_count} ä¸ªæˆåŠŸæå–å†…å®¹"
            if total_content_length > 0:
                zip_summary += f"ï¼Œæ€»å†…å®¹é•¿åº¦: {total_content_length} å­—ç¬¦"

            file_list = []
            for i, result in enumerate(results[:5]):
                # ğŸš¨ ä¿®æ”¹ï¼šæš‚æ—¶ä¸æ˜¾ç¤ºfile_typeï¼ˆå› ä¸ºAIåˆ¤æ–­è¿˜æœªæ‰§è¡Œï¼‰
                file_info = f"{result['file_name']}"
                if result.get('file_extension'):
                    file_info += f" (.{result['file_extension']})"
                file_list.append(file_info)

            if len(results) > 5:
                file_list.append(f"... è¿˜æœ‰ {len(results) - 5} ä¸ªæ–‡ä»¶")

            zip_summary += "\næ–‡ä»¶åˆ—è¡¨:\n- " + "\n- ".join(file_list)

            for result in results:
                if not result.get('extraction_failed'):
                    result['zip_summary'] = zip_summary

            logger.info(f"ZIPæ–‡ä»¶å¤„ç†å®Œæˆ: {file_name}, å­æ–‡ä»¶æ•°: {len(results)}, æœ‰æ•ˆæ–‡ä»¶æ•°: {valid_files_count}")
            return results
        else:
            logger.warning(f"ZIPæ–‡ä»¶ {file_name} å¤„ç†åæœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆå­æ–‡ä»¶")
            return None

    def _infer_file_type_by_extension(self, file_extension: str) -> str:
        """æ ¹æ®æ–‡ä»¶æ‰©å±•åæ¨æµ‹å¯èƒ½çš„æ–‡ä»¶ç±»å‹"""
        if file_extension in ['pdf', 'docx', 'doc']:
            return "æ–‡æ¡£æ–‡ä»¶"
        elif file_extension in ['jpg', 'jpeg', 'png', 'gif', 'bmp']:
            return "å›¾ç‰‡æ–‡ä»¶"
        elif file_extension in ['txt', 'md']:
            return "æ–‡æœ¬æ–‡ä»¶"
        elif file_extension in ['xlsx', 'xls', 'csv']:
            return "è¡¨æ ¼æ–‡ä»¶"
        elif file_extension in ['pptx', 'ppt']:
            return "æ¼”ç¤ºæ–‡ç¨¿"
        return "å…¶ä»–"

    def _determine_file_type(self, file_name: str, file_content: str) -> Dict[str, Any]:
        """ä½¿ç”¨å¤§æ¨¡å‹æ™ºèƒ½åˆ¤æ–­æ–‡ä»¶ç±»å‹å’Œæ—¶é—´ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶"""
        logger.info(f"å¼€å§‹AIæ–‡ä»¶ç±»å‹åˆ¤æ–­: {file_name}")
        logger.info(f"æ–‡ä»¶å†…å®¹é•¿åº¦: {len(file_content) if file_content else 0} å­—ç¬¦")

        if not file_content or not file_content.strip():
            logger.warning(f"æ–‡ä»¶å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡AIåˆ¤æ–­: {file_name}")
            return {"file_type": "å…¶ä»–", "exam_date": None}

        # æ£€æŸ¥æ–‡ä»¶å†…å®¹æ˜¯å¦åŒ…å«é”™è¯¯ä¿¡æ¯
        error_indicators = [
            "å¤„ç†å¤±è´¥", "æ— æ³•å¤„ç†", "ä¸å¯ç”¨", "å¤±è´¥", "é”™è¯¯", "error", "failed",
            "å›¾ç‰‡URLæ— æ•ˆ", "æ–‡ä»¶ä¸å­˜åœ¨", "æƒé™ä¸è¶³", "å¤šæ¨¡æ€APIè¿”å›å†…å®¹ä¸ºç©º"
        ]

        content_lower = file_content.lower()
        detected_errors = [indicator for indicator in error_indicators if indicator.lower() in content_lower]

        if detected_errors:
            logger.warning(f"æ£€æµ‹åˆ°é”™è¯¯ä¿¡æ¯åœ¨æ–‡ä»¶å†…å®¹ä¸­: {file_name}")
            logger.warning(f"æ£€æµ‹åˆ°çš„é”™è¯¯æŒ‡ç¤ºè¯: {detected_errors}")
            return {"file_type": "å…¶ä»–", "exam_date": None, "note": f"æ–‡ä»¶å¤„ç†å¤±è´¥: {', '.join(detected_errors)}"}

        # æˆªå–å‰2000å­—ç¬¦ç”¨äºåˆ†æ
        content_sample = file_content[:2000] if len(file_content) > 2000 else file_content
        logger.info(f"ç”¨äºAIåˆ†æçš„å†…å®¹é•¿åº¦: {len(content_sample)} å­—ç¬¦")

        # ğŸš¨ ä¿®å¤ï¼šå°†jsonå¯¼å…¥ç§»åˆ°tryå—å¤–ï¼Œé¿å…åœ¨exceptä¸­ä½¿ç”¨æœªå®šä¹‰çš„å˜é‡
        import json as json_lib

        # é‡è¯•æœºåˆ¶ï¼šæœ€å¤šé‡è¯•3æ¬¡
        max_retries = 3
        for attempt in range(max_retries):
            try:
                logger.info(f"AIæ–‡ä»¶ç±»å‹åˆ¤æ–­å°è¯• {attempt + 1}/{max_retries}: {file_name}")

                from openai import OpenAI

                api_key = os.getenv('FILE_TYPE_LLM_API_KEY')
                base_url = os.getenv('FILE_TYPE_LLM_BASE_URL')
                model_name = os.getenv('FILE_TYPE_LLM_MODEL', 'gemini-2.5-pro')

                if not api_key or not base_url:
                    logger.error(f"æ–‡ä»¶ç±»å‹åˆ¤æ–­ APIé…ç½®ç¼ºå¤±")
                    return {"file_type": "å…¶ä»–", "exam_date": None, "note": "AI APIé…ç½®ç¼ºå¤±"}

                client = OpenAI(api_key=api_key, base_url=base_url)

                prompt = f"""è¯·åˆ†æä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œåˆ¤æ–­å…¶æ˜¯å¦ä¸ºåŒ»ç–—æ–‡æ¡£ï¼Œå¦‚æœæ˜¯åŒ»ç–—æ–‡æ¡£åˆ™è¿›ä¸€æ­¥åˆ†ç±»ï¼Œå¹¶è¯†åˆ«ç›¸å…³æ—¶é—´ã€‚

ã€æ–‡æ¡£å†…å®¹ã€‘ï¼ˆå‰2000å­—ç¬¦ï¼‰:
{content_sample}

ã€åˆ†ç±»è§„åˆ™ã€‘ï¼š
å¦‚æœæ˜¯åŒ»ç–—ç›¸å…³æ–‡æ¡£ï¼Œè¯·ä»ä»¥ä¸‹8ä¸ªç±»å‹ä¸­é€‰æ‹©ï¼š
1. æ£€éªŒå• - è¡€æ¶²ã€å°¿æ¶²ã€ç”ŸåŒ–ç­‰å®éªŒå®¤æ£€æŸ¥ç»“æœ
2. å½±åƒæŠ¥å‘Š - CTã€MRIã€Xå…‰ã€è¶…å£°ç­‰å½±åƒå­¦æ£€æŸ¥æŠ¥å‘Š
3. ç—…ç†æŠ¥å‘Š - ç»„ç»‡ç—…ç†å­¦æ£€æŸ¥ã€æ´»æ£€ã€ç»†èƒå­¦æ£€æŸ¥æŠ¥å‘Š
4. è¯Šæ–­æŠ¥å‘Š - ä¸´åºŠè¯Šæ–­ä¹¦ã€ç—…å†ã€é—¨è¯Šè®°å½•ã€ä½é™¢è®°å½•
5. å¤„æ–¹å• - è¯ç‰©å¤„æ–¹ã€ç”¨è¯æŒ‡å¯¼
6. æ‰‹æœ¯è®°å½• - æ‰‹æœ¯æ“ä½œè®°å½•ã€æ‰‹æœ¯æŠ¥å‘Š
7. æŠ¤ç†è®°å½• - æŠ¤ç†è§‚å¯Ÿè®°å½•ã€æŠ¤ç†è®¡åˆ’
8. ä½“æ£€æŠ¥å‘Š - å¥åº·ä½“æ£€ã€å…¥èŒä½“æ£€ç­‰ç»¼åˆæ£€æŸ¥æŠ¥å‘Š

å¦‚æœä¸æ˜¯åŒ»ç–—æ–‡æ¡£æˆ–ä¸å±äºä»¥ä¸Šç±»å‹ï¼Œç»Ÿä¸€å½’ç±»ä¸º"å…¶ä»–"ã€‚

ã€æ—¶é—´è¯†åˆ«è§„åˆ™ã€‘ï¼š
- å¯¹äºæ£€æŸ¥æŠ¥å‘Šï¼ˆæ£€éªŒå•ã€å½±åƒæŠ¥å‘Šã€ç—…ç†æŠ¥å‘Šã€ä½“æ£€æŠ¥å‘Šï¼‰ï¼Œä¼˜å…ˆè¯†åˆ«æ£€æŸ¥æ—¶é—´ã€é‡‡æ ·æ—¶é—´ã€æ£€æŸ¥æ—¥æœŸ
- å¯¹äºè¯Šç–—è®°å½•ï¼ˆè¯Šæ–­æŠ¥å‘Šã€æ‰‹æœ¯è®°å½•ã€æŠ¤ç†è®°å½•ï¼‰ï¼Œè¯†åˆ«å°±è¯Šæ—¶é—´ã€æ‰‹æœ¯æ—¶é—´ã€è®°å½•æ—¶é—´
- å¯¹äºå¤„æ–¹å•ï¼Œè¯†åˆ«å¼€å…·æ—¶é—´
- å¦‚æœæ‰¾åˆ°å¤šä¸ªæ—¶é—´ï¼Œé€‰æ‹©æœ€æ—©çš„æ£€æŸ¥/æ²»ç–—ç›¸å…³æ—¶é—´

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
  "file_type": "ç±»å‹åç§°",
  "exam_date": "æ£€æŸ¥/æ²»ç–—æ—¶é—´ï¼ˆYYYY-MM-DDæ ¼å¼ï¼Œå¦‚æœªæ‰¾åˆ°åˆ™ä¸ºnullï¼‰",
  "note": "å¦‚æœæ˜¯å…¶ä»–ç±»å‹ï¼Œè¯·ç®€è¦è¯´æ˜æ–‡æ¡£å†…å®¹æ€§è´¨ï¼›å¦‚æœæ˜¯åŒ»ç–—æ–‡æ¡£ä½†æœªæ‰¾åˆ°æ—¶é—´ï¼Œè¯´æ˜æ—¶é—´æƒ…å†µ"
}}

æ³¨æ„ï¼šfile_typeåªèƒ½æ˜¯ä¸Šè¿°8ä¸ªåŒ»ç–—ç±»å‹ä¹‹ä¸€ï¼Œæˆ–è€…"å…¶ä»–"ã€‚exam_dateå¿…é¡»æ˜¯YYYY-MM-DDæ ¼å¼æˆ–nullã€‚"""

                response = client.chat.completions.create(
                    model=model_name,
                    messages=[{"role": "user", "content": prompt}],
                    response_format={"type": "json_object"},
                    temperature=0.1,
                    max_tokens=6000,
                    timeout=300.0  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°30ç§’ï¼Œç¡®ä¿å®Œæ•´æ¥æ”¶å“åº”
                )

                logger.info(f"æ–‡ä»¶ç±»å‹åˆ¤æ–­ APIè°ƒç”¨æˆåŠŸ: {file_name}")

                response_content = response.choices[0].message.content

                # ğŸš¨ è°ƒè¯•ï¼šæ‰“å°åŸå§‹å“åº”å†…å®¹
                if response_content:
                    logger.info(f"[æ–‡ä»¶: {file_name}] APIåŸå§‹å“åº”å†…å®¹é•¿åº¦: {len(response_content)}")
                    logger.info(f"[æ–‡ä»¶: {file_name}] APIåŸå§‹å“åº”å†…å®¹: {repr(response_content)}")
                else:
                    logger.warning(f"[æ–‡ä»¶: {file_name}] AIè¿”å›å†…å®¹ä¸ºNone")

                if not response_content or not response_content.strip():
                    if attempt < max_retries - 1:
                        logger.warning(f"[æ–‡ä»¶: {file_name}] AIè¿”å›å†…å®¹ä¸ºç©º (å°è¯• {attempt + 1}/{max_retries})ï¼Œå‡†å¤‡é‡è¯•")
                        continue
                    else:
                        logger.error(f"[æ–‡ä»¶: {file_name}] AIè¿”å›å†…å®¹ä¸ºç©ºï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                        return {"file_type": "å…¶ä»–", "exam_date": None}

                # æ¸…ç†å“åº”å†…å®¹ï¼šç§»é™¤å¯èƒ½çš„ç‰¹æ®Šæ ‡è®°
                response_content = response_content.strip()

                # å¤„ç†GLMæ¨¡å‹è¿”å›çš„ç‰¹æ®Šæ ‡è®°
                if response_content.startswith("<|begin_of_box|>"):
                    response_content = response_content[len("<|begin_of_box|>"):].strip()
                if response_content.endswith("<|end_of_box|>"):
                    response_content = response_content[:-len("<|end_of_box|>")].strip()

                # å¤„ç† markdown ä»£ç å—åŒ…è£¹ï¼ˆå¦‚ ```json ... ```ï¼‰
                if response_content.startswith("```"):
                    # ç§»é™¤å¼€å¤´çš„ ```json æˆ– ```
                    lines = response_content.split('\n')
                    if lines[0].startswith("```"):
                        lines = lines[1:]  # ç§»é™¤ç¬¬ä¸€è¡Œ ```json
                    if lines and lines[-1].strip() == "```":
                        lines = lines[:-1]  # ç§»é™¤æœ€åä¸€è¡Œ ```
                    response_content = '\n'.join(lines).strip()
                    logger.info(f"[æ–‡ä»¶: {file_name}] å·²ç§»é™¤ markdown ä»£ç å—åŒ…è£¹")

                logger.info(f"[æ–‡ä»¶: {file_name}] æ¸…ç†åçš„å“åº”å†…å®¹ï¼ˆå‰200å­—ç¬¦ï¼‰: {response_content[:200]}")

                # æ£€æŸ¥ JSON å®Œæ•´æ€§ï¼ˆç®€å•æ£€æŸ¥ï¼šæ˜¯å¦ä»¥ } ç»“å°¾ï¼‰
                if not response_content.rstrip().endswith('}'):
                    logger.warning(f"âš ï¸ [æ–‡ä»¶: {file_name}] JSON å¯èƒ½ä¸å®Œæ•´ï¼Œå“åº”æœªä»¥ '}}' ç»“å°¾")
                    logger.warning(f"  â””â”€ æœ€å50å­—ç¬¦: {response_content[-50:]}")
                    if attempt < max_retries - 1:
                        logger.warning(f"  â””â”€ å‡†å¤‡é‡è¯•...")
                        continue
                    else:
                        logger.error(f"âŒ [æ–‡ä»¶: {file_name}] JSON ä¸å®Œæ•´ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                        return {"file_type": "å…¶ä»–", "exam_date": None}

                result = json_lib.loads(response_content)
                file_type = result.get("file_type", "å…¶ä»–")
                if file_type is None:
                    file_type = "å…¶ä»–"
                else:
                    file_type = str(file_type).strip()

                exam_date = result.get("exam_date")
                note = result.get("note", "")

                logger.info(f"AIè§£æç»“æœ: {file_name} -> ç±»å‹: {file_type}, æ—¶é—´: {exam_date}, å¤‡æ³¨: {note}")

                processed_exam_date = exam_date if exam_date and exam_date != "null" else None

                valid_medical_types = [
                    "æ£€éªŒå•", "å½±åƒæŠ¥å‘Š", "ç—…ç†æŠ¥å‘Š", "è¯Šæ–­æŠ¥å‘Š", "å¤„æ–¹å•",
                    "æ‰‹æœ¯è®°å½•", "æŠ¤ç†è®°å½•", "ä½“æ£€æŠ¥å‘Š"
                ]

                if file_type in valid_medical_types:
                    return {"file_type": file_type, "exam_date": processed_exam_date}
                elif file_type == "å…¶ä»–":
                    return {"file_type": "å…¶ä»–", "exam_date": None}
                else:
                    logger.warning(f"AIè¿”å›äº†æœªçŸ¥çš„æ–‡ä»¶ç±»å‹: '{file_type}'ï¼Œå½’ç±»ä¸ºå…¶ä»–")
                    return {"file_type": "å…¶ä»–", "exam_date": None}

            except json_lib.JSONDecodeError as e:
                logger.error(f"âŒ [æ–‡ä»¶: {file_name}] JSONè§£æå¤±è´¥")
                logger.error(f"  â”œâ”€ é”™è¯¯ç±»å‹: JSONDecodeError")
                logger.error(f"  â”œâ”€ é”™è¯¯ä¿¡æ¯: {str(e)}")
                logger.error(f"  â”œâ”€ å“åº”å†…å®¹é•¿åº¦: {len(response_content) if 'response_content' in locals() else 'N/A'}")
                logger.error(f"  â”œâ”€ å®Œæ•´å“åº”å†…å®¹: {repr(response_content) if 'response_content' in locals() else 'N/A'}")
                logger.error(f"  â””â”€ å°è¯•æ¬¡æ•°: {attempt + 1}/{max_retries}")

                if attempt < max_retries - 1:
                    logger.warning(f"  â””â”€ å‡†å¤‡é‡è¯•...")
                    continue
                else:
                    logger.error(f"âŒ [æ–‡ä»¶: {file_name}] å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                    return {"file_type": "å…¶ä»–", "exam_date": None}

            except Exception as e:
                logger.error(f"âŒ [æ–‡ä»¶: {file_name}] AIåˆ¤æ–­å¤±è´¥")
                logger.error(f"  â”œâ”€ é”™è¯¯ç±»å‹: {type(e).__name__}")
                logger.error(f"  â”œâ”€ é”™è¯¯ä¿¡æ¯: {str(e)}")
                logger.error(f"  â””â”€ å°è¯•æ¬¡æ•°: {attempt + 1}/{max_retries}")

                if attempt < max_retries - 1:
                    logger.warning(f"  â””â”€ å‡†å¤‡é‡è¯•...")
                    continue
                else:
                    logger.error(f"âŒ [æ–‡ä»¶: {file_name}] å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                    return {"file_type": "å…¶ä»–", "exam_date": None}

        return {"file_type": "å…¶ä»–", "exam_date": None}

    def _calculate_file_hash(self, file_path: str) -> Optional[str]:
        """è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼"""
        if not file_path or not os.path.exists(file_path):
            return None

        try:
            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            logger.error(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼å¤±è´¥ {file_path}: {str(e)}")
            return None

    def _calculate_content_hash(self, content: Any) -> Optional[str]:
        """è®¡ç®—æ–‡ä»¶å†…å®¹çš„MD5å“ˆå¸Œå€¼"""
        if not content:
            return None

        try:
            if isinstance(content, str):
                content_bytes = content.encode('utf-8')
            elif isinstance(content, bytes):
                content_bytes = content
            else:
                content_bytes = str(content).encode('utf-8')

            return hashlib.md5(content_bytes).hexdigest()
        except Exception as e:
            logger.error(f"è®¡ç®—å†…å®¹å“ˆå¸Œå€¼å¤±è´¥: {str(e)}")
            return None

    def _detect_duplicate_files(self, files: List[Dict]) -> tuple[List[Dict], List[Dict]]:
        """
        æ£€æµ‹å¹¶å‰”é™¤é‡å¤æ–‡ä»¶
        è¿”å›å»é‡åçš„æ–‡ä»¶åˆ—è¡¨å’Œé‡å¤æ–‡ä»¶ä¿¡æ¯
        """
        unique_files = []
        duplicate_files = []
        seen_hashes = set()
        seen_names = set()

        logger.info(f"å¼€å§‹æ£€æµ‹é‡å¤æ–‡ä»¶ï¼Œæ€»è®¡ {len(files)} ä¸ªæ–‡ä»¶")

        for i, file in enumerate(files):
            file_name = file.get('file_name', f'æœªçŸ¥æ–‡ä»¶_{i}')
            file_content = file.get('file_content')
            file_path = file.get('file_path')

            # è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼
            file_hash = None
            content_hash = None

            if file_path and os.path.exists(file_path):
                file_hash = self._calculate_file_hash(file_path)

            if not file_hash and file_content:
                content_hash = self._calculate_content_hash(file_content)
                file_hash = content_hash

            # æ£€æŸ¥æ˜¯å¦é‡å¤
            is_duplicate = False
            duplicate_reason = ""

            if file_hash and file_hash in seen_hashes:
                is_duplicate = True
                duplicate_reason = "æ–‡ä»¶å†…å®¹ç›¸åŒ"
            elif file_name in seen_names:
                is_duplicate = True
                duplicate_reason = "æ–‡ä»¶åç›¸åŒ"

            if is_duplicate:
                duplicate_info = {
                    "file_name": file_name,
                    "file_uuid": file.get('file_uuid', file.get('file_id')),
                    "duplicate_reason": duplicate_reason,
                    "file_hash": file_hash,
                    "original_index": i
                }
                duplicate_files.append(duplicate_info)
                logger.info(f"å‘ç°é‡å¤æ–‡ä»¶: {file_name} - {duplicate_reason}")
            else:
                unique_files.append(file)
                if file_hash:
                    seen_hashes.add(file_hash)
                seen_names.add(file_name)
                file['file_hash'] = file_hash
                file['content_hash'] = content_hash

        logger.info(f"é‡å¤æ–‡ä»¶æ£€æµ‹å®Œæˆ: å”¯ä¸€æ–‡ä»¶ {len(unique_files)} ä¸ª, é‡å¤æ–‡ä»¶ {len(duplicate_files)} ä¸ª")

        if duplicate_files:
            logger.info("é‡å¤æ–‡ä»¶è¯¦æƒ…:")
            for dup in duplicate_files:
                logger.info(f"  - {dup['file_name']} ({dup['duplicate_reason']})")

        return unique_files, duplicate_files

    def process_files_concurrently(self, files: List[Dict], max_workers: int = 5) -> List[Dict]:
        """å¹¶å‘å¤„ç†æ–‡ä»¶åˆ—è¡¨ï¼ŒåŒ…æ‹¬AIæ–‡ä»¶ç±»å‹åˆ¤æ–­ï¼Œç°åœ¨åŒ…å«é‡å¤æ–‡ä»¶æ£€æµ‹"""
        if not files:
            return []

        logger.info("=" * 80)
        logger.info(f"å¼€å§‹å¹¶å‘å¤„ç† {len(files)} ä¸ªæ–‡ä»¶ï¼ˆå¹¶å‘æ•°: {max_workers}ï¼‰")
        logger.info("=" * 80)

        # ========== é˜¶æ®µ1: é‡å¤æ–‡ä»¶æ£€æµ‹ ==========
        duplicate_detection_start = time.time()
        logger.info("-" * 80)
        logger.info("ã€é˜¶æ®µ1ã€‘å¼€å§‹é‡å¤æ–‡ä»¶æ£€æµ‹")
        logger.info("-" * 80)

        # é¦–å…ˆè¿›è¡Œé‡å¤æ–‡ä»¶æ£€æµ‹
        unique_files, duplicate_files = self._detect_duplicate_files(files)

        duplicate_detection_duration = time.time() - duplicate_detection_start
        logger.info("-" * 80)
        logger.info(f"ã€é˜¶æ®µ1ã€‘é‡å¤æ–‡ä»¶æ£€æµ‹å®Œæˆï¼Œè€—æ—¶: {duplicate_detection_duration:.2f} ç§’")
        logger.info(f"  åŸå§‹æ–‡ä»¶: {len(files)} ä¸ªï¼Œå”¯ä¸€æ–‡ä»¶: {len(unique_files)} ä¸ªï¼Œé‡å¤æ–‡ä»¶: {len(duplicate_files)} ä¸ª")
        logger.info("-" * 80)

        if duplicate_files:
            logger.info(f"å·²å‰”é™¤ {len(duplicate_files)} ä¸ªé‡å¤æ–‡ä»¶")

        if not unique_files:
            logger.warning("æ‰€æœ‰æ–‡ä»¶éƒ½æ˜¯é‡å¤æ–‡ä»¶ï¼Œæ²¡æœ‰éœ€è¦å¤„ç†çš„æ–‡ä»¶")
            return []

        # ========== é˜¶æ®µ2: æ–‡ä»¶å†…å®¹æå–ï¼ˆå¹¶å‘ï¼‰ ==========
        extraction_start_time = time.time()
        logger.info("-" * 80)
        logger.info(f"ã€é˜¶æ®µ2ã€‘å¼€å§‹æ–‡ä»¶å†…å®¹æå–ï¼ˆå¹¶å‘æ•°: {max_workers}ï¼‰")
        logger.info("-" * 80)

        file_results = []

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰æ–‡ä»¶æå–ä»»åŠ¡
            future_to_file = {
                executor.submit(self._extract_single_file_content, file): file
                for file in unique_files
            }

            # æ”¶é›†æ–‡ä»¶æå–ç»“æœ
            extraction_results = []
            completed_count = 0
            for future in as_completed(future_to_file):
                try:
                    result = future.result()
                    completed_count += 1
                    logger.info(f"  è¿›åº¦: {completed_count}/{len(unique_files)} æ–‡ä»¶å·²å®Œæˆæå–")
                    if result:
                        if isinstance(result, list):
                            extraction_results.extend(result)
                        else:
                            extraction_results.append(result)
                except Exception as e:
                    file_info = future_to_file[future]
                    logger.error(f"Error processing file {file_info.get('file_name', 'unknown')}: {str(e)}")

            extraction_duration = time.time() - extraction_start_time
            logger.info("-" * 80)
            logger.info(f"ã€é˜¶æ®µ2ã€‘æ–‡ä»¶å†…å®¹æå–å®Œæˆï¼Œè€—æ—¶: {extraction_duration:.2f} ç§’")
            logger.info(f"  æå–æˆåŠŸ: {len(extraction_results)} ä¸ªæ–‡ä»¶/å­æ–‡ä»¶")
            if extraction_results:
                extraction_times = [r.get('extraction_time', 0) for r in extraction_results if r.get('extraction_time')]
                if extraction_times:
                    avg_time = sum(extraction_times) / len(extraction_times)
                    logger.info(f"  å•ä¸ªæ–‡ä»¶å¹³å‡æå–æ—¶é—´: {avg_time:.2f} ç§’")
            logger.info("-" * 80)

            # ========== é˜¶æ®µ3: AIæ–‡ä»¶ç±»å‹åˆ¤æ–­ï¼ˆæ‰¹é‡å¹¶å‘ï¼Œ20å¹¶å‘ï¼‰ ==========
            ai_start_time = time.time()
            if extraction_results:
                logger.info("-" * 80)
                logger.info(f"ã€é˜¶æ®µ3ã€‘å¼€å§‹AIæ–‡ä»¶ç±»å‹æ‰¹é‡åˆ¤æ–­ï¼ˆå¹¶å‘æ•°: {int(os.getenv('MULTIMODAL_IMAGE_CONCURRENT_WORKERS', '20'))}ï¼‰")
                logger.info("-" * 80)

                # ğŸš¨ ä¼˜åŒ–ï¼šæ‰€æœ‰æ–‡ä»¶ï¼ˆåŒ…æ‹¬zipå­æ–‡ä»¶ï¼‰éƒ½ç»Ÿä¸€æ‰¹é‡AIåˆ¤æ–­
                files_need_ai = [result for result in extraction_results if result.get('file_content')]

                logger.info(f"  éœ€è¦AIåˆ¤æ–­çš„æ–‡ä»¶æ€»æ•°: {len(files_need_ai)} ä¸ª")

                if files_need_ai:
                    # ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®çš„å¹¶å‘æ•°å¤„ç†AIåˆ¤æ–­
                    ai_max_workers = int(os.getenv("MULTIMODAL_IMAGE_CONCURRENT_WORKERS", "20"))
                    with ThreadPoolExecutor(max_workers=ai_max_workers) as ai_executor:
                        ai_futures = {
                            ai_executor.submit(self._determine_file_type, result['file_name'], result['file_content']): result
                            for result in files_need_ai
                        }

                        ai_completed_count = 0
                        for ai_future in as_completed(ai_futures):
                            try:
                                result = ai_futures[ai_future]
                                type_result = ai_future.result()
                                ai_completed_count += 1
                                logger.info(f"  è¿›åº¦: {ai_completed_count}/{len(files_need_ai)} AIåˆ¤æ–­å·²å®Œæˆ - {result['file_name']}")
                                if isinstance(type_result, dict):
                                    result['file_type'] = type_result["file_type"]
                                    result['exam_date'] = type_result.get("exam_date")
                                else:
                                    result['file_type'] = type_result
                                    result['exam_date'] = None
                                file_results.append(result)
                            except Exception as e:
                                result = ai_futures[ai_future]
                                logger.error(f"AIæ–‡ä»¶ç±»å‹åˆ¤æ–­å¤±è´¥ {result['file_name']}: {str(e)}")
                                result['file_type'] = "å…¶ä»–"
                                result['exam_date'] = None
                                file_results.append(result)
                else:
                    # å¦‚æœæ²¡æœ‰éœ€è¦AIåˆ¤æ–­çš„æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨æå–ç»“æœ
                    file_results = extraction_results

                ai_duration = time.time() - ai_start_time
                logger.info("-" * 80)
                logger.info(f"ã€é˜¶æ®µ3ã€‘AIæ–‡ä»¶ç±»å‹åˆ¤æ–­å®Œæˆï¼Œè€—æ—¶: {ai_duration:.2f} ç§’")
                if files_need_ai:
                    avg_ai_time = ai_duration / len(files_need_ai)
                    logger.info(f"  å•ä¸ªæ–‡ä»¶å¹³å‡AIåˆ¤æ–­æ—¶é—´: {avg_ai_time:.2f} ç§’")
                    concurrent_workers = int(os.getenv("MULTIMODAL_IMAGE_CONCURRENT_WORKERS", "20"))
                    logger.info(f"  å¹¶å‘æ•ˆç‡æå‡: çº¦ {max_workers / concurrent_workers * 100:.0f}% -> 100% (ä½¿ç”¨{concurrent_workers}å¹¶å‘)")
                logger.info("-" * 80)

        # ========== æ€»ä½“ç»Ÿè®¡ ==========
        total_duration = time.time() - extraction_start_time + duplicate_detection_duration
        logger.info("=" * 80)
        logger.info("æ–‡ä»¶å¹¶å‘å¤„ç†å®Œæˆ - è€—æ—¶ç»Ÿè®¡")
        logger.info("=" * 80)
        logger.info(f"ã€é˜¶æ®µ1ã€‘é‡å¤æ–‡ä»¶æ£€æµ‹:        {duplicate_detection_duration:.2f} ç§’ - {(duplicate_detection_duration/total_duration*100):.1f}%")
        logger.info(f"ã€é˜¶æ®µ2ã€‘æ–‡ä»¶å†…å®¹æå–:        {extraction_duration:.2f} ç§’ - {(extraction_duration/total_duration*100):.1f}%")
        if 'ai_duration' in locals():
            logger.info(f"ã€é˜¶æ®µ3ã€‘AIæ–‡ä»¶ç±»å‹åˆ¤æ–­:      {ai_duration:.2f} ç§’ - {(ai_duration/total_duration*100):.1f}%")
        logger.info("-" * 80)
        logger.info(f"ã€æ€»è®¡ã€‘æ–‡ä»¶å¤„ç†æ€»æ—¶é—´:      {total_duration:.2f} ç§’")
        logger.info(f"å¤„ç†ç»Ÿè®¡: åŸå§‹ {len(files)} ä¸ª -> å»é‡å {len(unique_files)} ä¸ª -> æˆåŠŸ {len(file_results)} ä¸ª (é‡å¤ {len(duplicate_files)} ä¸ª)")
        logger.info("=" * 80)

        # ä¸ºç»“æœæ·»åŠ é‡å¤æ–‡ä»¶ä¿¡æ¯
        if file_results and duplicate_files:
            for result in file_results[:1]:
                result['duplicate_files_detected'] = len(duplicate_files)
                result['duplicate_files_info'] = duplicate_files

        return file_results
