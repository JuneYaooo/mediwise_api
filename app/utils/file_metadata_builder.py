"""
æ–‡ä»¶å…ƒæ•°æ®æ„å»ºå·¥å…·
ç”¨äºæ„å»ºraw_files_dataç­‰æ–‡ä»¶å…ƒæ•°æ®
"""
import time
from typing import List, Dict, Any
from app.config.file_constants import (
    IMAGE_EXTENSIONS, DOCUMENT_EXTENSIONS, TEXT_EXTENSIONS
)
from app.utils.timezone_utils import get_beijing_now_naive
from src.utils.logger import BeijingLogger

logger = BeijingLogger().get_logger()


class FileMetadataBuilder:
    """æ–‡ä»¶å…ƒæ•°æ®æ„å»ºç±»"""

    @staticmethod
    def generate_extracted_text(file_content: str, original_filename: str,
                               file_type: str, file_extension: str,
                               extraction_failed: bool, parent_zip: str = "") -> str:
        """
        ä¸ºç©ºå†…å®¹æˆ–æå–å¤±è´¥çš„æ–‡ä»¶ç”Ÿæˆæè¿°æ€§æ–‡æœ¬

        Args:
            file_content: æ–‡ä»¶å†…å®¹
            original_filename: åŸå§‹æ–‡ä»¶å
            file_type: æ–‡ä»¶ç±»å‹
            file_extension: æ–‡ä»¶æ‰©å±•å
            extraction_failed: æ˜¯å¦æå–å¤±è´¥
            parent_zip: æ¥æºzipæ–‡ä»¶å

        Returns:
            æè¿°æ€§æ–‡æœ¬
        """
        if file_content and file_content.strip():
            return file_content

        # ç”Ÿæˆæè¿°æ€§å†…å®¹
        if extraction_failed:
            extracted_text = f"æ–‡ä»¶å: {original_filename}\næ–‡ä»¶ç±»å‹: {file_type}\nçŠ¶æ€: å†…å®¹æå–å¤±è´¥ï¼Œä½†æ–‡ä»¶å­˜åœ¨"
            if parent_zip:
                extracted_text += f"\næ¥æº: {parent_zip}"
        else:
            extracted_text = f"æ–‡ä»¶å: {original_filename}\næ–‡ä»¶ç±»å‹: {file_type}\nçŠ¶æ€: æ–‡ä»¶å†…å®¹ä¸ºç©º"
            if parent_zip:
                extracted_text += f"\næ¥æº: {parent_zip}"

        # æ ¹æ®æ–‡ä»¶æ‰©å±•åæ·»åŠ å¯èƒ½çš„å†…å®¹æè¿°
        if file_extension in DOCUMENT_EXTENSIONS or file_extension == 'pdf':
            extracted_text += "\nå¯èƒ½åŒ…å«: æ–‡æ¡£å†…å®¹ã€è¯Šæ–­æŠ¥å‘Šã€æ£€éªŒç»“æœç­‰"
        elif file_extension in IMAGE_EXTENSIONS:
            extracted_text += "\nå¯èƒ½åŒ…å«: åŒ»å­¦å½±åƒã€æ£€æŸ¥å›¾ç‰‡ã€ç—…ç†åˆ‡ç‰‡ç­‰"
        elif file_extension in TEXT_EXTENSIONS:
            extracted_text += "\nå¯èƒ½åŒ…å«: æ–‡æœ¬è®°å½•ã€ç—…å†ä¿¡æ¯ç­‰"
        elif file_extension in ['xlsx', 'xls', 'csv']:
            extracted_text += "\nå¯èƒ½åŒ…å«: æ•°æ®è¡¨æ ¼ã€æ£€éªŒæ•°å€¼ã€ç»Ÿè®¡ä¿¡æ¯ç­‰"
        elif file_extension == 'zip':
            extracted_text += "\nå¯èƒ½åŒ…å«: å‹ç¼©åŒ…ï¼ŒåŒ…å«å¤šä¸ªåŒ»ç–—ç›¸å…³æ–‡ä»¶"

        return extracted_text

    @classmethod
    def build_raw_file_item(cls, result: Dict) -> Dict:
        """
        æ„å»ºå•ä¸ªæ–‡ä»¶çš„raw_files_dataé¡¹

        Args:
            result: æ–‡ä»¶æå–ç»“æœ

        Returns:
            raw_files_dataé¡¹
        """
        original_filename = result.get('file_name', '')
        file_ext = result.get('file_extension', '')
        sub_file_uuid = result.get('file_uuid')

        # æ„å»ºä¸Šä¼ æ–‡ä»¶å
        if file_ext:
            upload_filename = f"{sub_file_uuid}.{file_ext}"
        else:
            upload_filename = sub_file_uuid

        file_content = result.get('file_content', '')
        extracted_text = cls.generate_extracted_text(
            file_content=file_content,
            original_filename=original_filename,
            file_type=result.get('file_type', 'å…¶ä»–'),
            file_extension=file_ext,
            extraction_failed=result.get('extraction_failed', False),
            parent_zip=result.get('parent_zip_file', '')
        )

        raw_file_item = {
            # ğŸ”§ å…³é”®å­—æ®µï¼šä¿ç•™åŸå§‹å­—æ®µåï¼Œä¾› save_patient_files ä½¿ç”¨
            "file_uuid": sub_file_uuid,
            "file_name": original_filename,           # âœ… ä¿ç•™åŸå­—æ®µåï¼ˆè€Œéæ”¹ä¸º filenameï¼‰
            "file_url": result.get('file_url'),       # âœ… ä¿ç•™åŸå­—æ®µåï¼ˆè€Œéæ”¹ä¸º cloud_storage_urlï¼‰
            "file_path": result.get('file_path'),     # âœ… æ–°å¢ï¼šä¿ç•™æ–‡ä»¶è·¯å¾„
            "file_hash": result.get('file_hash'),     # âœ… æ–°å¢ï¼šä¿ç•™æ–‡ä»¶å“ˆå¸Œï¼ˆç”¨äºå»é‡ï¼‰

            # å…¼å®¹å­—æ®µï¼šä¸ºå…¶ä»–æ¨¡å—æä¾›å¤‡ç”¨å­—æ®µå
            "filename": original_filename,
            "upload_filename": upload_filename,
            "cloud_storage_url": result.get('file_url'),

            "file_extension": file_ext,
            "file_type": result.get('file_type'),
            "has_medical_image": result.get('has_medical_image', False),
            "file_size": len(file_content) if file_content else 0,
            "file_content": file_content,
            "extracted_text": extracted_text,
            "upload_timestamp": get_beijing_now_naive().strftime('%Y-%m-%dT%H:%M:%S'),  # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨åŒ—äº¬æ—¶é—´
            "exam_date": result.get('exam_date'),

            # ZIPç›¸å…³å­—æ®µ
            "parent_zip_file": result.get('parent_zip_file'),
            "parent_zip_uuid": result.get('parent_zip_uuid'),
            "parent_zip_filename": result.get('parent_zip_filename'),
            "is_from_zip": result.get('is_from_zip', False),

            # PDFç›¸å…³å­—æ®µ
            "source_type": result.get('source_type', 'uploaded'),
            "parent_pdf_uuid": result.get('parent_pdf_uuid'),
            "parent_pdf_filename": result.get('parent_pdf_filename'),
            "is_from_pdf": result.get('is_from_pdf', False),
            "extraction_mode": result.get('extraction_mode'),
            "extracted_image_count": result.get('extracted_image_count'),
            "page_number": result.get('page_number'),
            "image_index_in_page": result.get('image_index_in_page'),

            # ä½ç½®ä¿¡æ¯
            "location": cls.build_location_info(result),

            # åŒ»å­¦å½±åƒè¾¹ç•Œæ¡†ï¼ˆç”¨äºè£å‰ªï¼‰
            "image_bbox": result.get('image_bbox'),

            # è£å‰ªåçš„åŒ»å­¦å½±åƒ
            "cropped_image_uuid": result.get('cropped_image_uuid'),
            "cropped_image_path": result.get('cropped_image_path'),
            "cropped_image_filename": result.get('cropped_image_filename'),
            "cropped_image_url": result.get('cropped_image_url'),
            "cropped_image_available": result.get('cropped_image_available', False),

            # æå–çŠ¶æ€
            "extraction_failed": result.get('extraction_failed', False),
            "extraction_success": result.get('extraction_success'),
            "extraction_error": result.get('extraction_error'),

            # ä¼šè¯å…³è”
            "conversation_id": result.get('conversation_id')
        }

        return raw_file_item

    @staticmethod
    def build_location_info(result: Dict) -> Dict:
        """
        æ„å»ºä½ç½®ä¿¡æ¯

        Args:
            result: æ–‡ä»¶æå–ç»“æœ

        Returns:
            ä½ç½®ä¿¡æ¯å­—å…¸
        """
        source_type = result.get('source_type', 'uploaded')

        if source_type == 'extracted_from_pdf':
            # ä»PDFæå–çš„å›¾ç‰‡
            page_number = result.get('page_number')
            image_index = result.get('image_index_in_page', 0)

            if page_number:
                return {
                    "type": "pdf_extracted_image",
                    "page_number": page_number,
                    "image_index_in_page": image_index,
                    "position_in_parent": result.get('position_in_parent', f"page_{page_number}_image_{image_index}"),
                    "description": f"ç¬¬{page_number}é¡µ å›¾ç‰‡{image_index + 1}"
                }

        elif source_type == 'uploaded':
            # ç”¨æˆ·ç›´æ¥ä¸Šä¼ 
            return {
                "type": "direct_upload",
                "description": "ç”¨æˆ·ç›´æ¥ä¸Šä¼ "
            }

        elif result.get('is_from_zip'):
            # ä»ZIPæå–
            return {
                "type": "zip_extracted",
                "parent_zip": result.get('parent_zip_file'),
                "description": f"æ¥è‡ª {result.get('parent_zip_file', 'å‹ç¼©åŒ…')}"
            }

        # é»˜è®¤
        return {
            "type": "unknown",
            "description": "æœªçŸ¥æ¥æº"
        }

    @classmethod
    def build_raw_files_data(cls, extracted_file_results: List[Dict]) -> List[Dict]:
        """
        æ„å»ºraw_files_dataåˆ—è¡¨ï¼Œç¡®ä¿zipæ–‡ä»¶åœ¨ç¬¬ä¸€ä¸ªä½ç½®

        Args:
            extracted_file_results: æ–‡ä»¶æå–ç»“æœåˆ—è¡¨

        Returns:
            raw_files_dataåˆ—è¡¨
        """
        zip_raw_data = []
        sub_raw_data = []
        other_raw_data = []

        for result in extracted_file_results:
            raw_file_item = cls.build_raw_file_item(result)

            # æŒ‰æ–‡ä»¶ç±»å‹åˆ†ç»„
            if result.get('is_from_zip'):
                sub_raw_data.append(raw_file_item)
            elif result.get('file_name', '').lower().endswith('.zip'):
                zip_raw_data.append(raw_file_item)
            else:
                other_raw_data.append(raw_file_item)

        # æŒ‰é¡ºåºç»„è£…ï¼šzipæ–‡ä»¶åœ¨ç¬¬ä¸€ä¸ªä½ç½®
        raw_files_data = zip_raw_data + sub_raw_data + other_raw_data

        logger.info(
            f"æ„å»ºäº† {len(raw_files_data)} ä¸ªæ–‡ä»¶çš„raw_files_data "
            f"(zip: {len(zip_raw_data)}, å­æ–‡ä»¶: {len(sub_raw_data)}, å…¶ä»–: {len(other_raw_data)})"
        )

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        cls.log_file_statistics(raw_files_data)

        return raw_files_data

    @staticmethod
    def log_file_statistics(raw_files_data: List[Dict]) -> None:
        """è®°å½•æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯"""
        total_count = len(raw_files_data)
        zip_files_count = sum(1 for item in raw_files_data if item.get('is_from_zip'))
        successful_extractions = sum(
            1 for item in raw_files_data
            if not item.get('extraction_failed') and item.get('extracted_text', '').strip()
        )
        failed_extractions = sum(1 for item in raw_files_data if item.get('extraction_failed'))

        logger.info(
            f"æ–‡ä»¶ç»Ÿè®¡: æ€»è®¡ {total_count} ä¸ª, æ¥è‡ªzip {zip_files_count} ä¸ª, "
            f"æˆåŠŸæå– {successful_extractions} ä¸ª, æå–å¤±è´¥ {failed_extractions} ä¸ª"
        )

        # è¾“å‡ºå‰å‡ ä¸ªæ–‡ä»¶çš„è¯¦ç»†ä¿¡æ¯
        for i, file_data in enumerate(raw_files_data[:5], 1):
            filename = file_data.get('filename', 'æœªçŸ¥')
            file_type = file_data.get('file_type', 'æœªçŸ¥')
            text_length = len(file_data.get('extracted_text', ''))
            is_from_zip = file_data.get('is_from_zip', False)
            extraction_failed = file_data.get('extraction_failed', False)

            status = "âœ…æˆåŠŸ" if not extraction_failed and text_length > 100 else (
                "âš ï¸å¤±è´¥" if extraction_failed else "ğŸ“„ç©ºå†…å®¹"
            )
            source = "(æ¥è‡ªzip)" if is_from_zip else ""

            logger.info(f"  {i}. {filename} {source} - {file_type} - {status} - {text_length} å­—ç¬¦")

        if len(raw_files_data) > 5:
            logger.info(f"  ... è¿˜æœ‰ {len(raw_files_data) - 5} ä¸ªæ–‡ä»¶")

    @staticmethod
    def build_file_info_for_api(extracted_file_results: List[Dict]) -> List[Dict]:
        """
        æ„å»ºä¼ é€’ç»™medical_apiçš„æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨

        Args:
            extracted_file_results: æ–‡ä»¶æå–ç»“æœåˆ—è¡¨

        Returns:
            æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
        """
        files_to_pass = []

        for extracted in extracted_file_results:
            file_info = {
                "file_id": extracted.get('file_uuid'),
                "file_uuid": extracted.get('file_uuid'),
                "file_name": extracted.get('file_name'),
                "file_url": extracted.get('file_url'),
                "file_extension": extracted.get('file_extension', ''),
                "file_type": extracted.get('file_type', 'å…¶ä»–'),
                "has_medical_image": extracted.get('has_medical_image', False),
                "file_size": len(extracted.get('file_content', '')) if extracted.get('file_content') else 0,
                "file_content": extracted.get('file_content', ''),  # ğŸš¨ æ–°å¢ï¼šä¿ç•™file_content
                "extracted_text": extracted.get('extracted_text', ''),
                "file_preview": extracted.get('file_preview', ''),
                "ai_file_type": extracted.get('file_type', 'å…¶ä»–'),
                "exam_date": extracted.get('exam_date'),
                "extraction_time": extracted.get('extraction_time'),
                "parent_zip_file": extracted.get('parent_zip_file'),
                "is_from_zip": extracted.get('is_from_zip', False),
                "extraction_failed": extracted.get('extraction_failed', False),

                # PDFç›¸å…³å­—æ®µ
                "source_type": extracted.get('source_type', 'uploaded'),
                "parent_pdf_uuid": extracted.get('parent_pdf_uuid'),
                "parent_pdf_filename": extracted.get('parent_pdf_filename'),

                # ğŸš¨ å…³é”®ä¿®å¤ï¼šæ·»åŠ æ–‡ä»¶è·¯å¾„å­—æ®µï¼Œç”¨äºä¸Šä¼ äºŒè¿›åˆ¶æ–‡ä»¶
                "original_file_path": extracted.get('original_file_path'),
                "temp_file_path": extracted.get('temp_file_path'),
                "temp_file_available": extracted.get('temp_file_available', False),
                "persistent_temp_file": extracted.get('persistent_temp_file', False),
                "cleanup_temp_dir": extracted.get('cleanup_temp_dir'),

                # è£å‰ªåŒ»å­¦å½±åƒä¿¡æ¯
                "cropped_image_uuid": extracted.get('cropped_image_uuid'),
                "cropped_image_path": extracted.get('cropped_image_path'),
                "cropped_image_filename": extracted.get('cropped_image_filename'),
                "cropped_image_available": extracted.get('cropped_image_available', False)
            }
            files_to_pass.append(file_info)

        logger.info(f"ä¼ é€’ç»™medical_apiçš„æ–‡ä»¶æ•°é‡: {len(files_to_pass)}")

        # è¾“å‡ºå‰å‡ ä¸ªæ–‡ä»¶çš„ä¿¡æ¯
        for i, file_info in enumerate(files_to_pass[:3], 1):
            file_name = file_info.get('file_name', 'æœªçŸ¥')
            is_from_zip = file_info.get('is_from_zip', False)
            extraction_failed = file_info.get('extraction_failed', False)
            text_length = len(file_info.get('extracted_text', ''))

            status = "âœ…æœ‰å†…å®¹" if text_length > 50 else (
                "âš ï¸æå–å¤±è´¥" if extraction_failed else "ğŸ“„æ— å†…å®¹"
            )
            source = "(æ¥è‡ªzip)" if is_from_zip else ""

            logger.info(f"  ä¼ é€’æ–‡ä»¶ {i}: {file_name} {source} - {status} - {text_length} å­—ç¬¦")

        return files_to_pass

    @staticmethod
    def collect_extraction_statistics(extracted_file_results: List[Dict]) -> Dict[str, Any]:
        """
        æ”¶é›†æ–‡ä»¶æå–ç»Ÿè®¡ä¿¡æ¯ï¼ˆåªç»Ÿè®¡åŸå§‹æ–‡ä»¶ï¼Œä¸ç»Ÿè®¡PDFå†…æå–çš„å›¾ç‰‡ï¼‰

        Args:
            extracted_file_results: æ–‡ä»¶æå–ç»“æœåˆ—è¡¨

        Returns:
            æå–ç»Ÿè®¡ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«ï¼š
            - total_files: æ€»æ–‡ä»¶æ•°ï¼ˆåªè®¡åŸå§‹æ–‡ä»¶ï¼‰
            - successful_extractions: æˆåŠŸæå–æ•°
            - failed_extractions: å¤±è´¥æå–æ•°
            - success_rate: æˆåŠŸç‡
            - failed_files: å¤±è´¥æ–‡ä»¶è¯¦æƒ…åˆ—è¡¨
        """
        # è¿‡æ»¤ï¼šåªç»Ÿè®¡åŸå§‹ä¸Šä¼ çš„æ–‡ä»¶ï¼Œæ’é™¤PDFå†…æå–çš„å›¾ç‰‡
        original_files = [
            result for result in extracted_file_results
            if result.get('source_type') not in ['extracted_from_pdf', 'rendered_pdf_page']
        ]

        total_files = len(original_files)
        successful_count = 0
        failed_count = 0
        failed_files = []

        for result in original_files:
            # ä½¿ç”¨extraction_successå­—æ®µåˆ¤æ–­æ˜¯å¦æˆåŠŸ
            extraction_success = result.get('extraction_success', None)

            # å¦‚æœæœ‰extraction_successå­—æ®µï¼Œç›´æ¥ä½¿ç”¨
            if extraction_success is not None:
                if extraction_success:
                    successful_count += 1
                else:
                    failed_count += 1
                    failed_files.append({
                        'filename': result.get('file_name', 'æœªçŸ¥æ–‡ä»¶'),
                        'file_type': result.get('file_extension', 'æœªçŸ¥ç±»å‹'),
                        'error_reason': result.get('extraction_error', 'æœªæä¾›é”™è¯¯åŸå› ')
                    })
            # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰extraction_successå­—æ®µï¼Œä½¿ç”¨åŸæœ‰çš„extraction_failedåˆ¤æ–­
            elif result.get('extraction_failed', False):
                failed_count += 1
                failed_files.append({
                    'filename': result.get('file_name', 'æœªçŸ¥æ–‡ä»¶'),
                    'file_type': result.get('file_extension', 'æœªçŸ¥ç±»å‹'),
                    'error_reason': 'æå–å¤±è´¥'
                })
            else:
                # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆå†…å®¹
                file_content = result.get('file_content', '') or result.get('extracted_text', '')
                has_content = file_content and len(str(file_content).strip()) > 0

                if has_content:
                    successful_count += 1
                else:
                    failed_count += 1
                    failed_files.append({
                        'filename': result.get('file_name', 'æœªçŸ¥æ–‡ä»¶'),
                        'file_type': result.get('file_extension', 'æœªçŸ¥ç±»å‹'),
                        'error_reason': 'æå–å†…å®¹ä¸ºç©º'
                    })

        success_rate = (successful_count / total_files * 100) if total_files > 0 else 100

        statistics = {
            'total_files': total_files,
            'successful_extractions': successful_count,
            'failed_extractions': failed_count,
            'success_rate': round(success_rate, 2),
            'failed_files': failed_files,
            'collected_at': time.strftime('%Y-%m-%dT%H:%M:%S', time.localtime())
        }

        # è®¡ç®—PDFå†…æå–çš„å›¾ç‰‡æ•°é‡ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        pdf_extracted_count = len(extracted_file_results) - len(original_files)

        logger.info(
            f"æ–‡ä»¶æå–ç»Ÿè®¡ï¼ˆä»…ç»Ÿè®¡åŸå§‹æ–‡ä»¶ï¼‰: æ€»è®¡ {total_files} ä¸ªåŸå§‹æ–‡ä»¶, æˆåŠŸ {successful_count} ä¸ª, "
            f"å¤±è´¥ {failed_count} ä¸ª, æˆåŠŸç‡ {success_rate:.2f}%"
        )
        if pdf_extracted_count > 0:
            logger.info(f"  ï¼ˆå¦æœ‰ {pdf_extracted_count} ä¸ªPDFå†…æå–çš„å›¾ç‰‡æœªè®¡å…¥ç»Ÿè®¡ï¼‰")

        if failed_files:
            logger.warning(f"ä»¥ä¸‹ {len(failed_files)} ä¸ªæ–‡ä»¶æå–å¤±è´¥:")
            for i, failed_file in enumerate(failed_files[:10], 1):  # æœ€å¤šæ˜¾ç¤º10ä¸ª
                logger.warning(
                    f"  {i}. {failed_file['filename']} ({failed_file['file_type']}) - "
                    f"åŸå› : {failed_file['error_reason']}"
                )
            if len(failed_files) > 10:
                logger.warning(f"  ... è¿˜æœ‰ {len(failed_files) - 10} ä¸ªæ–‡ä»¶æå–å¤±è´¥")

        return statistics

    @staticmethod
    def filter_for_llm_input(raw_files_data: List[Dict]) -> List[Dict]:
        """
        è¿‡æ»¤ç”¨äºLLMè¾“å…¥çš„æ–‡ä»¶æ•°æ®ï¼ˆé¿å…é‡å¤ï¼‰
        è·³è¿‡ä»PDFæå–/æ¸²æŸ“çš„å›¾ç‰‡ï¼Œå› ä¸ºPDFçš„file_contentå·²åŒ…å«å›¾ç‰‡æè¿°

        Args:
            raw_files_data: åŸå§‹æ–‡ä»¶æ•°æ®åˆ—è¡¨

        Returns:
            è¿‡æ»¤åçš„æ–‡ä»¶æ•°æ®åˆ—è¡¨
        """
        if not raw_files_data:
            return []

        # è¿‡æ»¤æ‰ä»PDFæå–æˆ–æ¸²æŸ“çš„å›¾ç‰‡
        filtered = [
            file_item for file_item in raw_files_data
            if file_item.get('source_type') not in ['extracted_from_pdf', 'rendered_pdf_page']
        ]

        logger.info(
            f"è¿‡æ»¤ç”¨äºLLMè¾“å…¥çš„æ–‡ä»¶: åŸå§‹ {len(raw_files_data)} ä¸ª, "
            f"è¿‡æ»¤å {len(filtered)} ä¸ª (è·³è¿‡ {len(raw_files_data) - len(filtered)} ä¸ªPDFæå–/æ¸²æŸ“çš„å›¾ç‰‡)"
        )

        return filtered

    @staticmethod
    def filter_medical_images(raw_files_data: List[Dict]) -> List[Dict]:
        """
        åªä¿ç•™åŒ»å­¦å½±åƒï¼ˆåŒ…æ‹¬ä»PDFæå–çš„ï¼‰
        ç”¨äºPPTç”Ÿæˆç­‰éœ€è¦å›¾ç‰‡URLçš„åœºæ™¯

        Args:
            raw_files_data: åŸå§‹æ–‡ä»¶æ•°æ®åˆ—è¡¨

        Returns:
            åŒ»å­¦å½±åƒåˆ—è¡¨
        """
        if not raw_files_data:
            return []

        medical_images = [
            file_item for file_item in raw_files_data
            if file_item.get('has_medical_image')
        ]

        logger.info(f"ç­›é€‰åŒ»å­¦å½±åƒ: å…± {len(medical_images)} å¼ ")

        return medical_images

    @staticmethod
    def get_pdf_extracted_images(raw_files_data: List[Dict], pdf_uuid: str) -> List[Dict]:
        """
        è·å–æŸä¸ªPDFæå–çš„æ‰€æœ‰å›¾ç‰‡

        Args:
            raw_files_data: åŸå§‹æ–‡ä»¶æ•°æ®åˆ—è¡¨
            pdf_uuid: PDFæ–‡ä»¶çš„UUID

        Returns:
            è¯¥PDFæå–çš„å›¾ç‰‡åˆ—è¡¨
        """
        if not raw_files_data or not pdf_uuid:
            return []

        pdf_images = [
            file_item for file_item in raw_files_data
            if file_item.get('parent_pdf_uuid') == pdf_uuid
        ]

        logger.info(f"PDF {pdf_uuid} æå–çš„å›¾ç‰‡: {len(pdf_images)} å¼ ")

        return pdf_images

