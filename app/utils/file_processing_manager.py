"""
æ–‡ä»¶å¤„ç†ç®¡ç†å™¨
åè°ƒæ–‡ä»¶ä¸Šä¼ ã€å†…å®¹æå–ã€å…ƒæ•°æ®æ„å»ºç­‰æ“ä½œ
"""
import os
import shutil
import uuid
from pathlib import Path
from typing import List, Dict, Any, Optional
from app.utils.file_processing import FileContentExtractor
from app.utils.qiniu_upload_service import QiniuUploadService
from app.utils.file_metadata_builder import FileMetadataBuilder
from app.config.file_constants import MAX_CONCURRENT_FILE_WORKERS
from src.utils.logger import BeijingLogger

logger = BeijingLogger().get_logger()


class FileProcessingManager:
    """æ–‡ä»¶å¤„ç†ç®¡ç†å™¨ï¼Œæ•´åˆæ‰€æœ‰æ–‡ä»¶ç›¸å…³æ“ä½œ"""

    def __init__(self):
        self.upload_service = QiniuUploadService()
        self.extractor = FileContentExtractor()
        self.metadata_builder = FileMetadataBuilder()

    def process_files(self, files: List[Dict], conversation_id: str, progress_callback=None) -> tuple[List[Dict], List[str], List[Dict]]:
        """
        å¤„ç†æ–‡ä»¶ä¸Šä¼ ã€æå–å’Œå…ƒæ•°æ®æ„å»ºçš„å®Œæ•´æµç¨‹

        Args:
            files: åŸå§‹æ–‡ä»¶åˆ—è¡¨
            conversation_id: ä¼šè¯ID
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶ (current, total, message, file_info) å‚æ•°

        Returns:
            (formatted_files, uploaded_file_ids, extracted_file_results)
        """
        if not files:
            return [], [], []

        total_files = len(files)

        # ç¬¬ä¸€æ­¥ï¼šä¸Šä¼ æ–‡ä»¶åˆ°ä¸ƒç‰›äº‘ï¼ˆå¸¦è¿›åº¦å›è°ƒï¼‰
        formatted_files = self._upload_files(files, conversation_id, progress_callback, total_files)

        # æå–å·²ä¸Šä¼ æ–‡ä»¶çš„UUID
        uploaded_file_ids = [f.get('file_uuid') for f in formatted_files if f.get('file_uuid')]

        # é€šçŸ¥æ–‡ä»¶ä¸Šä¼ å®Œæˆ
        if progress_callback:
            progress_callback(
                current=total_files,
                total=total_files,
                message=f"âœ… æ‰€æœ‰æ–‡ä»¶å·²ä¸Šä¼ åˆ°äº‘å­˜å‚¨ï¼ˆ{total_files}/{total_files}ï¼‰ï¼Œå¼€å§‹æå–å†…å®¹",
                file_info=None,
                stage='upload_complete'
            )

        # ç¬¬äºŒæ­¥ï¼šæå–æ–‡ä»¶å†…å®¹
        extracted_file_results = []
        if formatted_files:
            extracted_file_results = self._extract_file_contents(formatted_files)

            # ç¬¬ä¸‰æ­¥ï¼šä¸Šä¼ zipå­æ–‡ä»¶
            if extracted_file_results:
                extracted_file_results = self._process_zip_subfiles(
                    extracted_file_results, conversation_id
                )

            # ç¬¬å››æ­¥ï¼šæ¸…ç†ä¸´æ—¶æ–‡ä»¶
            self._cleanup_temp_files(formatted_files, extracted_file_results)

        return formatted_files, uploaded_file_ids, extracted_file_results

    def _upload_files(self, files: List[Dict], conversation_id: str, progress_callback=None, total_files=None) -> List[Dict]:
        """
        ä¸Šä¼ æ–‡ä»¶åˆ°ä¸ƒç‰›äº‘

        Args:
            files: åŸå§‹æ–‡ä»¶åˆ—è¡¨
            conversation_id: ä¼šè¯ID
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            total_files: æ–‡ä»¶æ€»æ•°

        Returns:
            æ ¼å¼åŒ–çš„æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
        """
        formatted_files = []
        total = total_files or len(files)

        for idx, file in enumerate(files, 1):
            file_name = file.get("file_name", "æœªçŸ¥æ–‡ä»¶")
            file_size = file.get("file_size", 0)

            # é€šçŸ¥å¼€å§‹å¤„ç†å½“å‰æ–‡ä»¶
            if progress_callback:
                size_mb = file_size / (1024 * 1024) if file_size else 0
                progress_callback(
                    current=idx,
                    total=total,
                    message=f"â˜ï¸ æ­£åœ¨ä¸Šä¼ æ–‡ä»¶ {idx}/{total}: {file_name} ({size_mb:.2f}MB)",
                    file_info={'file_name': file_name, 'file_size': file_size},
                    stage='uploading'
                )

            file_uuid = str(uuid.uuid4())

            # å¤„ç†æ–‡ä»¶ä¸Šä¼ 
            file_info = self.upload_service.process_file_upload(
                file, conversation_id, file_uuid
            )

            if file_info:
                formatted_files.append(file_info)
            else:
                # ä¸Šä¼ å¤±è´¥ï¼Œåˆ›å»ºå¤‡ç”¨æ–‡ä»¶ä¿¡æ¯
                fallback_info = self._create_fallback_file_info(file, file_uuid)
                if fallback_info:
                    formatted_files.append(fallback_info)

            # é€šçŸ¥å½“å‰æ–‡ä»¶å¤„ç†å®Œæˆ
            if progress_callback:
                progress_callback(
                    current=idx,
                    total=total,
                    message=f"âœ… æ–‡ä»¶ {idx}/{total} å·²ä¸Šä¼ åˆ°äº‘å­˜å‚¨: {file_name}",
                    file_info=file_info or fallback_info,
                    stage='uploaded'
                )

        logger.info(f"æˆåŠŸå¤„ç† {len(formatted_files)}/{len(files)} ä¸ªæ–‡ä»¶ä¸Šä¼ ")
        return formatted_files

    def _create_fallback_file_info(self, file: Dict, file_uuid: str) -> Optional[Dict]:
        """
        ä¸ºä¸Šä¼ å¤±è´¥çš„æ–‡ä»¶åˆ›å»ºå¤‡ç”¨ä¿¡æ¯

        Args:
            file: åŸå§‹æ–‡ä»¶ä¿¡æ¯
            file_uuid: æ–‡ä»¶UUID

        Returns:
            å¤‡ç”¨æ–‡ä»¶ä¿¡æ¯
        """
        file_name = file.get("file_name", "")
        file_ext = self.upload_service.get_file_extension(
            file_name, file.get("file_type", "")
        )
        file_extension = file_ext.lstrip('.').lower() if file_ext else ""

        qiniu_key = f"{file_uuid}{file_ext}" if file_ext else file_uuid

        return {
            "file_id": file_uuid,
            "file_uuid": file_uuid,
            "file_name": file_name,
            "file_url": file.get("file_url"),
            "file_extension": file_extension,
            "file_type": file.get("file_type"),
            "file_size": file.get("file_size"),
            "file_content": file.get("file_content"),
            "cloud_storage_url": file.get("file_url"),
            "qiniu_key": qiniu_key,
            "file_path": None,
            "temp_file_created": False
        }

    def _extract_file_contents(self, formatted_files: List[Dict]) -> List[Dict]:
        """
        æå–æ–‡ä»¶å†…å®¹

        Args:
            formatted_files: æ ¼å¼åŒ–çš„æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨

        Returns:
            æå–ç»“æœåˆ—è¡¨
        """
        logger.info(f"å¼€å§‹æå– {len(formatted_files)} ä¸ªæ–‡ä»¶çš„å†…å®¹")

        try:
            extracted_results = self.extractor.process_files_concurrently(
                formatted_files, max_workers=MAX_CONCURRENT_FILE_WORKERS
            )

            if extracted_results:
                logger.info(f"æˆåŠŸæå–äº† {len(extracted_results)} ä¸ªæ–‡ä»¶çš„å†…å®¹")

                # ğŸš¨ğŸš¨ğŸš¨ DEBUG: è¾“å‡ºå‰3ä¸ªæ–‡ä»¶çš„å…³é”®å­—æ®µ
                logger.info("=" * 100)
                logger.info("ğŸ”ğŸ”ğŸ” DEBUG - æå–ç»“æœè¯¦ç»†ä¿¡æ¯ï¼ˆå‰3ä¸ªæ–‡ä»¶ï¼‰")
                logger.info("=" * 100)
                for i, result in enumerate(extracted_results[:3], 1):
                    logger.info(f"ğŸ“„ æ–‡ä»¶ {i}/{len(extracted_results)}:")
                    logger.info(f"  â”œâ”€ file_name: {result.get('file_name')}")
                    logger.info(f"  â”œâ”€ file_uuid: {result.get('file_uuid')}")
                    logger.info(f"  â”œâ”€ source_type: {result.get('source_type')}")
                    logger.info(f"  â”œâ”€ is_from_zip: {result.get('is_from_zip')}")
                    logger.info(f"  â”œâ”€ extraction_mode: {result.get('extraction_mode')}")
                    logger.info(f"  â”œâ”€ original_file_path: {result.get('original_file_path')}")
                    logger.info(f"  â”œâ”€ temp_file_available: {result.get('temp_file_available')}")
                    logger.info(f"  â””â”€ temp_file_path: {result.get('temp_file_path')}")
                logger.info("=" * 100)

                return extracted_results
            else:
                logger.warning("æœªèƒ½æå–ä»»ä½•æ–‡ä»¶å†…å®¹")
                return []

        except Exception as e:
            logger.error(f"æ–‡ä»¶å†…å®¹æå–å¤±è´¥: {str(e)}")
            return []

    def _process_zip_subfiles(self, extracted_results: List[Dict],
                              conversation_id: str) -> List[Dict]:
        """
        å¤„ç†zipæ–‡ä»¶å’ŒPDFæ–‡ä»¶çš„å­æ–‡ä»¶ä¸Šä¼ 

        Args:
            extracted_results: æå–ç»“æœåˆ—è¡¨
            conversation_id: ä¼šè¯ID

        Returns:
            æ›´æ–°åçš„æå–ç»“æœåˆ—è¡¨
        """
        # ğŸš¨ğŸš¨ğŸš¨ DEBUG: åœ¨åˆ†ç±»å‰è¾“å‡ºç¬¬ä¸€ä¸ªæ–‡ä»¶çš„è¯¦ç»†ä¿¡æ¯
        logger.info("=" * 100)
        logger.info("ğŸ”ğŸ”ğŸ” DEBUG - å¼€å§‹æ–‡ä»¶åˆ†ç±»ï¼Œç¬¬ä¸€ä¸ªæ–‡ä»¶çš„å®Œæ•´ä¿¡æ¯:")
        logger.info("=" * 100)
        if extracted_results:
            first_file = extracted_results[0]
            logger.info(f"ğŸ“„ ç¬¬ä¸€ä¸ªæ–‡ä»¶:")
            logger.info(f"  â”œâ”€ file_name: {first_file.get('file_name')}")
            logger.info(f"  â”œâ”€ file_uuid: {first_file.get('file_uuid')}")
            logger.info(f"  â”œâ”€ source_type: {first_file.get('source_type')}")
            logger.info(f"  â”œâ”€ is_from_zip: {first_file.get('is_from_zip')}")
            logger.info(f"  â”œâ”€ extraction_mode: {first_file.get('extraction_mode')}")
            logger.info(f"  â”œâ”€ original_file_path å­˜åœ¨: {bool(first_file.get('original_file_path'))}")
            logger.info(f"  â”œâ”€ original_file_path å€¼: {first_file.get('original_file_path')}")
            logger.info(f"  â””â”€ temp_file_available: {first_file.get('temp_file_available')}")
        logger.info("=" * 100)

        # åˆ†ç±»æ–‡ä»¶ï¼ˆä¼˜å…ˆçº§ï¼šsource_type > is_from_zipï¼‰
        zip_files = [f for f in extracted_results if f.get('file_name', '').lower().endswith('.zip')]
        pdf_files = [f for f in extracted_results if f.get('extraction_mode') == 'with_images']  # PDFæœ¬èº«ï¼ˆå¸¦å›¾ç‰‡æ¨¡å¼ï¼‰

        # PDFæå–/æ¸²æŸ“çš„å›¾ç‰‡ï¼ˆæ— è®ºæ˜¯å¦æ¥è‡ªZIPï¼‰
        pdf_extracted_images = [
            f for f in extracted_results
            if f.get('source_type') in ['extracted_from_pdf', 'rendered_pdf_page']
        ]

        # ZIPå­æ–‡ä»¶ï¼ˆæ’é™¤PDFæå–çš„å›¾ç‰‡ï¼‰
        sub_files = [
            f for f in extracted_results
            if f.get('is_from_zip')
            and f.get('source_type') not in ['extracted_from_pdf', 'rendered_pdf_page']
        ]

        other_files = [
            f for f in extracted_results
            if not f.get('is_from_zip')
            and not f.get('file_name', '').lower().endswith('.zip')
            and f.get('source_type') not in ['extracted_from_pdf', 'rendered_pdf_page']
            and f.get('extraction_mode') != 'with_images'
        ]

        logger.info(
            f"æ–‡ä»¶åˆ†ç±»: zipæ–‡ä»¶ {len(zip_files)} ä¸ª, "
            f"PDFæ–‡ä»¶(å¸¦å›¾ç‰‡) {len(pdf_files)} ä¸ª, "
            f"ZIPå­æ–‡ä»¶ {len(sub_files)} ä¸ª, "
            f"PDFæå–å›¾ç‰‡ {len(pdf_extracted_images)} ä¸ª, "
            f"å…¶ä»–æ–‡ä»¶ {len(other_files)} ä¸ª"
        )

        # ğŸš¨ğŸš¨ğŸš¨ DEBUG: è¾“å‡ºåˆ†ç±»è¯¦æƒ…
        logger.info("=" * 100)
        logger.info("ğŸ”ğŸ”ğŸ” DEBUG - æ–‡ä»¶åˆ†ç±»è¯¦æƒ…:")
        logger.info("=" * 100)

        # ğŸš¨ DEBUG: å¦‚æœæœ‰PDFæå–å›¾ç‰‡ï¼Œè¾“å‡ºç¬¬ä¸€ä¸ªçš„è¯¦ç»†ä¿¡æ¯
        if pdf_extracted_images:
            logger.info(f"âœ… PDFæå–å›¾ç‰‡ {len(pdf_extracted_images)} ä¸ªï¼Œç¬¬1ä¸ªç¤ºä¾‹:")
            first_pdf_img = pdf_extracted_images[0]
            logger.info(f"  â”œâ”€ file_name: {first_pdf_img.get('file_name')}")
            logger.info(f"  â”œâ”€ source_type: {first_pdf_img.get('source_type')}")
            logger.info(f"  â”œâ”€ original_file_path: {first_pdf_img.get('original_file_path')}")
            logger.info(f"  â”œâ”€ temp_file_path: {first_pdf_img.get('temp_file_path')}")
            logger.info(f"  â””â”€ temp_file_available: {first_pdf_img.get('temp_file_available')}")
        else:
            logger.info("âŒ æ²¡æœ‰PDFæå–å›¾ç‰‡")

        # ğŸš¨ DEBUG: å¦‚æœæœ‰ZIPå­æ–‡ä»¶ï¼Œè¾“å‡ºç¬¬ä¸€ä¸ªçš„è¯¦ç»†ä¿¡æ¯
        if sub_files:
            logger.info(f"âœ… ZIPå­æ–‡ä»¶ {len(sub_files)} ä¸ªï¼Œç¬¬1ä¸ªç¤ºä¾‹:")
            first_sub = sub_files[0]
            logger.info(f"  â”œâ”€ file_name: {first_sub.get('file_name')}")
            logger.info(f"  â”œâ”€ source_type: {first_sub.get('source_type')}")
            logger.info(f"  â”œâ”€ is_from_zip: {first_sub.get('is_from_zip')}")
            logger.info(f"  â”œâ”€ original_file_path: {first_sub.get('original_file_path')}")
            logger.info(f"  â””â”€ temp_file_available: {first_sub.get('temp_file_available')}")
        else:
            logger.info("âŒ æ²¡æœ‰ZIPå­æ–‡ä»¶")

        logger.info("=" * 100)

        # å¤„ç†åŸå§‹zipæ–‡ä»¶ä¸Šä¼ 
        self._upload_zip_files(zip_files)

        # å¤„ç†PDFæ–‡ä»¶ä¸Šä¼ ï¼ˆPDFæœ¬èº«ï¼‰
        self._upload_pdf_files(pdf_files)

        # å¤„ç†ZIPå­æ–‡ä»¶ä¸Šä¼ 
        self._upload_subfiles(sub_files, conversation_id)

        # å¤„ç†PDFæå–çš„å›¾ç‰‡ä¸Šä¼ 
        self._upload_pdf_images(pdf_extracted_images, conversation_id)

        # å¤„ç†å…¶ä»–æ–‡ä»¶ä¸Šä¼ 
        self._upload_other_files(other_files, conversation_id)

        # é‡æ–°ç»„è£…ç»“æœ
        final_results = zip_files + pdf_files + sub_files + pdf_extracted_images + other_files

        # è¾“å‡ºä¸Šä¼ ç»Ÿè®¡
        self._log_upload_statistics(sub_files, other_files, pdf_extracted_images)

        return final_results

    def _upload_zip_files(self, zip_files: List[Dict]) -> None:
        """ä¸Šä¼ åŸå§‹zipæ–‡ä»¶"""
        for zip_file in zip_files:
            if zip_file.get('cloud_storage_url') and zip_file.get('uploaded_to_qiniu'):
                continue

            zip_file_uuid = zip_file.get('file_uuid')
            zip_file_name = zip_file.get('file_name', 'æœªçŸ¥zipæ–‡ä»¶')
            zip_file_path = zip_file.get('file_path')

            if not zip_file_path or not os.path.exists(zip_file_path):
                logger.warning(f"âš ï¸ ZIPæ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨ï¼Œè·³è¿‡ä¸Šä¼ : {zip_file_path}")
                zip_file['upload_skipped'] = True
                zip_file['skip_reason'] = 'ZIPæ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨'
                continue

            try:
                file_ext = os.path.splitext(zip_file_name)[1]
                qiniu_key = f"{zip_file_uuid}{file_ext}"

                success, cloud_url, error = self.upload_service.upload_file(
                    zip_file_path, qiniu_key
                )

                if success:
                    zip_file['file_url'] = cloud_url
                    zip_file['cloud_storage_url'] = cloud_url
                    zip_file['qiniu_key'] = qiniu_key
                    zip_file['uploaded_to_qiniu'] = True
                    logger.info(f"âœ… ZIPæ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {zip_file_name} -> {cloud_url}")
                else:
                    logger.error(f"âŒ ZIPæ–‡ä»¶ä¸Šä¼ å¤±è´¥: {zip_file_name}, é”™è¯¯: {error}")
                    zip_file['upload_failed'] = True

            except Exception as e:
                logger.error(f"å¤„ç†ZIPæ–‡ä»¶ä¸Šä¼ æ—¶å‡ºé”™: {zip_file_name}, é”™è¯¯: {str(e)}")
                zip_file['upload_failed'] = True
                zip_file['upload_error'] = str(e)

    def _upload_subfiles(self, sub_files: List[Dict], conversation_id: str) -> None:
        """ä¸Šä¼ zipå­æ–‡ä»¶"""
        logger.info(f"å¼€å§‹ä¸Šä¼  {len(sub_files)} ä¸ªZIPå­æ–‡ä»¶")
        for idx, sub_file in enumerate(sub_files, 1):
            logger.info(f"DEBUG - å‡†å¤‡ä¸Šä¼ ç¬¬ {idx}/{len(sub_files)} ä¸ªZIPå­æ–‡ä»¶:")
            logger.info(f"  file_name: {sub_file.get('file_name')}")
            logger.info(f"  file_content å­˜åœ¨: {bool(sub_file.get('file_content'))}")
            logger.info(f"  original_file_path: {sub_file.get('original_file_path')}")
            logger.info(f"  temp_file_available: {sub_file.get('temp_file_available')}")

            if sub_file.get('file_content'):
                self.upload_service.upload_zip_subfile(sub_file, conversation_id)
            else:
                logger.warning(f"  âš ï¸ è·³è¿‡ä¸Šä¼ ï¼ˆæ— file_contentï¼‰: {sub_file.get('file_name')}")

    def _upload_pdf_files(self, pdf_files: List[Dict]) -> None:
        """ä¸Šä¼ PDFæ–‡ä»¶ï¼ˆå¸¦å›¾ç‰‡æå–æ¨¡å¼ï¼‰"""
        for pdf_file in pdf_files:
            # PDFæœ¬èº«éœ€è¦ä¸Šä¼ 
            if pdf_file.get('cloud_storage_url') and pdf_file.get('uploaded_to_qiniu'):
                continue

            pdf_uuid = pdf_file.get('file_uuid')
            pdf_name = pdf_file.get('file_name', 'æœªçŸ¥PDFæ–‡ä»¶')
            pdf_path = pdf_file.get('file_path')

            if not pdf_path or not os.path.exists(pdf_path):
                logger.warning(f"âš ï¸ PDFæ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨ï¼Œè·³è¿‡ä¸Šä¼ : {pdf_path}")
                pdf_file['upload_skipped'] = True
                pdf_file['skip_reason'] = 'PDFæ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨'
                continue

            try:
                file_ext = os.path.splitext(pdf_name)[1]
                qiniu_key = f"{pdf_uuid}{file_ext}"

                success, cloud_url, error = self.upload_service.upload_file(
                    pdf_path, qiniu_key
                )

                if success:
                    pdf_file['file_url'] = cloud_url
                    pdf_file['cloud_storage_url'] = cloud_url
                    pdf_file['qiniu_key'] = qiniu_key
                    pdf_file['uploaded_to_qiniu'] = True
                    logger.info(f"âœ… PDFæ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {pdf_name} -> {cloud_url}")
                else:
                    logger.error(f"âŒ PDFæ–‡ä»¶ä¸Šä¼ å¤±è´¥: {pdf_name}, é”™è¯¯: {error}")
                    pdf_file['upload_failed'] = True

            except Exception as e:
                logger.error(f"å¤„ç†PDFæ–‡ä»¶ä¸Šä¼ æ—¶å‡ºé”™: {pdf_name}, é”™è¯¯: {str(e)}")
                pdf_file['upload_failed'] = True
                pdf_file['upload_error'] = str(e)

    def _upload_pdf_images(self, pdf_images: List[Dict], conversation_id: str) -> None:
        """ä¸Šä¼ ä»PDFæå–çš„å›¾ç‰‡"""
        logger.info(f"å¼€å§‹ä¸Šä¼  {len(pdf_images)} å¼ ä»PDFæå–çš„å›¾ç‰‡")

        for pdf_image in pdf_images:
            # ç¡®ä¿ original_file_path æ­£ç¡®è®¾ç½®ï¼ˆä¼˜å…ˆä½¿ç”¨ temp_file_pathï¼‰
            if not pdf_image.get('original_file_path') and pdf_image.get('temp_file_path'):
                pdf_image['original_file_path'] = pdf_image['temp_file_path']

            # ç¡®ä¿ temp_file_available å·²è®¾ç½®
            if pdf_image.get('temp_file_path') and os.path.exists(pdf_image.get('temp_file_path', '')):
                pdf_image['temp_file_available'] = True

            self.upload_service.upload_zip_subfile(pdf_image, conversation_id)

            # å¦‚æœæœ‰è£å‰ªçš„åŒ»å­¦å½±åƒï¼Œä¹Ÿä¸Šä¼ 
            if pdf_image.get('cropped_image_available') and pdf_image.get('cropped_image_path'):
                self._upload_cropped_image(pdf_image, conversation_id)

    def _upload_cropped_image(self, image_file: Dict, conversation_id: str) -> None:
        """ä¸Šä¼ è£å‰ªåçš„åŒ»å­¦å½±åƒ"""
        cropped_path = image_file.get('cropped_image_path')
        if not cropped_path or not os.path.exists(cropped_path):
            logger.warning(f"è£å‰ªå›¾ç‰‡è·¯å¾„æ— æ•ˆæˆ–ä¸å­˜åœ¨: {cropped_path}")
            return

        try:
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨å·²æœ‰çš„ cropped_image_uuidï¼Œå¦‚æœæ²¡æœ‰æ‰ç”Ÿæˆæ–°çš„
            cropped_uuid = image_file.get('cropped_image_uuid')
            if not cropped_uuid:
                import uuid
                cropped_uuid = str(uuid.uuid4())
                image_file['cropped_image_uuid'] = cropped_uuid
                logger.warning(f"è£å‰ªå›¾ç‰‡ç¼ºå°‘UUIDï¼Œå·²ç”Ÿæˆæ–°UUID: {cropped_uuid}")

            original_filename = image_file.get('file_name', 'image')
            base_name = os.path.splitext(original_filename)[0]
            cropped_filename = f"cropped_{base_name}.jpg"

            qiniu_key = f"{conversation_id}/cropped/{cropped_uuid}.jpg"

            logger.info(f"ä¸Šä¼ è£å‰ªå›¾ç‰‡: {cropped_filename} -> {qiniu_key} (UUID: {cropped_uuid})")

            # ä¸Šä¼ åˆ°ä¸ƒç‰›äº‘
            success, cloud_url, error = self.upload_service.upload_file(
                cropped_path,
                qiniu_key
            )

            if success:
                # æ›´æ–°åŸæ–‡ä»¶ä¿¡æ¯ï¼Œæ·»åŠ è£å‰ªå›¾ç‰‡URLå’ŒUUID
                image_file['cropped_image_url'] = cloud_url
                image_file['cropped_image_uuid'] = cropped_uuid  # ğŸ”§ ç¡®ä¿UUIDè¢«ä¿å­˜
                logger.info(f"è£å‰ªå›¾ç‰‡ä¸Šä¼ æˆåŠŸ: {cloud_url}, UUID: {cropped_uuid}")

                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    os.unlink(cropped_path)
                    temp_dir = image_file.get('cropped_temp_dir')
                    if temp_dir and os.path.exists(temp_dir):
                        os.rmdir(temp_dir)
                    logger.debug(f"æ¸…ç†è£å‰ªå›¾ç‰‡ä¸´æ—¶æ–‡ä»¶: {cropped_path}")
                except Exception as cleanup_error:
                    logger.warning(f"æ¸…ç†è£å‰ªå›¾ç‰‡ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {cleanup_error}")
            else:
                logger.error(f"è£å‰ªå›¾ç‰‡ä¸Šä¼ å¤±è´¥: {error}")
                image_file['cropped_image_available'] = False

        except Exception as e:
            logger.error(f"ä¸Šä¼ è£å‰ªå›¾ç‰‡æ—¶å‡ºé”™: {str(e)}")
            image_file['cropped_image_available'] = False


    def _upload_other_files(self, other_files: List[Dict], conversation_id: str) -> None:
        """ä¸Šä¼ å…¶ä»–æ–‡ä»¶"""
        for other_file in other_files:
            if other_file.get('cloud_storage_url') and other_file.get('uploaded_to_qiniu'):
                continue

            other_file_uuid = other_file.get('file_uuid')
            other_file_name = other_file.get('file_name', 'æœªçŸ¥æ–‡ä»¶')
            other_file_path = other_file.get('file_path')

            if not other_file_uuid or not other_file_path or not os.path.exists(other_file_path):
                logger.warning(f"âš ï¸ ézipæ–‡ä»¶ç¼ºå°‘å¿…è¦ä¿¡æ¯ï¼Œè·³è¿‡ä¸Šä¼ : {other_file_name}")
                other_file['upload_skipped'] = True
                other_file['skip_reason'] = 'ç¼ºå°‘æ–‡ä»¶UUIDæˆ–è·¯å¾„'
                continue

            try:
                file_ext = os.path.splitext(other_file_name)[1]
                qiniu_key = f"{other_file_uuid}{file_ext}"

                success, cloud_url, error = self.upload_service.upload_file(
                    other_file_path, qiniu_key
                )

                if success:
                    other_file['file_url'] = cloud_url
                    other_file['cloud_storage_url'] = cloud_url
                    other_file['qiniu_key'] = qiniu_key
                    other_file['uploaded_to_qiniu'] = True
                    other_file['upload_method'] = 'direct_original_file'
                    logger.info(f"âœ… ézipæ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {other_file_name} -> {cloud_url}")

                    # ğŸ”§ ä¿®å¤ï¼šå¦‚æœæ˜¯å›¾ç‰‡ä¸”æœ‰è£å‰ªçš„åŒ»å­¦å½±åƒï¼Œä¹Ÿä¸Šä¼ ï¼ˆä½¿ç”¨æ­£ç¡®çš„conversation_idï¼‰
                    if file_ext.lower() in ['.png', '.jpg', '.jpeg', '.webp', '.heic', '.heif']:
                        if other_file.get('cropped_image_available') and other_file.get('cropped_image_path'):
                            self._upload_cropped_image(other_file, conversation_id)
                else:
                    logger.error(f"âŒ ézipæ–‡ä»¶ä¸Šä¼ å¤±è´¥: {other_file_name}, é”™è¯¯: {error}")
                    other_file['upload_failed'] = True
                    other_file['upload_error'] = error

            except Exception as e:
                logger.error(f"å¤„ç†ézipæ–‡ä»¶ä¸Šä¼ æ—¶å‡ºé”™: {other_file_name}, é”™è¯¯: {str(e)}")
                other_file['upload_failed'] = True
                other_file['upload_error'] = str(e)

    def _log_upload_statistics(self, sub_files: List[Dict], other_files: List[Dict],
                              pdf_images: List[Dict] = None) -> None:
        """è®°å½•ä¸Šä¼ ç»Ÿè®¡ä¿¡æ¯"""
        # zipå­æ–‡ä»¶ç»Ÿè®¡
        uploaded_sub = [f for f in sub_files if f.get('uploaded_to_qiniu')]
        skipped_sub = [f for f in sub_files if f.get('upload_skipped')]
        failed_sub = [f for f in sub_files if f.get('upload_failed')]

        if sub_files:
            logger.info(f"zipå­æ–‡ä»¶ç»Ÿè®¡: æ€»è®¡ {len(sub_files)} ä¸ª")
            logger.info(f"  âœ… æˆåŠŸä¸Šä¼ : {len(uploaded_sub)} ä¸ª")
            logger.info(f"  âš ï¸ è·³è¿‡ä¸Šä¼ : {len(skipped_sub)} ä¸ª")
            logger.info(f"  âŒ ä¸Šä¼ å¤±è´¥: {len(failed_sub)} ä¸ª")

        # PDFæå–å›¾ç‰‡ç»Ÿè®¡ï¼ˆæ–°å¢ï¼‰
        if pdf_images:
            uploaded_pdf_imgs = [f for f in pdf_images if f.get('uploaded_to_qiniu')]
            skipped_pdf_imgs = [f for f in pdf_images if f.get('upload_skipped')]
            failed_pdf_imgs = [f for f in pdf_images if f.get('upload_failed')]

            logger.info(f"PDFæå–å›¾ç‰‡ç»Ÿè®¡: æ€»è®¡ {len(pdf_images)} ä¸ª")
            logger.info(f"  âœ… æˆåŠŸä¸Šä¼ : {len(uploaded_pdf_imgs)} ä¸ª")
            logger.info(f"  âš ï¸ è·³è¿‡ä¸Šä¼ : {len(skipped_pdf_imgs)} ä¸ª")
            logger.info(f"  âŒ ä¸Šä¼ å¤±è´¥: {len(failed_pdf_imgs)} ä¸ª")

        # ézipæ–‡ä»¶ç»Ÿè®¡
        uploaded_other = [f for f in other_files if f.get('uploaded_to_qiniu')]
        skipped_other = [f for f in other_files if f.get('upload_skipped')]
        failed_other = [f for f in other_files if f.get('upload_failed')]

        if other_files:
            logger.info(f"ézipæ–‡ä»¶ç»Ÿè®¡: æ€»è®¡ {len(other_files)} ä¸ª")
            logger.info(f"  âœ… æˆåŠŸä¸Šä¼ : {len(uploaded_other)} ä¸ª")
            logger.info(f"  âš ï¸ è·³è¿‡ä¸Šä¼ : {len(skipped_other)} ä¸ª")
            logger.info(f"  âŒ ä¸Šä¼ å¤±è´¥: {len(failed_other)} ä¸ª")

    def _cleanup_temp_files(self, formatted_files: List[Dict],
                           extracted_results: List[Dict]) -> None:
        """
        æ¸…ç†ä¸´æ—¶æ–‡ä»¶

        Args:
            formatted_files: æ ¼å¼åŒ–çš„æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
            extracted_results: æå–ç»“æœåˆ—è¡¨
        """
        # æ¸…ç†formatted_filesä¸­çš„ä¸´æ—¶æ–‡ä»¶
        temp_files_cleaned = 0
        for file_info in formatted_files:
            if file_info.get('temp_file_created') and file_info.get('file_path'):
                try:
                    temp_file_path = file_info['file_path']
                    if os.path.exists(temp_file_path):
                        os.unlink(temp_file_path)
                        temp_files_cleaned += 1
                        logger.debug(f"Cleaned temp file: {temp_file_path}")
                except Exception as e:
                    logger.warning(f"Failed to clean temp file {file_info.get('file_path')}: {str(e)}")

        if temp_files_cleaned > 0:
            logger.info(f"æ¸…ç†äº† {temp_files_cleaned} ä¸ªä¸´æ—¶æ–‡ä»¶")

        # æ¸…ç†zipæ–‡ä»¶çš„æŒä¹…ä¸´æ—¶ç›®å½•
        persistent_temp_dirs = set()
        for result in extracted_results:
            if isinstance(result, dict) and result.get('cleanup_temp_dir'):
                persistent_temp_dirs.add(result['cleanup_temp_dir'])

        persistent_dirs_cleaned = 0
        for temp_dir in persistent_temp_dirs:
            try:
                if os.path.exists(temp_dir):
                    shutil.rmtree(temp_dir)
                    persistent_dirs_cleaned += 1
                    logger.info(f"æ¸…ç†äº†zipæŒä¹…ä¸´æ—¶ç›®å½•: {temp_dir}")
            except Exception as e:
                logger.warning(f"æ¸…ç†zipæŒä¹…ä¸´æ—¶ç›®å½•å¤±è´¥ {temp_dir}: {str(e)}")

        if persistent_dirs_cleaned > 0:
            logger.info(f"æ€»è®¡æ¸…ç†äº† {persistent_dirs_cleaned} ä¸ªzipæŒä¹…ä¸´æ—¶ç›®å½•")
