"""
æµ‹è¯•Tokenç®¡ç†å’Œæ•°æ®å‹ç¼©åŠŸèƒ½

æµ‹è¯•åœºæ™¯ï¼š
1. Tokenä¼°ç®—å‡†ç¡®æ€§
2. æ•°æ®å‹ç¼©åŠŸèƒ½
3. åˆ†å—å¤„ç†åŠŸèƒ½
4. è¾“å‡ºå®Œæ•´æ€§éªŒè¯
"""

import os
import sys
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv
load_dotenv()

from src.utils.token_manager import TokenManager
from src.utils.data_compressor import PatientDataCompressor
from src.utils.chunked_processor import ChunkedPPTProcessor
from src.utils.output_completeness_guard import OutputCompletenessGuard
from src.utils.logger import BeijingLogger

# åˆå§‹åŒ–logger
logger = BeijingLogger().get_logger()


def create_test_patient_data(size='small'):
    """åˆ›å»ºæµ‹è¯•æ‚£è€…æ•°æ®

    Args:
        size: æ•°æ®å¤§å° ('small', 'medium', 'large', 'xlarge')

    Returns:
        dict: æµ‹è¯•æ‚£è€…æ•°æ®
    """
    # åŸºç¡€æ•°æ®
    base_data = {
        'patient_name': 'å¼ ä¸‰',
        'patient_info': {
            'basic': {
                'name': 'å¼ ä¸‰',
                'age': 45,
                'gender': 'ç”·',
                'id': '123456789012345678'
            },
            'contact': {
                'phone': '13800138000',
                'address': 'åŒ—äº¬å¸‚æœé˜³åŒº'
            }
        },
        'diagnoses': [
            {
                'date': '2024-01-15',
                'diagnosis': 'é«˜è¡€å‹',
                'icd_code': 'I10',
                'doctor': 'æåŒ»ç”Ÿ'
            },
            {
                'date': '2024-02-20',
                'diagnosis': 'ç³–å°¿ç—…',
                'icd_code': 'E11',
                'doctor': 'ç‹åŒ»ç”Ÿ'
            }
        ],
        'current_medications': [
            {
                'name': 'é™å‹è¯',
                'dosage': '10mg',
                'frequency': 'æ¯æ—¥ä¸€æ¬¡'
            }
        ]
    }

    # æ ¹æ®sizeç”Ÿæˆä¸åŒæ•°é‡çš„æ—¶é—´è½´è®°å½•
    timeline_counts = {
        'small': 10,
        'medium': 50,
        'large': 200,
        'xlarge': 500
    }

    count = timeline_counts.get(size, 10)

    # ç”Ÿæˆæ—¶é—´è½´æ•°æ®
    timeline = []
    for i in range(count):
        record = {
            'date': f'2024-{(i % 12) + 1:02d}-{(i % 28) + 1:02d}',
            'event_type': ['æ£€æŸ¥', 'æ²»ç–—', 'å¤è¯Š', 'ä½é™¢'][i % 4],
            'description': f'è¿™æ˜¯ç¬¬{i+1}æ¬¡å°±è¯Šè®°å½•ï¼Œæ‚£è€…è¿›è¡Œäº†å¸¸è§„æ£€æŸ¥å’Œæ²»ç–—ã€‚' * 5,  # é‡å¤5æ¬¡å¢åŠ é•¿åº¦
            'doctor': f'åŒ»ç”Ÿ{i % 10}',
            'department': ['å†…ç§‘', 'å¤–ç§‘', 'å¿ƒå†…ç§‘', 'ç¥ç»ç§‘'][i % 4],
            'result': f'æ£€æŸ¥ç»“æœæ­£å¸¸ï¼Œç»§ç»­è§‚å¯Ÿæ²»ç–—ã€‚æ‚£è€…çŠ¶æ€è‰¯å¥½ã€‚' * 3
        }
        timeline.append(record)

    base_data['patient_timeline'] = timeline

    # ç”ŸæˆåŸå§‹æ–‡ä»¶æ•°æ®
    raw_files = []
    file_count = count // 2  # æ–‡ä»¶æ•°é‡æ˜¯æ—¶é—´è½´çš„ä¸€åŠ
    for i in range(file_count):
        file_item = {
            'file_uuid': f'uuid-{i:04d}',
            'filename': f'æ£€æŸ¥æŠ¥å‘Š_{i+1}.pdf',
            'file_type': ['æ£€éªŒæŠ¥å‘Š', 'å½±åƒæŠ¥å‘Š', 'ç—…å†'][i % 3],
            'exam_date': f'2024-{(i % 12) + 1:02d}-{(i % 28) + 1:02d}',
            'has_medical_image': i % 3 == 0,  # æ¯3ä¸ªæ–‡ä»¶æœ‰1ä¸ªåŒ»å­¦å½±åƒ
            'cropped_image_available': i % 3 == 0,
            'cropped_image_url': f'https://example.com/image_{i}.jpg' if i % 3 == 0 else None,
            'cloud_storage_url': f'https://example.com/file_{i}.pdf',
            'extracted_text': f'è¿™æ˜¯ç¬¬{i+1}ä¸ªæ–‡ä»¶çš„æå–æ–‡æœ¬å†…å®¹ã€‚' * 20  # é‡å¤20æ¬¡
        }
        raw_files.append(file_item)

    base_data['raw_files_data'] = raw_files

    return base_data


def test_token_estimation():
    """æµ‹è¯•1: Tokenä¼°ç®—"""
    logger.info("=" * 80)
    logger.info("æµ‹è¯•1: Tokenä¼°ç®—")
    logger.info("=" * 80)

    token_manager = TokenManager(logger=logger)

    # æµ‹è¯•ä¸åŒå¤§å°çš„æ–‡æœ¬
    test_texts = {
        'çŸ­æ–‡æœ¬': 'è¿™æ˜¯ä¸€ä¸ªç®€çŸ­çš„æµ‹è¯•æ–‡æœ¬ã€‚',
        'ä¸­æ–‡æœ¬': 'è¿™æ˜¯ä¸€ä¸ªä¸­ç­‰é•¿åº¦çš„æµ‹è¯•æ–‡æœ¬ã€‚' * 50,
        'é•¿æ–‡æœ¬': 'è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„æµ‹è¯•æ–‡æœ¬ã€‚' * 500,
        'æ··åˆæ–‡æœ¬': 'This is a mixed text with ä¸­æ–‡ and English. ' * 100
    }

    for name, text in test_texts.items():
        tokens = token_manager.estimate_tokens(text)
        logger.info(f"{name}: å­—ç¬¦æ•°={len(text)}, ä¼°ç®—tokens={tokens}, æ¯”ä¾‹={len(text)/tokens:.2f}å­—ç¬¦/token")

    logger.info("âœ… Tokenä¼°ç®—æµ‹è¯•å®Œæˆ\n")


def test_data_compression():
    """æµ‹è¯•2: æ•°æ®å‹ç¼©"""
    logger.info("=" * 80)
    logger.info("æµ‹è¯•2: æ•°æ®å‹ç¼©")
    logger.info("=" * 80)

    token_manager = TokenManager(logger=logger)
    data_compressor = PatientDataCompressor(logger=logger, token_manager=token_manager)

    # æµ‹è¯•ä¸åŒå¤§å°çš„æ•°æ®
    for size in ['small', 'medium', 'large']:
        logger.info(f"\n--- æµ‹è¯• {size} æ•°æ®é›† ---")

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        patient_data = create_test_patient_data(size=size)

        # ä¼°ç®—åŸå§‹tokenæ•°
        original_tokens = token_manager.estimate_tokens(patient_data)
        logger.info(f"åŸå§‹æ•°æ®: tokens={original_tokens}")

        # å‹ç¼©åˆ°50%
        target_tokens = original_tokens // 2
        compressed_data = data_compressor.compress_data(patient_data, target_tokens)

        # ä¼°ç®—å‹ç¼©åtokenæ•°
        compressed_tokens = token_manager.estimate_tokens(compressed_data)
        compression_ratio = compressed_tokens / original_tokens if original_tokens > 0 else 1.0

        logger.info(f"å‹ç¼©åæ•°æ®: tokens={compressed_tokens}, å‹ç¼©æ¯”ä¾‹={compression_ratio:.1%}")

        # éªŒè¯å…³é”®å­—æ®µæ˜¯å¦ä¿ç•™
        assert 'patient_name' in compressed_data, "å…³é”®å­—æ®µpatient_nameä¸¢å¤±"
        assert 'patient_info' in compressed_data, "å…³é”®å­—æ®µpatient_infoä¸¢å¤±"
        logger.info("âœ… å…³é”®å­—æ®µéªŒè¯é€šè¿‡")

    logger.info("\nâœ… æ•°æ®å‹ç¼©æµ‹è¯•å®Œæˆ\n")


def test_input_limit_check():
    """æµ‹è¯•3: è¾“å…¥é™åˆ¶æ£€æŸ¥"""
    logger.info("=" * 80)
    logger.info("æµ‹è¯•3: è¾“å…¥é™åˆ¶æ£€æŸ¥")
    logger.info("=" * 80)

    token_manager = TokenManager(logger=logger)

    # æµ‹è¯•ä¸åŒå¤§å°çš„æ•°æ®
    for size in ['small', 'medium', 'large', 'xlarge']:
        logger.info(f"\n--- æµ‹è¯• {size} æ•°æ®é›† ---")

        patient_data = create_test_patient_data(size=size)

        # æ£€æŸ¥è¾“å…¥é™åˆ¶
        check_result = token_manager.check_input_limit(patient_data, 'gemini-3-flash-preview')

        logger.info(f"æ£€æŸ¥ç»“æœ:")
        logger.info(f"  - æ€»tokens: {check_result['total_tokens']}")
        logger.info(f"  - é™åˆ¶: {check_result['limit']}")
        logger.info(f"  - å®‰å…¨é™åˆ¶: {check_result['safe_limit']}")
        logger.info(f"  - åœ¨é™åˆ¶å†…: {check_result['within_limit']}")
        logger.info(f"  - éœ€è¦å‹ç¼©: {check_result['compression_needed']}")
        logger.info(f"  - ä½¿ç”¨ç‡: {check_result['usage_ratio']:.1%}")

    logger.info("\nâœ… è¾“å…¥é™åˆ¶æ£€æŸ¥æµ‹è¯•å®Œæˆ\n")


def test_timeline_compression():
    """æµ‹è¯•4: æ—¶é—´è½´å‹ç¼©"""
    logger.info("=" * 80)
    logger.info("æµ‹è¯•4: æ—¶é—´è½´å‹ç¼©")
    logger.info("=" * 80)

    token_manager = TokenManager(logger=logger)
    data_compressor = PatientDataCompressor(logger=logger, token_manager=token_manager)

    # åˆ›å»ºå¤§æ•°æ®é›†
    patient_data = create_test_patient_data(size='large')
    timeline = patient_data['patient_timeline']

    logger.info(f"åŸå§‹æ—¶é—´è½´è®°å½•æ•°: {len(timeline)}")

    # å‹ç¼©åˆ°50æ¡è®°å½•
    target_tokens = 10000  # ç›®æ ‡tokenæ•°
    compressed_timeline = data_compressor.compress_timeline(timeline, target_tokens)

    logger.info(f"å‹ç¼©åæ—¶é—´è½´è®°å½•æ•°: {len(compressed_timeline)}")

    # éªŒè¯æ˜¯å¦æŒ‰æ—¥æœŸæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    if len(compressed_timeline) > 1:
        first_date = compressed_timeline[0].get('date', '')
        last_date = compressed_timeline[-1].get('date', '')
        logger.info(f"æ—¥æœŸèŒƒå›´: {last_date} åˆ° {first_date}")

    logger.info("âœ… æ—¶é—´è½´å‹ç¼©æµ‹è¯•å®Œæˆ\n")


def test_raw_files_compression():
    """æµ‹è¯•5: åŸå§‹æ–‡ä»¶å‹ç¼©"""
    logger.info("=" * 80)
    logger.info("æµ‹è¯•5: åŸå§‹æ–‡ä»¶å‹ç¼©")
    logger.info("=" * 80)

    token_manager = TokenManager(logger=logger)
    data_compressor = PatientDataCompressor(logger=logger, token_manager=token_manager)

    # åˆ›å»ºå¤§æ•°æ®é›†
    patient_data = create_test_patient_data(size='large')
    raw_files = patient_data['raw_files_data']

    logger.info(f"åŸå§‹æ–‡ä»¶æ•°: {len(raw_files)}")

    # ç»Ÿè®¡åŒ»å­¦å½±åƒæ–‡ä»¶æ•°
    medical_image_count = sum(1 for f in raw_files if f.get('has_medical_image', False))
    logger.info(f"åŒ»å­¦å½±åƒæ–‡ä»¶æ•°: {medical_image_count}")

    # å‹ç¼©åˆ°30ä¸ªæ–‡ä»¶
    target_tokens = 5000
    compressed_files = data_compressor.compress_raw_files(raw_files, target_tokens)

    logger.info(f"å‹ç¼©åæ–‡ä»¶æ•°: {len(compressed_files)}")

    # ç»Ÿè®¡å‹ç¼©åçš„åŒ»å­¦å½±åƒæ–‡ä»¶æ•°
    compressed_medical_count = sum(1 for f in compressed_files if f.get('has_medical_image', False))
    logger.info(f"å‹ç¼©ååŒ»å­¦å½±åƒæ–‡ä»¶æ•°: {compressed_medical_count}")

    # éªŒè¯ä¼˜å…ˆä¿ç•™åŒ»å­¦å½±åƒ
    if medical_image_count > 0:
        retention_ratio = compressed_medical_count / medical_image_count
        logger.info(f"åŒ»å­¦å½±åƒä¿ç•™ç‡: {retention_ratio:.1%}")

    logger.info("âœ… åŸå§‹æ–‡ä»¶å‹ç¼©æµ‹è¯•å®Œæˆ\n")


def test_output_completeness():
    """æµ‹è¯•6: è¾“å‡ºå®Œæ•´æ€§éªŒè¯"""
    logger.info("=" * 80)
    logger.info("æµ‹è¯•6: è¾“å‡ºå®Œæ•´æ€§éªŒè¯")
    logger.info("=" * 80)

    output_guard = OutputCompletenessGuard(logger=logger)

    # æµ‹è¯•å®Œæ•´çš„PPTæ•°æ®
    complete_ppt_data = {
        'pptTemplate2Vm': {
            'title': 'æ‚£è€…ç—…å†æŠ¥å‘Š',
            'patient': {
                'name': 'å¼ ä¸‰',
                'age': 45
            },
            'diag': {
                'diagnosis': 'é«˜è¡€å‹',
                'date': '2024-01-15'
            },
            'treatments': [],
            'examinations': []
        }
    }

    result = output_guard.validate_ppt_data(complete_ppt_data)
    logger.info(f"å®Œæ•´æ•°æ®éªŒè¯ç»“æœ: is_complete={result['is_complete']}")

    # æµ‹è¯•ä¸å®Œæ•´çš„PPTæ•°æ®
    incomplete_ppt_data = {
        'pptTemplate2Vm': {
            'title': 'æ‚£è€…ç—…å†æŠ¥å‘Š'
            # ç¼ºå°‘ patient å’Œ diag å­—æ®µ
        }
    }

    result = output_guard.validate_ppt_data(incomplete_ppt_data)
    logger.info(f"ä¸å®Œæ•´æ•°æ®éªŒè¯ç»“æœ: is_complete={result['is_complete']}")
    logger.info(f"ç¼ºå¤±å­—æ®µ: {result['missing_required_fields']}")
    logger.info(f"å»ºè®®: {result['suggestions']}")

    logger.info("âœ… è¾“å‡ºå®Œæ•´æ€§éªŒè¯æµ‹è¯•å®Œæˆ\n")


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    logger.info("ğŸš€ å¼€å§‹è¿è¡ŒTokenç®¡ç†å’Œæ•°æ®å‹ç¼©åŠŸèƒ½æµ‹è¯•")
    logger.info("=" * 80)

    try:
        test_token_estimation()
        test_data_compression()
        test_input_limit_check()
        test_timeline_compression()
        test_raw_files_compression()
        test_output_completeness()

        logger.info("=" * 80)
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        logger.info("=" * 80)

    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}", exc_info=True)
        raise


if __name__ == '__main__':
    run_all_tests()
