"""
ç®€åŒ–ç‰ˆæµ‹è¯•è„šæœ¬ - æµ‹è¯•Tokenç®¡ç†å’Œæ•°æ®å‹ç¼©åŠŸèƒ½ï¼ˆä¸ä¾èµ–å¤–éƒ¨åº“ï¼‰

æµ‹è¯•åœºæ™¯ï¼š
1. Tokenä¼°ç®—å‡†ç¡®æ€§
2. æ•°æ®å‹ç¼©åŠŸèƒ½
3. è¾“å…¥é™åˆ¶æ£€æŸ¥
"""

import os
import sys
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆæ¨¡æ‹Ÿ.envæ–‡ä»¶ï¼‰
os.environ['MODEL_MAX_INPUT_TOKENS'] = '1000000'
os.environ['MODEL_MAX_OUTPUT_TOKENS'] = '65535'
os.environ['TOKEN_SAFE_INPUT_RATIO'] = '0.7'
os.environ['TOKEN_SAFE_OUTPUT_RATIO'] = '0.9'
os.environ['ENABLE_AUTO_COMPRESSION'] = 'true'
os.environ['COMPRESSION_STRATEGY'] = 'smart'
os.environ['MAX_RAW_FILES_COUNT'] = '50'
os.environ['MAX_TIMELINE_RECORDS'] = '100'
os.environ['EXTRACTED_TEXT_MAX_LENGTH'] = '200'


class SimpleLogger:
    """ç®€å•çš„æ—¥å¿—è®°å½•å™¨"""
    def info(self, msg):
        print(f"[INFO] {msg}")

    def warning(self, msg):
        print(f"[WARNING] {msg}")

    def error(self, msg, exc_info=False):
        print(f"[ERROR] {msg}")


# åˆ›å»ºç®€å•çš„logger
logger = SimpleLogger()


def create_test_patient_data(size='small'):
    """åˆ›å»ºæµ‹è¯•æ‚£è€…æ•°æ®"""
    timeline_counts = {
        'small': 10,
        'medium': 50,
        'large': 200,
    }

    count = timeline_counts.get(size, 10)

    # ç”Ÿæˆæ—¶é—´è½´æ•°æ®
    timeline = []
    for i in range(count):
        record = {
            'date': f'2024-{(i % 12) + 1:02d}-{(i % 28) + 1:02d}',
            'event_type': ['æ£€æŸ¥', 'æ²»ç–—', 'å¤è¯Š', 'ä½é™¢'][i % 4],
            'description': f'è¿™æ˜¯ç¬¬{i+1}æ¬¡å°±è¯Šè®°å½•ï¼Œæ‚£è€…è¿›è¡Œäº†å¸¸è§„æ£€æŸ¥å’Œæ²»ç–—ã€‚' * 5,
            'doctor': f'åŒ»ç”Ÿ{i % 10}',
            'result': f'æ£€æŸ¥ç»“æœæ­£å¸¸ï¼Œç»§ç»­è§‚å¯Ÿæ²»ç–—ã€‚' * 3
        }
        timeline.append(record)

    # ç”ŸæˆåŸå§‹æ–‡ä»¶æ•°æ®
    raw_files = []
    file_count = count // 2
    for i in range(file_count):
        file_item = {
            'file_uuid': f'uuid-{i:04d}',
            'filename': f'æ£€æŸ¥æŠ¥å‘Š_{i+1}.pdf',
            'file_type': ['æ£€éªŒæŠ¥å‘Š', 'å½±åƒæŠ¥å‘Š', 'ç—…å†'][i % 3],
            'exam_date': f'2024-{(i % 12) + 1:02d}-{(i % 28) + 1:02d}',
            'has_medical_image': i % 3 == 0,
            'extracted_text': f'è¿™æ˜¯ç¬¬{i+1}ä¸ªæ–‡ä»¶çš„æå–æ–‡æœ¬å†…å®¹ã€‚' * 20
        }
        raw_files.append(file_item)

    return {
        'patient_name': 'å¼ ä¸‰',
        'patient_info': {
            'basic': {'name': 'å¼ ä¸‰', 'age': 45, 'gender': 'ç”·'}
        },
        'patient_timeline': timeline,
        'raw_files_data': raw_files
    }


def test_token_estimation():
    """æµ‹è¯•1: Tokenä¼°ç®—"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•1: Tokenä¼°ç®—")
    print("=" * 80)

    # å¯¼å…¥TokenManagerï¼ˆå»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…dotenvé—®é¢˜ï¼‰
    from src.utils.token_manager import TokenManager
    token_manager = TokenManager(logger=logger)

    test_texts = {
        'çŸ­æ–‡æœ¬': 'è¿™æ˜¯ä¸€ä¸ªç®€çŸ­çš„æµ‹è¯•æ–‡æœ¬ã€‚',
        'ä¸­æ–‡æœ¬': 'è¿™æ˜¯ä¸€ä¸ªä¸­ç­‰é•¿åº¦çš„æµ‹è¯•æ–‡æœ¬ã€‚' * 50,
        'é•¿æ–‡æœ¬': 'è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„æµ‹è¯•æ–‡æœ¬ã€‚' * 500,
    }

    for name, text in test_texts.items():
        tokens = token_manager.estimate_tokens(text)
        logger.info(f"{name}: å­—ç¬¦æ•°={len(text)}, ä¼°ç®—tokens={tokens}, æ¯”ä¾‹={len(text)/tokens:.2f}å­—ç¬¦/token")

    print("âœ… Tokenä¼°ç®—æµ‹è¯•å®Œæˆ\n")


def test_data_compression():
    """æµ‹è¯•2: æ•°æ®å‹ç¼©"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•2: æ•°æ®å‹ç¼©")
    print("=" * 80)

    from src.utils.token_manager import TokenManager
    from src.utils.data_compressor import PatientDataCompressor

    token_manager = TokenManager(logger=logger)
    data_compressor = PatientDataCompressor(logger=logger, token_manager=token_manager)

    for size in ['small', 'medium', 'large']:
        print(f"\n--- æµ‹è¯• {size} æ•°æ®é›† ---")

        patient_data = create_test_patient_data(size=size)
        original_tokens = token_manager.estimate_tokens(patient_data)
        logger.info(f"åŸå§‹æ•°æ®: tokens={original_tokens}")

        # å‹ç¼©åˆ°50%
        target_tokens = original_tokens // 2
        compressed_data = data_compressor.compress_data(patient_data, target_tokens)

        compressed_tokens = token_manager.estimate_tokens(compressed_data)
        compression_ratio = compressed_tokens / original_tokens if original_tokens > 0 else 1.0

        logger.info(f"å‹ç¼©åæ•°æ®: tokens={compressed_tokens}, å‹ç¼©æ¯”ä¾‹={compression_ratio:.1%}")

        # éªŒè¯å…³é”®å­—æ®µ
        assert 'patient_name' in compressed_data, "å…³é”®å­—æ®µpatient_nameä¸¢å¤±"
        assert 'patient_info' in compressed_data, "å…³é”®å­—æ®µpatient_infoä¸¢å¤±"
        logger.info("âœ… å…³é”®å­—æ®µéªŒè¯é€šè¿‡")

    print("\nâœ… æ•°æ®å‹ç¼©æµ‹è¯•å®Œæˆ\n")


def test_input_limit_check():
    """æµ‹è¯•3: è¾“å…¥é™åˆ¶æ£€æŸ¥"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•3: è¾“å…¥é™åˆ¶æ£€æŸ¥")
    print("=" * 80)

    from src.utils.token_manager import TokenManager
    token_manager = TokenManager(logger=logger)

    for size in ['small', 'medium', 'large']:
        print(f"\n--- æµ‹è¯• {size} æ•°æ®é›† ---")

        patient_data = create_test_patient_data(size=size)
        check_result = token_manager.check_input_limit(patient_data, 'gemini-3-flash-preview')

        logger.info(f"æ£€æŸ¥ç»“æœ:")
        logger.info(f"  - æ€»tokens: {check_result['total_tokens']}")
        logger.info(f"  - é™åˆ¶: {check_result['limit']}")
        logger.info(f"  - å®‰å…¨é™åˆ¶: {check_result['safe_limit']}")
        logger.info(f"  - åœ¨é™åˆ¶å†…: {check_result['within_limit']}")
        logger.info(f"  - éœ€è¦å‹ç¼©: {check_result['compression_needed']}")
        logger.info(f"  - ä½¿ç”¨ç‡: {check_result['usage_ratio']:.1%}")

    print("\nâœ… è¾“å…¥é™åˆ¶æ£€æŸ¥æµ‹è¯•å®Œæˆ\n")


def test_timeline_compression():
    """æµ‹è¯•4: æ—¶é—´è½´å‹ç¼©"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•4: æ—¶é—´è½´å‹ç¼©")
    print("=" * 80)

    from src.utils.token_manager import TokenManager
    from src.utils.data_compressor import PatientDataCompressor

    token_manager = TokenManager(logger=logger)
    data_compressor = PatientDataCompressor(logger=logger, token_manager=token_manager)

    patient_data = create_test_patient_data(size='large')
    timeline = patient_data['patient_timeline']

    logger.info(f"åŸå§‹æ—¶é—´è½´è®°å½•æ•°: {len(timeline)}")

    target_tokens = 10000
    compressed_timeline = data_compressor.compress_timeline(timeline, target_tokens)

    logger.info(f"å‹ç¼©åæ—¶é—´è½´è®°å½•æ•°: {len(compressed_timeline)}")

    if len(compressed_timeline) > 1:
        first_date = compressed_timeline[0].get('date', '')
        last_date = compressed_timeline[-1].get('date', '')
        logger.info(f"æ—¥æœŸèŒƒå›´: {last_date} åˆ° {first_date}")

    print("âœ… æ—¶é—´è½´å‹ç¼©æµ‹è¯•å®Œæˆ\n")


def test_raw_files_compression():
    """æµ‹è¯•5: åŸå§‹æ–‡ä»¶å‹ç¼©"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•5: åŸå§‹æ–‡ä»¶å‹ç¼©")
    print("=" * 80)

    from src.utils.token_manager import TokenManager
    from src.utils.data_compressor import PatientDataCompressor

    token_manager = TokenManager(logger=logger)
    data_compressor = PatientDataCompressor(logger=logger, token_manager=token_manager)

    patient_data = create_test_patient_data(size='large')
    raw_files = patient_data['raw_files_data']

    logger.info(f"åŸå§‹æ–‡ä»¶æ•°: {len(raw_files)}")

    medical_image_count = sum(1 for f in raw_files if f.get('has_medical_image', False))
    logger.info(f"åŒ»å­¦å½±åƒæ–‡ä»¶æ•°: {medical_image_count}")

    target_tokens = 5000
    compressed_files = data_compressor.compress_raw_files(raw_files, target_tokens)

    logger.info(f"å‹ç¼©åæ–‡ä»¶æ•°: {len(compressed_files)}")

    compressed_medical_count = sum(1 for f in compressed_files if f.get('has_medical_image', False))
    logger.info(f"å‹ç¼©ååŒ»å­¦å½±åƒæ–‡ä»¶æ•°: {compressed_medical_count}")

    if medical_image_count > 0:
        retention_ratio = compressed_medical_count / medical_image_count
        logger.info(f"åŒ»å­¦å½±åƒä¿ç•™ç‡: {retention_ratio:.1%}")

    print("âœ… åŸå§‹æ–‡ä»¶å‹ç¼©æµ‹è¯•å®Œæˆ\n")


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\nğŸš€ å¼€å§‹è¿è¡ŒTokenç®¡ç†å’Œæ•°æ®å‹ç¼©åŠŸèƒ½æµ‹è¯•")
    print("=" * 80)

    try:
        test_token_estimation()
        test_data_compression()
        test_input_limit_check()
        test_timeline_compression()
        test_raw_files_compression()

        print("=" * 80)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("=" * 80)

    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == '__main__':
    run_all_tests()
